{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"RoBERTa_MBTI_MASK_NS_optuna.ipynb","provenance":[{"file_id":"1GViGI-xQToPJ6TlIhvkfgKwhSCBhG4Ht","timestamp":1621319032244}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"020bd69c50344b948172aaa82401b996":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9b361cc740854eacb58a422cc335d6c2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8553cb22f0b9488c9a547d64e981921b","IPY_MODEL_df40aeaf8424466682b514693e86522b","IPY_MODEL_a5b4f4b69ca249daa63281b2b10b9462"]}},"9b361cc740854eacb58a422cc335d6c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8553cb22f0b9488c9a547d64e981921b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d1c783a267234464b5358ff179e53044","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0c0af6260e914e73a212d1ddffaff755"}},"df40aeaf8424466682b514693e86522b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_cb975f5b0296459199fd11f800802357","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":481,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":481,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_01c1ce546d374f71a8e3ae6db5c1b90f"}},"a5b4f4b69ca249daa63281b2b10b9462":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8860b715f19a49c2b6da10ca49a6a020","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 481/481 [00:00&lt;00:00, 16.9kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_751038adb10342f0828466126f12626e"}},"d1c783a267234464b5358ff179e53044":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0c0af6260e914e73a212d1ddffaff755":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cb975f5b0296459199fd11f800802357":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"01c1ce546d374f71a8e3ae6db5c1b90f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8860b715f19a49c2b6da10ca49a6a020":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"751038adb10342f0828466126f12626e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6eb0f4e6b2a04d47942acb17ba3bc6ca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_55df2950381943dcb615bc00266f520f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_551b67599cc648288d2eae48fbcbfaa7","IPY_MODEL_26af01e3052446cba5061623089f3fe7","IPY_MODEL_73a438f8ccbd4605856cb7a43b51d120"]}},"55df2950381943dcb615bc00266f520f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"551b67599cc648288d2eae48fbcbfaa7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9bdb59331f974b449f1bbe9bf241d786","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f41a7cc5ce9d4eedaca2944784cfaf13"}},"26af01e3052446cba5061623089f3fe7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7a76cc446fce482c866dd8db4f13f7fa","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":898823,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":898823,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0ace3ebbfbc544108a8fd627002874f1"}},"73a438f8ccbd4605856cb7a43b51d120":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7c4f48d5da9445e991208810bfa58bfc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 878k/878k [00:00&lt;00:00, 1.51MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8cfbdb69b3524eee86529e349fb34f41"}},"9bdb59331f974b449f1bbe9bf241d786":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f41a7cc5ce9d4eedaca2944784cfaf13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7a76cc446fce482c866dd8db4f13f7fa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0ace3ebbfbc544108a8fd627002874f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7c4f48d5da9445e991208810bfa58bfc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8cfbdb69b3524eee86529e349fb34f41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"09f262924ec3480586ea6c1c88de6a48":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_30aa5a7279804a71ae5f7cba71843a0e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_17c5aa12439d47519b51dec4b8bb4670","IPY_MODEL_ca55808c20c840f7b527d107fb78c82b","IPY_MODEL_a70e07cb440e4fda820a1f8525ebe3c7"]}},"30aa5a7279804a71ae5f7cba71843a0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"17c5aa12439d47519b51dec4b8bb4670":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e0d6d99241eb4334a1901def3a3b32ec","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0ef7c227bb154a3aa92c468245bd1af4"}},"ca55808c20c840f7b527d107fb78c82b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b37f3841414a4cf78b1935495309b5d9","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":456318,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":456318,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_469a30f8f8fc4a08a6773e595a520480"}},"a70e07cb440e4fda820a1f8525ebe3c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ac3e5a7f225949da96b423394cf63733","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 446k/446k [00:00&lt;00:00, 1.71MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_aabf4afc3d9042a7822b48ac9124e20e"}},"e0d6d99241eb4334a1901def3a3b32ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0ef7c227bb154a3aa92c468245bd1af4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b37f3841414a4cf78b1935495309b5d9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"469a30f8f8fc4a08a6773e595a520480":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ac3e5a7f225949da96b423394cf63733":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"aabf4afc3d9042a7822b48ac9124e20e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7e5e4a0b720e4e53a4ddfb0d7ee8e9e8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_43a967cd19b14c859a5e5f26ed0c9f7e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7b9ae85286b645bead0680c37bac0d67","IPY_MODEL_4ac510859ee848c2978dc2b384b01285","IPY_MODEL_c636c7f4919d4583b3d4fe6213a39d2f"]}},"43a967cd19b14c859a5e5f26ed0c9f7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b9ae85286b645bead0680c37bac0d67":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_73ae8106e0bb4839bf354cb4fbb37046","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1a033c04d05541428b3536048da4310e"}},"4ac510859ee848c2978dc2b384b01285":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0d198c65147743b59ed83faaa7790760","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1355863,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1355863,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cef2588bced84cf9845d58acc3ce8442"}},"c636c7f4919d4583b3d4fe6213a39d2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b98e082d441e4e249f02e21fbce10fb0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.29M/1.29M [00:00&lt;00:00, 1.39MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0e1a7a93389e4187a41758d85a963b6b"}},"73ae8106e0bb4839bf354cb4fbb37046":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1a033c04d05541428b3536048da4310e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0d198c65147743b59ed83faaa7790760":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cef2588bced84cf9845d58acc3ce8442":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b98e082d441e4e249f02e21fbce10fb0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0e1a7a93389e4187a41758d85a963b6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1d3a1cded7474467818debe1027718d9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3a44c08bc13b47a0a8bf670326b68bdb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_346b40bf7aec4bfcb1fae37c3f9e34de","IPY_MODEL_7f57b3a0f3bb4c09b31b8f4367d60cca","IPY_MODEL_3f9f9935758b4bd99f96a53ae91629db"]}},"3a44c08bc13b47a0a8bf670326b68bdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"346b40bf7aec4bfcb1fae37c3f9e34de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c96938fad1144c7bb6814e86d7235ced","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_66fd132acfa94e0eb8d8c307c7cc46fa"}},"7f57b3a0f3bb4c09b31b8f4367d60cca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0d8eda612c4d4b2986f1af45f03c126b","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_25d51005ef6b41349f9a228d35685d68"}},"3f9f9935758b4bd99f96a53ae91629db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_533d6d2d5b8744b794fd6564070be75e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:03&lt;00:00,  1.84s/ba]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e167112001474a5ebc8272e7ac3b7505"}},"c96938fad1144c7bb6814e86d7235ced":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"66fd132acfa94e0eb8d8c307c7cc46fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0d8eda612c4d4b2986f1af45f03c126b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"25d51005ef6b41349f9a228d35685d68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"533d6d2d5b8744b794fd6564070be75e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e167112001474a5ebc8272e7ac3b7505":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5a33856101fb4466a942a0dc25e6e40b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4055c523ccf74658b5bffbbe85dc4238","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3f9535cbe6004079b9725a5a9c19eaff","IPY_MODEL_cbffb3ff9358424093b70bea112b4a96","IPY_MODEL_0b33c006f52a4ce6bc061d701fb8590d"]}},"4055c523ccf74658b5bffbbe85dc4238":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3f9535cbe6004079b9725a5a9c19eaff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4c125715cc6d4a1ca1873e38b367c891","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e18faa2aa1d849d0b55333243401a0b2"}},"cbffb3ff9358424093b70bea112b4a96":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_74d7a3f0d3984357b03019f2b32586a3","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_08bb6fc781d540a39655c417162f6207"}},"0b33c006f52a4ce6bc061d701fb8590d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b4af41efcba34bee9420cc3076d5c741","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:01&lt;00:00,  1.01s/ba]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_448d19f131f1491b9da248af60c93be6"}},"4c125715cc6d4a1ca1873e38b367c891":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e18faa2aa1d849d0b55333243401a0b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"74d7a3f0d3984357b03019f2b32586a3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"08bb6fc781d540a39655c417162f6207":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b4af41efcba34bee9420cc3076d5c741":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"448d19f131f1491b9da248af60c93be6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"eb9be87423d14f78a673f8753844bee9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b9808bf391a24327abe5e2a76fc1aa68","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d37c4c0d9da74ad3a45b64408a29b396","IPY_MODEL_ddb0e5dd38814fdabdf366360250cc78","IPY_MODEL_df3f1514f19040b8b449ccef3d2a46fc"]}},"b9808bf391a24327abe5e2a76fc1aa68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d37c4c0d9da74ad3a45b64408a29b396":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7b71a9691af046f1a571237914900692","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_92eafd68f4954972ba3939fb50a7cf66"}},"ddb0e5dd38814fdabdf366360250cc78":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_61bb3c644ee44950adb7eeef0cc9f51a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":501200538,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":501200538,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bf6b18c3ddcb47f287ef8cbbfe7fd69a"}},"df3f1514f19040b8b449ccef3d2a46fc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_627034d249784ad6bf2f49bdf3086eb2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 478M/478M [00:22&lt;00:00, 28.0MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_894ea08c319b420091799e5e90708a3c"}},"7b71a9691af046f1a571237914900692":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"92eafd68f4954972ba3939fb50a7cf66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"61bb3c644ee44950adb7eeef0cc9f51a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bf6b18c3ddcb47f287ef8cbbfe7fd69a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"627034d249784ad6bf2f49bdf3086eb2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"894ea08c319b420091799e5e90708a3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"-9igszFADeud"},"source":["# Initiliation"]},{"cell_type":"code","metadata":{"id":"4EOaUe7B1xDa","executionInfo":{"status":"ok","timestamp":1639316150934,"user_tz":-60,"elapsed":12011,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fed92504-dbbd-4672-a510-254c3db7660b"},"source":["!pip install transformers datasets --quiet"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 3.3 MB 5.3 MB/s \n","\u001b[K     |████████████████████████████████| 298 kB 36.8 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 54.5 MB/s \n","\u001b[K     |████████████████████████████████| 3.3 MB 68.4 MB/s \n","\u001b[K     |████████████████████████████████| 895 kB 55.1 MB/s \n","\u001b[K     |████████████████████████████████| 61 kB 630 kB/s \n","\u001b[K     |████████████████████████████████| 132 kB 62.4 MB/s \n","\u001b[K     |████████████████████████████████| 243 kB 53.4 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 53.2 MB/s \n","\u001b[K     |████████████████████████████████| 271 kB 74.3 MB/s \n","\u001b[K     |████████████████████████████████| 192 kB 63.1 MB/s \n","\u001b[K     |████████████████████████████████| 160 kB 56.0 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","metadata":{"id":"8KpxUoNWQDGZ","executionInfo":{"status":"ok","timestamp":1639316159107,"user_tz":-60,"elapsed":8178,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}}},"source":["from transformers import TrainingArguments\n","from transformers import Trainer\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoTokenizer\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report, confusion_matrix\n","from datasets import Dataset\n","from datasets import load_metric\n","\n","import numpy as np\n","import math\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from google.colab import drive"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KCQCqALkqtIM"},"source":["# Data Preparation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bv6QdmgkwMsZ","executionInfo":{"status":"ok","timestamp":1639328284290,"user_tz":-60,"elapsed":12125188,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"77efd2d4-4a55-4736-ca3a-69dba9476386"},"source":["drive.mount('/content/drive/')"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f9Z1U4B7zvOi","executionInfo":{"status":"ok","timestamp":1639328285841,"user_tz":-60,"elapsed":1558,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"d575a867-76e4-4780-f8c2-7221ef07ed15"},"source":["%cd 'drive/MyDrive/Masterarbeit/Colab Notebooks/OVERVIEW MBTI/Datasets/URL_Balanced_MASK'"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1aHXlqhpj1STohhfU4gn53D4whaLH__Jz/Masterarbeit/Colab Notebooks/OVERVIEW MBTI/Datasets/URL_Balanced_MASK\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"QYahF3fduAU4","executionInfo":{"status":"ok","timestamp":1639328287318,"user_tz":-60,"elapsed":1122,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"79cb86e3-fc89-487f-ee7b-9fa208c77f5d"},"source":["dfNS = pd.read_csv('MBTI_NS_URL_Balanced_MASK.csv', sep=\",\", error_bad_lines=False)\n","dfNS"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>i like that you are kind as [MASK] i find that...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>oh my you are right who really talks like tha...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>yep yep yep especially the last one yep agree ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>things that are generalizable to the entire po...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>work student hobbies studying gaming reading d...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1947</th>\n","      <td>favorites one flew over the cuckoos nest1984 a...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1948</th>\n","      <td>could i have my name changed to barkhouse than...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1949</th>\n","      <td>i hate small talk just get to the point if its...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1950</th>\n","      <td>when some days you talk with every people you ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1951</th>\n","      <td>[MASK] type most likely to believe in second c...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1952 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                   text  label\n","0     i like that you are kind as [MASK] i find that...      0\n","1      oh my you are right who really talks like tha...      0\n","2     yep yep yep especially the last one yep agree ...      0\n","3     things that are generalizable to the entire po...      0\n","4     work student hobbies studying gaming reading d...      0\n","...                                                 ...    ...\n","1947  favorites one flew over the cuckoos nest1984 a...      1\n","1948  could i have my name changed to barkhouse than...      1\n","1949  i hate small talk just get to the point if its...      1\n","1950  when some days you talk with every people you ...      1\n","1951  [MASK] type most likely to believe in second c...      1\n","\n","[1952 rows x 2 columns]"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"38B9v6_0sP3h"},"source":["# Model Training"]},{"cell_type":"code","metadata":{"id":"2ewqnPtVdCgV","executionInfo":{"status":"ok","timestamp":1639328287319,"user_tz":-60,"elapsed":11,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}}},"source":["modeltype = \"roberta-base\""],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":352,"referenced_widgets":["020bd69c50344b948172aaa82401b996","9b361cc740854eacb58a422cc335d6c2","8553cb22f0b9488c9a547d64e981921b","df40aeaf8424466682b514693e86522b","a5b4f4b69ca249daa63281b2b10b9462","d1c783a267234464b5358ff179e53044","0c0af6260e914e73a212d1ddffaff755","cb975f5b0296459199fd11f800802357","01c1ce546d374f71a8e3ae6db5c1b90f","8860b715f19a49c2b6da10ca49a6a020","751038adb10342f0828466126f12626e","6eb0f4e6b2a04d47942acb17ba3bc6ca","55df2950381943dcb615bc00266f520f","551b67599cc648288d2eae48fbcbfaa7","26af01e3052446cba5061623089f3fe7","73a438f8ccbd4605856cb7a43b51d120","9bdb59331f974b449f1bbe9bf241d786","f41a7cc5ce9d4eedaca2944784cfaf13","7a76cc446fce482c866dd8db4f13f7fa","0ace3ebbfbc544108a8fd627002874f1","7c4f48d5da9445e991208810bfa58bfc","8cfbdb69b3524eee86529e349fb34f41","09f262924ec3480586ea6c1c88de6a48","30aa5a7279804a71ae5f7cba71843a0e","17c5aa12439d47519b51dec4b8bb4670","ca55808c20c840f7b527d107fb78c82b","a70e07cb440e4fda820a1f8525ebe3c7","e0d6d99241eb4334a1901def3a3b32ec","0ef7c227bb154a3aa92c468245bd1af4","b37f3841414a4cf78b1935495309b5d9","469a30f8f8fc4a08a6773e595a520480","ac3e5a7f225949da96b423394cf63733","aabf4afc3d9042a7822b48ac9124e20e","7e5e4a0b720e4e53a4ddfb0d7ee8e9e8","43a967cd19b14c859a5e5f26ed0c9f7e","7b9ae85286b645bead0680c37bac0d67","4ac510859ee848c2978dc2b384b01285","c636c7f4919d4583b3d4fe6213a39d2f","73ae8106e0bb4839bf354cb4fbb37046","1a033c04d05541428b3536048da4310e","0d198c65147743b59ed83faaa7790760","cef2588bced84cf9845d58acc3ce8442","b98e082d441e4e249f02e21fbce10fb0","0e1a7a93389e4187a41758d85a963b6b","1d3a1cded7474467818debe1027718d9","3a44c08bc13b47a0a8bf670326b68bdb","346b40bf7aec4bfcb1fae37c3f9e34de","7f57b3a0f3bb4c09b31b8f4367d60cca","3f9f9935758b4bd99f96a53ae91629db","c96938fad1144c7bb6814e86d7235ced","66fd132acfa94e0eb8d8c307c7cc46fa","0d8eda612c4d4b2986f1af45f03c126b","25d51005ef6b41349f9a228d35685d68","533d6d2d5b8744b794fd6564070be75e","e167112001474a5ebc8272e7ac3b7505","5a33856101fb4466a942a0dc25e6e40b","4055c523ccf74658b5bffbbe85dc4238","3f9535cbe6004079b9725a5a9c19eaff","cbffb3ff9358424093b70bea112b4a96","0b33c006f52a4ce6bc061d701fb8590d","4c125715cc6d4a1ca1873e38b367c891","e18faa2aa1d849d0b55333243401a0b2","74d7a3f0d3984357b03019f2b32586a3","08bb6fc781d540a39655c417162f6207","b4af41efcba34bee9420cc3076d5c741","448d19f131f1491b9da248af60c93be6","eb9be87423d14f78a673f8753844bee9","b9808bf391a24327abe5e2a76fc1aa68","d37c4c0d9da74ad3a45b64408a29b396","ddb0e5dd38814fdabdf366360250cc78","df3f1514f19040b8b449ccef3d2a46fc","7b71a9691af046f1a571237914900692","92eafd68f4954972ba3939fb50a7cf66","61bb3c644ee44950adb7eeef0cc9f51a","bf6b18c3ddcb47f287ef8cbbfe7fd69a","627034d249784ad6bf2f49bdf3086eb2","894ea08c319b420091799e5e90708a3c"]},"id":"kB1ZJyhlp4O8","executionInfo":{"status":"ok","timestamp":1639328320226,"user_tz":-60,"elapsed":32917,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"1dc05530-ddb9-4089-abf8-ce1c81ede447"},"source":["train, test = train_test_split(dfNS, test_size=0.2, random_state=0, stratify=dfNS.label)\n","\n","train = Dataset.from_pandas(train)\n","test = Dataset.from_pandas(test)\n","\n","tokenizer = AutoTokenizer.from_pretrained(modeltype)\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n","\n","tokenized_train = train.map(tokenize_function, batched=True)\n","tokenized_test = test.map(tokenize_function, batched=True)\n","\n","full_train_dataset = tokenized_train\n","full_eval_dataset = tokenized_test\n","\n","model = AutoModelForSequenceClassification.from_pretrained(modeltype, num_labels=2)\n","\n","training_args = TrainingArguments(\n","    \"RoBERTa_NS_MASK\", \n","    evaluation_strategy=\"epoch\",\n","    save_strategy = 'no',\n","    save_steps = 100000,\n","    save_total_limit = 1,\n","    metric_for_best_model=\"eval_f1\")\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n","    acc = accuracy_score(labels, preds)\n","    print(classification_report(labels, preds, labels=[0,1]))\n","    print(confusion_matrix(labels,preds))\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"020bd69c50344b948172aaa82401b996","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6eb0f4e6b2a04d47942acb17ba3bc6ca","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09f262924ec3480586ea6c1c88de6a48","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e5e4a0b720e4e53a4ddfb0d7ee8e9e8","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d3a1cded7474467818debe1027718d9","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a33856101fb4466a942a0dc25e6e40b","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eb9be87423d14f78a673f8753844bee9","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","metadata":{"id":"W_PFsTTOqm4a"},"source":["# Hyperparameter Optimization"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N9iWNo8V7gby","executionInfo":{"status":"ok","timestamp":1639328326330,"user_tz":-60,"elapsed":6111,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"653824e7-e1ca-4c74-9ffd-144ec8779ff3"},"source":["! pip install optuna --quiet"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 308 kB 5.2 MB/s \n","\u001b[K     |████████████████████████████████| 80 kB 11.3 MB/s \n","\u001b[K     |████████████████████████████████| 209 kB 65.3 MB/s \n","\u001b[K     |████████████████████████████████| 75 kB 5.5 MB/s \n","\u001b[K     |████████████████████████████████| 49 kB 8.3 MB/s \n","\u001b[K     |████████████████████████████████| 149 kB 72.3 MB/s \n","\u001b[K     |████████████████████████████████| 112 kB 75.7 MB/s \n","\u001b[?25h  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","metadata":{"id":"y4BgLFRH7kVg","executionInfo":{"status":"ok","timestamp":1639328326331,"user_tz":-60,"elapsed":4,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}}},"source":["def model_init():\n","    return AutoModelForSequenceClassification.from_pretrained(modeltype, num_labels=2)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mpa3mq0u7sN3","executionInfo":{"status":"ok","timestamp":1639328338400,"user_tz":-60,"elapsed":12073,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"e6627d77-b881-4829-a1bc-e814a38e68fb"},"source":["trainer = Trainer(\n","      model_init=model_init,\n","      args=training_args, \n","      train_dataset=full_train_dataset, \n","      eval_dataset=full_eval_dataset,\n","      compute_metrics=compute_metrics \n","  )"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"pjNfFgAH7voa","executionInfo":{"status":"ok","timestamp":1639338175155,"user_tz":-60,"elapsed":2198760,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"02945183-5a71-4a31-86dc-8a7e7c0ca6d6"},"source":["import sklearn.metrics as metrics\n","import optuna\n","import sys\n","import logging\n","\n","def objective (metrics):\n","  return metrics['eval_f1']\n","\n","def hyperparameter_space(trial):\n","\n","    return {\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 5e-8, 5e-1, log=True),\n","        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [2, 4, 8, 16]),\n","        \"weight_decay\": trial.suggest_float(\"weight_decay\", 5e-12, 5e-1, log=True),\n","        \"num_train_epochs\": trial.suggest_float(\"num_train_epochs\",1,8,log=True),\n","        #\"adam_epsilon\": trial.suggest_float(\"adam_epsilon\", 1e-10, 1e-6, log=True),\n","        #\"seed\" : trial.suggest_float(\"seed\",10,60,log=True)\n","        }\n","\n","optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n","study_name = \"RoBERTa_MASK_NS\"  # Unique identifier of the study.\n","storage_name = \"sqlite:///{}.db\".format(study_name)\n","\n","best_run = trainer.hyperparameter_search(hp_space=hyperparameter_space,compute_objective=objective, n_trials=50, direction=\"maximize\",study_name=study_name, storage=storage_name )\n","\n","study = optuna.create_study()"],"execution_count":11,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-12-12 17:02:21,689]\u001b[0m A new study created in RDB with name: RoBERTa_MASK_NS\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["A new study created in RDB with name: RoBERTa_MASK_NS\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 8\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6032\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6032' max='6032' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6032/6032 16:28, Epoch 7/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.699600</td>\n","      <td>0.693003</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.694300</td>\n","      <td>0.701743</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.702800</td>\n","      <td>0.693610</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.697900</td>\n","      <td>0.693073</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.696900</td>\n","      <td>0.693119</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.696300</td>\n","      <td>0.693242</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.692800</td>\n","      <td>0.692922</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.692700</td>\n","      <td>0.692914</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-12 17:18:53,141]\u001b[0m Trial 0 finished with value: 0.3384094754653131 and parameters: {'learning_rate': 1.0696002857866855e-05, 'per_device_train_batch_size': 2, 'weight_decay': 8.320150727863287e-08, 'num_train_epochs': 7.722647604976535}. Best is trial 0 with value: 0.3384094754653131.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 0 finished with value: 0.3384094754653131 and parameters: {'learning_rate': 1.0696002857866855e-05, 'per_device_train_batch_size': 2, 'weight_decay': 8.320150727863287e-08, 'num_train_epochs': 7.722647604976535}. Best is trial 0 with value: 0.3384094754653131.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2225\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2225' max='2225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2225/2225 10:20, Epoch 5/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>63.963413</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>38.979600</td>\n","      <td>23.543633</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>27.888500</td>\n","      <td>34.840885</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>19.730000</td>\n","      <td>3.168558</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>19.730000</td>\n","      <td>9.346238</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>10.670900</td>\n","      <td>1.094561</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-12 17:29:16,156]\u001b[0m Trial 1 finished with value: 0.3384094754653131 and parameters: {'learning_rate': 0.4221776849260434, 'per_device_train_batch_size': 4, 'weight_decay': 0.041326081068267045, 'num_train_epochs': 5.688070851486295}. Best is trial 0 with value: 0.3384094754653131.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 1 finished with value: 0.3384094754653131 and parameters: {'learning_rate': 0.4221776849260434, 'per_device_train_batch_size': 4, 'weight_decay': 0.041326081068267045, 'num_train_epochs': 5.688070851486295}. Best is trial 0 with value: 0.3384094754653131.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 538\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='538' max='538' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [538/538 04:33, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.692870</td>\n","      <td>0.491049</td>\n","      <td>0.333719</td>\n","      <td>0.744872</td>\n","      <td>0.502500</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.689997</td>\n","      <td>0.514066</td>\n","      <td>0.348745</td>\n","      <td>0.589777</td>\n","      <td>0.502736</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.692100</td>\n","      <td>0.689222</td>\n","      <td>0.531969</td>\n","      <td>0.440629</td>\n","      <td>0.559797</td>\n","      <td>0.522945</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      0.01      0.01       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.74      0.50      0.33       391\n","weighted avg       0.75      0.49      0.33       391\n","\n","[[  1 199]\n"," [  0 191]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      0.99      0.68       200\n","           1       0.67      0.01      0.02       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.59      0.50      0.35       391\n","weighted avg       0.59      0.51      0.36       391\n","\n","[[199   1]\n"," [189   2]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.52      0.92      0.67       200\n","           1       0.60      0.13      0.21       191\n","\n","    accuracy                           0.53       391\n","   macro avg       0.56      0.52      0.44       391\n","weighted avg       0.56      0.53      0.45       391\n","\n","[[183  17]\n"," [166  25]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-12 17:33:52,484]\u001b[0m Trial 2 finished with value: 0.4406294706723891 and parameters: {'learning_rate': 2.4400310752316386e-06, 'per_device_train_batch_size': 8, 'weight_decay': 5.252611964132717e-07, 'num_train_epochs': 2.7442387285254384}. Best is trial 2 with value: 0.4406294706723891.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 2 finished with value: 0.4406294706723891 and parameters: {'learning_rate': 2.4400310752316386e-06, 'per_device_train_batch_size': 8, 'weight_decay': 5.252611964132717e-07, 'num_train_epochs': 2.7442387285254384}. Best is trial 2 with value: 0.4406294706723891.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 293\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='293' max='293' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [293/293 04:37, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.690054</td>\n","      <td>0.493606</td>\n","      <td>0.377268</td>\n","      <td>0.517985</td>\n","      <td>0.503822</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.692057</td>\n","      <td>0.598465</td>\n","      <td>0.579037</td>\n","      <td>0.612367</td>\n","      <td>0.593835</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.646226</td>\n","      <td>0.634271</td>\n","      <td>0.634118</td>\n","      <td>0.634108</td>\n","      <td>0.634136</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.55      0.06      0.11       200\n","           1       0.49      0.95      0.65       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.52      0.50      0.38       391\n","weighted avg       0.52      0.49      0.37       391\n","\n","[[ 12 188]\n"," [ 10 181]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      0.80      0.67       200\n","           1       0.65      0.39      0.49       191\n","\n","    accuracy                           0.60       391\n","   macro avg       0.61      0.59      0.58       391\n","weighted avg       0.61      0.60      0.58       391\n","\n","[[159  41]\n"," [116  75]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.64      0.64      0.64       200\n","           1       0.62      0.63      0.63       191\n","\n","    accuracy                           0.63       391\n","   macro avg       0.63      0.63      0.63       391\n","weighted avg       0.63      0.63      0.63       391\n","\n","[[128  72]\n"," [ 71 120]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-12 17:38:32,834]\u001b[0m Trial 3 finished with value: 0.6341179319054817 and parameters: {'learning_rate': 1.0813915071256551e-05, 'per_device_train_batch_size': 16, 'weight_decay': 3.5099819965382977e-09, 'num_train_epochs': 2.981915880251583}. Best is trial 3 with value: 0.6341179319054817.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 3 finished with value: 0.6341179319054817 and parameters: {'learning_rate': 1.0813915071256551e-05, 'per_device_train_batch_size': 16, 'weight_decay': 3.5099819965382977e-09, 'num_train_epochs': 2.981915880251583}. Best is trial 3 with value: 0.6341179319054817.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1006\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1006' max='1006' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1006/1006 04:42, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.690643</td>\n","      <td>0.554987</td>\n","      <td>0.524504</td>\n","      <td>0.584866</td>\n","      <td>0.561113</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.692400</td>\n","      <td>0.683923</td>\n","      <td>0.526854</td>\n","      <td>0.376514</td>\n","      <td>0.697291</td>\n","      <td>0.515825</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.672000</td>\n","      <td>0.667631</td>\n","      <td>0.616368</td>\n","      <td>0.616366</td>\n","      <td>0.616784</td>\n","      <td>0.616754</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.64      0.29      0.40       200\n","           1       0.53      0.83      0.64       191\n","\n","    accuracy                           0.55       391\n","   macro avg       0.58      0.56      0.52       391\n","weighted avg       0.59      0.55      0.52       391\n","\n","[[ 59 141]\n"," [ 33 158]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.52      0.99      0.68       200\n","           1       0.88      0.04      0.07       191\n","\n","    accuracy                           0.53       391\n","   macro avg       0.70      0.52      0.38       391\n","weighted avg       0.69      0.53      0.38       391\n","\n","[[199   1]\n"," [184   7]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.63      0.60      0.62       200\n","           1       0.60      0.63      0.62       191\n","\n","    accuracy                           0.62       391\n","   macro avg       0.62      0.62      0.62       391\n","weighted avg       0.62      0.62      0.62       391\n","\n","[[120  80]\n"," [ 70 121]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-12 17:43:17,950]\u001b[0m Trial 4 finished with value: 0.6163657770800628 and parameters: {'learning_rate': 2.1074410097143765e-06, 'per_device_train_batch_size': 4, 'weight_decay': 4.113693549617035e-11, 'num_train_epochs': 2.571573491376362}. Best is trial 3 with value: 0.6341179319054817.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 4 finished with value: 0.6163657770800628 and parameters: {'learning_rate': 2.1074410097143765e-06, 'per_device_train_batch_size': 4, 'weight_decay': 4.113693549617035e-11, 'num_train_epochs': 2.571573491376362}. Best is trial 3 with value: 0.6341179319054817.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 457\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='457' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/457 01:24 < 05:10, 1.15 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.693982</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 17:44:53,009]\u001b[0m Trial 5 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 5 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 8\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1438\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='393' max='1438' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 393/1438 03:10 < 08:28, 2.05 it/s, Epoch 2/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>2.316601</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.705415</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 17:48:13,852]\u001b[0m Trial 6 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 6 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1063\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1064' max='1063' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1063/1063 02:51, Epoch 1.36/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.719300</td>\n","      <td>0.693007</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.699900</td>\n","      <td>0.692900</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 17:51:15,538]\u001b[0m Trial 7 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 7 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 329\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='329' max='329' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [329/329 05:16, Epoch 3/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.673365</td>\n","      <td>0.608696</td>\n","      <td>0.608440</td>\n","      <td>0.610155</td>\n","      <td>0.609607</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.661759</td>\n","      <td>0.613811</td>\n","      <td>0.604498</td>\n","      <td>0.620233</td>\n","      <td>0.610602</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.665142</td>\n","      <td>0.634271</td>\n","      <td>0.632105</td>\n","      <td>0.635068</td>\n","      <td>0.632840</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.714520</td>\n","      <td>0.634271</td>\n","      <td>0.625451</td>\n","      <td>0.642487</td>\n","      <td>0.631073</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.63      0.57      0.60       200\n","           1       0.59      0.65      0.62       191\n","\n","    accuracy                           0.61       391\n","   macro avg       0.61      0.61      0.61       391\n","weighted avg       0.61      0.61      0.61       391\n","\n","[[114  86]\n"," [ 67 124]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.60      0.75      0.67       200\n","           1       0.64      0.47      0.54       191\n","\n","    accuracy                           0.61       391\n","   macro avg       0.62      0.61      0.60       391\n","weighted avg       0.62      0.61      0.61       391\n","\n","[[150  50]\n"," [101  90]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.63      0.69      0.66       200\n","           1       0.64      0.57      0.60       191\n","\n","    accuracy                           0.63       391\n","   macro avg       0.64      0.63      0.63       391\n","weighted avg       0.63      0.63      0.63       391\n","\n","[[139  61]\n"," [ 82 109]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.61      0.77      0.68       200\n","           1       0.67      0.49      0.57       191\n","\n","    accuracy                           0.63       391\n","   macro avg       0.64      0.63      0.63       391\n","weighted avg       0.64      0.63      0.63       391\n","\n","[[154  46]\n"," [ 97  94]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-12 17:56:35,168]\u001b[0m Trial 8 finished with value: 0.6254513300420013 and parameters: {'learning_rate': 3.01722160191678e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.0034806136879271246, 'num_train_epochs': 3.355906718431561}. Best is trial 3 with value: 0.6341179319054817.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 8 finished with value: 0.6254513300420013 and parameters: {'learning_rate': 3.01722160191678e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.0034806136879271246, 'num_train_epochs': 3.355906718431561}. Best is trial 3 with value: 0.6341179319054817.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 230\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='197' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [197/230 01:30 < 00:15, 2.15 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.693878</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 17:58:16,453]\u001b[0m Trial 9 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 9 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 178\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='178' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/178 01:24 < 01:08, 1.15 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.698762</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 17:59:51,470]\u001b[0m Trial 10 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 10 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 356\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='356' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/356 01:24 < 03:43, 1.15 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.694234</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 18:01:26,509]\u001b[0m Trial 11 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 11 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 210\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/210 01:24 < 01:36, 1.15 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.698773</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 18:03:01,806]\u001b[0m Trial 12 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 12 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 390\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='390' max='390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [390/390 06:10, Epoch 3/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.682153</td>\n","      <td>0.560102</td>\n","      <td>0.472947</td>\n","      <td>0.635854</td>\n","      <td>0.551034</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.659050</td>\n","      <td>0.624041</td>\n","      <td>0.621176</td>\n","      <td>0.625154</td>\n","      <td>0.622369</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.646753</td>\n","      <td>0.649616</td>\n","      <td>0.646275</td>\n","      <td>0.652111</td>\n","      <td>0.647723</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.705227</td>\n","      <td>0.639386</td>\n","      <td>0.636639</td>\n","      <td>0.640857</td>\n","      <td>0.637723</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.54      0.94      0.69       200\n","           1       0.73      0.16      0.26       191\n","\n","    accuracy                           0.56       391\n","   macro avg       0.64      0.55      0.47       391\n","weighted avg       0.63      0.56      0.48       391\n","\n","[[189  11]\n"," [161  30]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.69      0.65       200\n","           1       0.63      0.55      0.59       191\n","\n","    accuracy                           0.62       391\n","   macro avg       0.63      0.62      0.62       391\n","weighted avg       0.62      0.62      0.62       391\n","\n","[[139  61]\n"," [ 86 105]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.64      0.73      0.68       200\n","           1       0.67      0.57      0.61       191\n","\n","    accuracy                           0.65       391\n","   macro avg       0.65      0.65      0.65       391\n","weighted avg       0.65      0.65      0.65       391\n","\n","[[146  54]\n"," [ 83 108]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.63      0.71      0.67       200\n","           1       0.65      0.57      0.61       191\n","\n","    accuracy                           0.64       391\n","   macro avg       0.64      0.64      0.64       391\n","weighted avg       0.64      0.64      0.64       391\n","\n","[[142  58]\n"," [ 83 108]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-12 18:09:14,828]\u001b[0m Trial 13 finished with value: 0.6366386554621848 and parameters: {'learning_rate': 2.9136645854693335e-05, 'per_device_train_batch_size': 16, 'weight_decay': 1.0946416634758784e-08, 'num_train_epochs': 3.9728281289117455}. Best is trial 13 with value: 0.6366386554621848.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 13 finished with value: 0.6366386554621848 and parameters: {'learning_rate': 2.9136645854693335e-05, 'per_device_train_batch_size': 16, 'weight_decay': 1.0946416634758784e-08, 'num_train_epochs': 3.9728281289117455}. Best is trial 13 with value: 0.6366386554621848.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 405\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='405' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/405 01:24 < 04:26, 1.15 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.829817</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 18:10:50,038]\u001b[0m Trial 14 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 14 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 514\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='514' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/514 01:24 < 06:00, 1.15 it/s, Epoch 1/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.695934</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 18:12:25,129]\u001b[0m Trial 15 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 15 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 180\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/180 01:24 < 01:10, 1.15 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>2.994910</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 18:14:00,301]\u001b[0m Trial 16 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 16 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1353\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='392' max='1353' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 392/1353 01:40 < 04:07, 3.88 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.693640</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 18:15:51,132]\u001b[0m Trial 17 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 17 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1912\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='782' max='1912' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 782/1912 02:00 < 02:54, 6.49 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.699900</td>\n","      <td>0.702908</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 18:18:01,381]\u001b[0m Trial 18 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 18 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 564\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/564 01:24 < 06:44, 1.15 it/s, Epoch 1/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.692894</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 18:19:36,550]\u001b[0m Trial 19 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 19 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 314\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/314 01:24 < 03:06, 1.15 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.696312</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 18:21:11,718]\u001b[0m Trial 20 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 20 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 391\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='295' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [295/391 04:30 < 01:28, 1.08 it/s, Epoch 3/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.692661</td>\n","      <td>0.575448</td>\n","      <td>0.566229</td>\n","      <td>0.577980</td>\n","      <td>0.572395</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.700582</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.692540</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.57      0.70      0.63       200\n","           1       0.59      0.44      0.50       191\n","\n","    accuracy                           0.58       391\n","   macro avg       0.58      0.57      0.57       391\n","weighted avg       0.58      0.58      0.57       391\n","\n","[[141  59]\n"," [107  84]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 18:25:52,717]\u001b[0m Trial 21 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 21 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 303\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='303' max='303' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [303/303 04:53, Epoch 3/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.675485</td>\n","      <td>0.629156</td>\n","      <td>0.629069</td>\n","      <td>0.630150</td>\n","      <td>0.629843</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.667878</td>\n","      <td>0.613811</td>\n","      <td>0.600256</td>\n","      <td>0.624703</td>\n","      <td>0.609895</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.648524</td>\n","      <td>0.629156</td>\n","      <td>0.626331</td>\n","      <td>0.630388</td>\n","      <td>0.627487</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.648824</td>\n","      <td>0.629156</td>\n","      <td>0.626331</td>\n","      <td>0.630388</td>\n","      <td>0.627487</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.65      0.60      0.62       200\n","           1       0.61      0.66      0.63       191\n","\n","    accuracy                           0.63       391\n","   macro avg       0.63      0.63      0.63       391\n","weighted avg       0.63      0.63      0.63       391\n","\n","[[120  80]\n"," [ 65 126]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.59      0.78      0.67       200\n","           1       0.66      0.44      0.53       191\n","\n","    accuracy                           0.61       391\n","   macro avg       0.62      0.61      0.60       391\n","weighted avg       0.62      0.61      0.60       391\n","\n","[[156  44]\n"," [107  84]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.70      0.66       200\n","           1       0.64      0.55      0.59       191\n","\n","    accuracy                           0.63       391\n","   macro avg       0.63      0.63      0.63       391\n","weighted avg       0.63      0.63      0.63       391\n","\n","[[140  60]\n"," [ 85 106]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.70      0.66       200\n","           1       0.64      0.55      0.59       191\n","\n","    accuracy                           0.63       391\n","   macro avg       0.63      0.63      0.63       391\n","weighted avg       0.63      0.63      0.63       391\n","\n","[[140  60]\n"," [ 85 106]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-12 18:30:49,421]\u001b[0m Trial 22 finished with value: 0.6263305322128851 and parameters: {'learning_rate': 1.3558861400977725e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.0016058525778814324, 'num_train_epochs': 3.087744367688028}. Best is trial 13 with value: 0.6366386554621848.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 22 finished with value: 0.6263305322128851 and parameters: {'learning_rate': 1.3558861400977725e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.0016058525778814324, 'num_train_epochs': 3.087744367688028}. Best is trial 13 with value: 0.6366386554621848.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 290\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/290 01:24 < 02:45, 1.15 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.691873</td>\n","      <td>0.480818</td>\n","      <td>0.324698</td>\n","      <td>0.242268</td>\n","      <td>0.492147</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.48      0.98      0.65       191\n","\n","    accuracy                           0.48       391\n","   macro avg       0.24      0.49      0.32       391\n","weighted avg       0.24      0.48      0.32       391\n","\n","[[  0 200]\n"," [  3 188]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-12-12 18:32:24,924]\u001b[0m Trial 23 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 23 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 214\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='214' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/214 01:24 < 01:39, 1.15 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.693520</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 18:33:59,807]\u001b[0m Trial 24 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 24 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 450\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/450 01:24 < 05:05, 1.15 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.695985</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 18:35:34,801]\u001b[0m Trial 25 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 25 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 334\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='197' max='334' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [197/334 01:31 < 01:03, 2.14 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.692647</td>\n","      <td>0.488491</td>\n","      <td>0.332537</td>\n","      <td>0.494216</td>\n","      <td>0.499882</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.50      0.01      0.01       200\n","           1       0.49      0.99      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.49      0.50      0.33       391\n","weighted avg       0.49      0.49      0.33       391\n","\n","[[  1 199]\n"," [  1 190]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-12-12 18:37:16,087]\u001b[0m Trial 26 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 26 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 868\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='392' max='868' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [392/868 01:39 < 02:01, 3.91 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.692931</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 18:39:06,004]\u001b[0m Trial 27 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 27 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 287\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='288' max='287' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [287/287 04:24, Epoch 2.93/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.659404</td>\n","      <td>0.608696</td>\n","      <td>0.608532</td>\n","      <td>0.609881</td>\n","      <td>0.609490</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.658282</td>\n","      <td>0.608696</td>\n","      <td>0.606378</td>\n","      <td>0.609050</td>\n","      <td>0.607251</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.668200</td>\n","      <td>0.611253</td>\n","      <td>0.603406</td>\n","      <td>0.616022</td>\n","      <td>0.608338</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.63      0.57      0.60       200\n","           1       0.59      0.64      0.62       191\n","\n","    accuracy                           0.61       391\n","   macro avg       0.61      0.61      0.61       391\n","weighted avg       0.61      0.61      0.61       391\n","\n","[[115  85]\n"," [ 68 123]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.61      0.67      0.64       200\n","           1       0.61      0.54      0.58       191\n","\n","    accuracy                           0.61       391\n","   macro avg       0.61      0.61      0.61       391\n","weighted avg       0.61      0.61      0.61       391\n","\n","[[134  66]\n"," [ 87 104]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.60      0.73      0.66       200\n","           1       0.63      0.48      0.55       191\n","\n","    accuracy                           0.61       391\n","   macro avg       0.62      0.61      0.60       391\n","weighted avg       0.62      0.61      0.60       391\n","\n","[[147  53]\n"," [ 99  92]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-12-12 18:43:40,944]\u001b[0m Trial 28 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 28 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5327\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2344' max='5327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2344/5327 06:13 < 07:55, 6.27 it/s, Epoch 3/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.694700</td>\n","      <td>0.691884</td>\n","      <td>0.557545</td>\n","      <td>0.550189</td>\n","      <td>0.565872</td>\n","      <td>0.560785</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.689800</td>\n","      <td>0.691405</td>\n","      <td>0.519182</td>\n","      <td>0.368488</td>\n","      <td>0.591187</td>\n","      <td>0.508207</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.684000</td>\n","      <td>0.673210</td>\n","      <td>0.601023</td>\n","      <td>0.595174</td>\n","      <td>0.611814</td>\n","      <td>0.604110</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.60      0.42      0.49       200\n","           1       0.54      0.70      0.61       191\n","\n","    accuracy                           0.56       391\n","   macro avg       0.57      0.56      0.55       391\n","weighted avg       0.57      0.56      0.55       391\n","\n","[[ 84 116]\n"," [ 57 134]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.52      0.98      0.68       200\n","           1       0.67      0.03      0.06       191\n","\n","    accuracy                           0.52       391\n","   macro avg       0.59      0.51      0.37       391\n","weighted avg       0.59      0.52      0.38       391\n","\n","[[197   3]\n"," [185   6]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.65      0.47      0.55       200\n","           1       0.57      0.74      0.64       191\n","\n","    accuracy                           0.60       391\n","   macro avg       0.61      0.60      0.60       391\n","weighted avg       0.61      0.60      0.59       391\n","\n","[[ 94 106]\n"," [ 50 141]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-12-12 18:50:04,350]\u001b[0m Trial 29 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 29 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3113\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='782' max='3113' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 782/3113 01:59 < 05:56, 6.55 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.117600</td>\n","      <td>1.248005</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 18:52:13,577]\u001b[0m Trial 30 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 30 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 328\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='328' max='328' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [328/328 05:15, Epoch 3/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.658447</td>\n","      <td>0.613811</td>\n","      <td>0.612350</td>\n","      <td>0.613795</td>\n","      <td>0.612723</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.648751</td>\n","      <td>0.629156</td>\n","      <td>0.629146</td>\n","      <td>0.629324</td>\n","      <td>0.629372</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.672306</td>\n","      <td>0.639386</td>\n","      <td>0.633868</td>\n","      <td>0.643952</td>\n","      <td>0.636898</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.691414</td>\n","      <td>0.636829</td>\n","      <td>0.633179</td>\n","      <td>0.639130</td>\n","      <td>0.634869</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.61      0.66      0.64       200\n","           1       0.61      0.57      0.59       191\n","\n","    accuracy                           0.61       391\n","   macro avg       0.61      0.61      0.61       391\n","weighted avg       0.61      0.61      0.61       391\n","\n","[[132  68]\n"," [ 83 108]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.64      0.62      0.63       200\n","           1       0.62      0.64      0.63       191\n","\n","    accuracy                           0.63       391\n","   macro avg       0.63      0.63      0.63       391\n","weighted avg       0.63      0.63      0.63       391\n","\n","[[124  76]\n"," [ 69 122]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.74      0.68       200\n","           1       0.66      0.53      0.59       191\n","\n","    accuracy                           0.64       391\n","   macro avg       0.64      0.64      0.63       391\n","weighted avg       0.64      0.64      0.63       391\n","\n","[[149  51]\n"," [ 90 101]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.63      0.72      0.67       200\n","           1       0.65      0.55      0.60       191\n","\n","    accuracy                           0.64       391\n","   macro avg       0.64      0.63      0.63       391\n","weighted avg       0.64      0.64      0.63       391\n","\n","[[144  56]\n"," [ 86 105]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-12 18:57:32,256]\u001b[0m Trial 31 finished with value: 0.633179175475687 and parameters: {'learning_rate': 2.482091827924726e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.0024601092568661046, 'num_train_epochs': 3.34403559358756}. Best is trial 13 with value: 0.6366386554621848.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 31 finished with value: 0.633179175475687 and parameters: {'learning_rate': 2.482091827924726e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.0024601092568661046, 'num_train_epochs': 3.34403559358756}. Best is trial 13 with value: 0.6366386554621848.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 455\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='455' max='455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [455/455 07:14, Epoch 4/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.687446</td>\n","      <td>0.572890</td>\n","      <td>0.554229</td>\n","      <td>0.595753</td>\n","      <td>0.577906</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.668458</td>\n","      <td>0.634271</td>\n","      <td>0.629580</td>\n","      <td>0.637561</td>\n","      <td>0.632016</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.645724</td>\n","      <td>0.624041</td>\n","      <td>0.620456</td>\n","      <td>0.625761</td>\n","      <td>0.622134</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.659245</td>\n","      <td>0.636829</td>\n","      <td>0.624445</td>\n","      <td>0.650305</td>\n","      <td>0.632984</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.668237</td>\n","      <td>0.629156</td>\n","      <td>0.626960</td>\n","      <td>0.629864</td>\n","      <td>0.627723</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.65      0.36      0.46       200\n","           1       0.54      0.80      0.65       191\n","\n","    accuracy                           0.57       391\n","   macro avg       0.60      0.58      0.55       391\n","weighted avg       0.60      0.57      0.55       391\n","\n","[[ 72 128]\n"," [ 39 152]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.73      0.67       200\n","           1       0.65      0.53      0.59       191\n","\n","    accuracy                           0.63       391\n","   macro avg       0.64      0.63      0.63       391\n","weighted avg       0.64      0.63      0.63       391\n","\n","[[146  54]\n"," [ 89 102]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.70      0.66       200\n","           1       0.64      0.54      0.58       191\n","\n","    accuracy                           0.62       391\n","   macro avg       0.63      0.62      0.62       391\n","weighted avg       0.63      0.62      0.62       391\n","\n","[[141  59]\n"," [ 88 103]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.61      0.80      0.69       200\n","           1       0.69      0.47      0.56       191\n","\n","    accuracy                           0.64       391\n","   macro avg       0.65      0.63      0.62       391\n","weighted avg       0.65      0.64      0.63       391\n","\n","[[160  40]\n"," [102  89]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.69      0.66       200\n","           1       0.64      0.57      0.60       191\n","\n","    accuracy                           0.63       391\n","   macro avg       0.63      0.63      0.63       391\n","weighted avg       0.63      0.63      0.63       391\n","\n","[[138  62]\n"," [ 83 108]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-12 19:04:49,480]\u001b[0m Trial 32 finished with value: 0.6269599489409861 and parameters: {'learning_rate': 1.0895430041659685e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.0030793307766418536, 'num_train_epochs': 4.635265632779619}. Best is trial 13 with value: 0.6366386554621848.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 32 finished with value: 0.6269599489409861 and parameters: {'learning_rate': 1.0895430041659685e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.0030793307766418536, 'num_train_epochs': 4.635265632779619}. Best is trial 13 with value: 0.6366386554621848.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 473\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='473' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/473 01:24 < 05:25, 1.15 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.692909</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 19:06:24,626]\u001b[0m Trial 33 pruned. \u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trial 33 pruned. \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2342\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='712' max='2342' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 712/2342 03:10 < 07:16, 3.73 it/s, Epoch 1.82/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.678106</td>\n","      <td>0.626598</td>\n","      <td>0.620136</td>\n","      <td>0.631288</td>\n","      <td>0.623927</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.61      0.74      0.67       200\n","           1       0.65      0.51      0.57       191\n","\n","    accuracy                           0.63       391\n","   macro avg       0.63      0.62      0.62       391\n","weighted avg       0.63      0.63      0.62       391\n","\n","[[148  52]\n"," [ 94  97]]\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1174' max='2342' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1174/2342 05:16 < 05:15, 3.70 it/s, Epoch 3/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.678106</td>\n","      <td>0.626598</td>\n","      <td>0.620136</td>\n","      <td>0.631288</td>\n","      <td>0.623927</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.687500</td>\n","      <td>0.666478</td>\n","      <td>0.626598</td>\n","      <td>0.618491</td>\n","      <td>0.644521</td>\n","      <td>0.630288</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.643000</td>\n","      <td>0.628921</td>\n","      <td>0.621483</td>\n","      <td>0.614398</td>\n","      <td>0.626408</td>\n","      <td>0.618691</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.70      0.47      0.56       200\n","           1       0.59      0.79      0.67       191\n","\n","    accuracy                           0.63       391\n","   macro avg       0.64      0.63      0.62       391\n","weighted avg       0.65      0.63      0.62       391\n","\n","[[ 94 106]\n"," [ 40 151]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.61      0.74      0.67       200\n","           1       0.65      0.50      0.56       191\n","\n","    accuracy                           0.62       391\n","   macro avg       0.63      0.62      0.61       391\n","weighted avg       0.63      0.62      0.62       391\n","\n","[[148  52]\n"," [ 96  95]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-12 19:11:51,229]\u001b[0m Trial 34 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 34 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 373\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='373' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/373 01:24 < 03:58, 1.15 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.693972</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 19:13:26,341]\u001b[0m Trial 35 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 35 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 508\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='197' max='508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [197/508 01:31 < 02:25, 2.14 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.692905</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 19:15:07,829]\u001b[0m Trial 36 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 36 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 435\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/435 01:24 < 04:52, 1.15 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.697718</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 19:16:42,876]\u001b[0m Trial 37 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 37 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 528\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='528' max='528' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [528/528 08:25, Epoch 5/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.660606</td>\n","      <td>0.611253</td>\n","      <td>0.610129</td>\n","      <td>0.611073</td>\n","      <td>0.610340</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.662962</td>\n","      <td>0.636829</td>\n","      <td>0.636807</td>\n","      <td>0.637472</td>\n","      <td>0.637343</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.651779</td>\n","      <td>0.631714</td>\n","      <td>0.629949</td>\n","      <td>0.632139</td>\n","      <td>0.630458</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.749231</td>\n","      <td>0.618926</td>\n","      <td>0.618886</td>\n","      <td>0.619652</td>\n","      <td>0.619490</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>0.809150</td>\n","      <td>0.629156</td>\n","      <td>0.624400</td>\n","      <td>0.632229</td>\n","      <td>0.626898</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.541900</td>\n","      <td>0.835212</td>\n","      <td>0.618926</td>\n","      <td>0.605551</td>\n","      <td>0.630510</td>\n","      <td>0.615013</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.61      0.65      0.63       200\n","           1       0.61      0.57      0.59       191\n","\n","    accuracy                           0.61       391\n","   macro avg       0.61      0.61      0.61       391\n","weighted avg       0.61      0.61      0.61       391\n","\n","[[130  70]\n"," [ 82 109]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.65      0.61      0.63       200\n","           1       0.62      0.66      0.64       191\n","\n","    accuracy                           0.64       391\n","   macro avg       0.64      0.64      0.64       391\n","weighted avg       0.64      0.64      0.64       391\n","\n","[[123  77]\n"," [ 65 126]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.63      0.69      0.66       200\n","           1       0.64      0.58      0.60       191\n","\n","    accuracy                           0.63       391\n","   macro avg       0.63      0.63      0.63       391\n","weighted avg       0.63      0.63      0.63       391\n","\n","[[137  63]\n"," [ 81 110]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.64      0.59      0.61       200\n","           1       0.60      0.64      0.62       191\n","\n","    accuracy                           0.62       391\n","   macro avg       0.62      0.62      0.62       391\n","weighted avg       0.62      0.62      0.62       391\n","\n","[[119  81]\n"," [ 68 123]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.72      0.67       200\n","           1       0.65      0.53      0.58       191\n","\n","    accuracy                           0.63       391\n","   macro avg       0.63      0.63      0.62       391\n","weighted avg       0.63      0.63      0.63       391\n","\n","[[145  55]\n"," [ 90 101]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.60      0.79      0.68       200\n","           1       0.66      0.45      0.53       191\n","\n","    accuracy                           0.62       391\n","   macro avg       0.63      0.62      0.61       391\n","weighted avg       0.63      0.62      0.61       391\n","\n","[[157  43]\n"," [106  85]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-12 19:25:11,504]\u001b[0m Trial 38 finished with value: 0.6055505528209781 and parameters: {'learning_rate': 1.7126909465601443e-05, 'per_device_train_batch_size': 16, 'weight_decay': 1.7275485042756645e-06, 'num_train_epochs': 5.38631612164996}. Best is trial 13 with value: 0.6366386554621848.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 38 finished with value: 0.6055505528209781 and parameters: {'learning_rate': 1.7126909465601443e-05, 'per_device_train_batch_size': 16, 'weight_decay': 1.7275485042756645e-06, 'num_train_epochs': 5.38631612164996}. Best is trial 13 with value: 0.6366386554621848.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2444\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='392' max='2444' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 392/2444 01:40 < 08:46, 3.90 it/s, Epoch 1/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>135.295609</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 19:27:01,641]\u001b[0m Trial 39 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 39 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 8\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1519\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='197' max='1519' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 197/1519 01:31 < 10:18, 2.14 it/s, Epoch 1/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.693260</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 19:28:43,135]\u001b[0m Trial 40 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 40 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 306\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/306 01:24 < 02:59, 1.15 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.692863</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 19:30:18,189]\u001b[0m Trial 41 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 41 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 267\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='268' max='267' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [267/267 04:07, Epoch 2.72/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.685204</td>\n","      <td>0.621483</td>\n","      <td>0.620388</td>\n","      <td>0.625113</td>\n","      <td>0.623050</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.671300</td>\n","      <td>0.613811</td>\n","      <td>0.601023</td>\n","      <td>0.623858</td>\n","      <td>0.610013</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.644448</td>\n","      <td>0.616368</td>\n","      <td>0.612902</td>\n","      <td>0.617708</td>\n","      <td>0.614516</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.65      0.56      0.60       200\n","           1       0.60      0.69      0.64       191\n","\n","    accuracy                           0.62       391\n","   macro avg       0.63      0.62      0.62       391\n","weighted avg       0.63      0.62      0.62       391\n","\n","[[111  89]\n"," [ 59 132]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.59      0.78      0.67       200\n","           1       0.65      0.45      0.53       191\n","\n","    accuracy                           0.61       391\n","   macro avg       0.62      0.61      0.60       391\n","weighted avg       0.62      0.61      0.60       391\n","\n","[[155  45]\n"," [106  85]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.61      0.69      0.65       200\n","           1       0.63      0.53      0.58       191\n","\n","    accuracy                           0.62       391\n","   macro avg       0.62      0.61      0.61       391\n","weighted avg       0.62      0.62      0.61       391\n","\n","[[139  61]\n"," [ 89 102]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-12 19:34:36,504]\u001b[0m Trial 42 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 42 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 352\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='352' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/352 01:24 < 03:39, 1.15 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.693967</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 19:36:11,581]\u001b[0m Trial 43 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 43 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 236\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='236' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/236 01:24 < 01:59, 1.15 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.709101</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 19:37:46,710]\u001b[0m Trial 44 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 44 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 425\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/425 01:24 < 04:43, 1.15 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.693007</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 19:39:21,678]\u001b[0m Trial 45 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 45 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 493\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='493' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/493 01:24 < 05:42, 1.15 it/s, Epoch 1/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.694079</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 19:40:56,854]\u001b[0m Trial 46 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 46 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2517\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='782' max='2517' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 782/2517 01:59 < 04:25, 6.54 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.709500</td>\n","      <td>0.692908</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 19:43:06,194]\u001b[0m Trial 47 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 47 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 361\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='361' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/361 01:24 < 03:47, 1.15 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.692911</td>\n","      <td>0.511509</td>\n","      <td>0.338409</td>\n","      <td>0.255754</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      1.00      0.68       200\n","           1       0.00      0.00      0.00       191\n","\n","    accuracy                           0.51       391\n","   macro avg       0.26      0.50      0.34       391\n","weighted avg       0.26      0.51      0.35       391\n","\n","[[200   0]\n"," [191   0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 19:44:41,417]\u001b[0m Trial 48 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 48 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.13.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 1561\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 277\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99' max='277' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 99/277 01:24 < 02:34, 1.15 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.693635</td>\n","      <td>0.488491</td>\n","      <td>0.328179</td>\n","      <td>0.244246</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 391\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       200\n","           1       0.49      1.00      0.66       191\n","\n","    accuracy                           0.49       391\n","   macro avg       0.24      0.50      0.33       391\n","weighted avg       0.24      0.49      0.32       391\n","\n","[[  0 200]\n"," [  0 191]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-12 19:46:16,604]\u001b[0m Trial 49 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 49 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-12 19:46:16,642]\u001b[0m A new study created in memory with name: no-name-39219a64-861d-4682-a20e-1ba445b59167\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["A new study created in memory with name: no-name-39219a64-861d-4682-a20e-1ba445b59167\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"rHREpYlgsNkL","executionInfo":{"status":"ok","timestamp":1639338175157,"user_tz":-60,"elapsed":5,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"69293705-0097-420f-ebab-35614355ac07"},"source":["storage_name"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'sqlite:///RoBERTa_MASK_NS.db'"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"vlqXIjF6sPqH","executionInfo":{"status":"ok","timestamp":1639338175158,"user_tz":-60,"elapsed":5,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"3a00af86-afc0-4e72-8823-c849c747b69e"},"source":["study_name"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'RoBERTa_MASK_NS'"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mXXqQglwpuZy","executionInfo":{"status":"ok","timestamp":1639338175856,"user_tz":-60,"elapsed":702,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"8bd544fa-44da-4275-c3e0-3980de5f1231"},"source":["study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True, direction=\"maximize\")\n","df = study.trials_dataframe(attrs=(\"number\", \"value\", \"params\", \"state\"))"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-12 19:46:16,753]\u001b[0m Using an existing study with name 'RoBERTa_MASK_NS' instead of creating a new one.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Using an existing study with name 'RoBERTa_MASK_NS' instead of creating a new one.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"JzU25SO_tKCP","executionInfo":{"status":"ok","timestamp":1639338175858,"user_tz":-60,"elapsed":5,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"08c93015-17ed-4ebd-cf57-2dca4f644405"},"source":["df"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>number</th>\n","      <th>value</th>\n","      <th>params_learning_rate</th>\n","      <th>params_num_train_epochs</th>\n","      <th>params_per_device_train_batch_size</th>\n","      <th>params_weight_decay</th>\n","      <th>state</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.338409</td>\n","      <td>1.069600e-05</td>\n","      <td>7.722648</td>\n","      <td>2</td>\n","      <td>8.320151e-08</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.338409</td>\n","      <td>4.221777e-01</td>\n","      <td>5.688071</td>\n","      <td>4</td>\n","      <td>4.132608e-02</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.440629</td>\n","      <td>2.440031e-06</td>\n","      <td>2.744239</td>\n","      <td>8</td>\n","      <td>5.252612e-07</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.634118</td>\n","      <td>1.081392e-05</td>\n","      <td>2.981916</td>\n","      <td>16</td>\n","      <td>3.509982e-09</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.616366</td>\n","      <td>2.107441e-06</td>\n","      <td>2.571573</td>\n","      <td>4</td>\n","      <td>4.113694e-11</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>0.328179</td>\n","      <td>2.308410e-06</td>\n","      <td>4.656619</td>\n","      <td>16</td>\n","      <td>4.119576e-11</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>0.328179</td>\n","      <td>5.303481e-03</td>\n","      <td>7.334966</td>\n","      <td>8</td>\n","      <td>1.188048e-01</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>0.338409</td>\n","      <td>8.714368e-05</td>\n","      <td>1.361011</td>\n","      <td>2</td>\n","      <td>2.285913e-04</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>0.625451</td>\n","      <td>3.017222e-05</td>\n","      <td>3.355907</td>\n","      <td>16</td>\n","      <td>3.480614e-03</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>0.338409</td>\n","      <td>3.571422e-04</td>\n","      <td>1.173250</td>\n","      <td>8</td>\n","      <td>1.629541e-11</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>0.328179</td>\n","      <td>6.465548e-08</td>\n","      <td>1.815984</td>\n","      <td>16</td>\n","      <td>2.863498e-09</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>0.338409</td>\n","      <td>7.055705e-04</td>\n","      <td>3.624568</td>\n","      <td>16</td>\n","      <td>2.184359e-04</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>0.328179</td>\n","      <td>6.061709e-08</td>\n","      <td>2.142344</td>\n","      <td>16</td>\n","      <td>5.851830e-05</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>0.636639</td>\n","      <td>2.913665e-05</td>\n","      <td>3.972828</td>\n","      <td>16</td>\n","      <td>1.094642e-08</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>0.328179</td>\n","      <td>3.581283e-03</td>\n","      <td>4.130220</td>\n","      <td>16</td>\n","      <td>4.924460e-09</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>15</td>\n","      <td>0.328179</td>\n","      <td>4.901876e-07</td>\n","      <td>5.243057</td>\n","      <td>16</td>\n","      <td>1.850170e-09</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>0.328179</td>\n","      <td>3.185627e-02</td>\n","      <td>1.827557</td>\n","      <td>16</td>\n","      <td>5.339320e-06</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>0.338409</td>\n","      <td>3.613061e-05</td>\n","      <td>3.458876</td>\n","      <td>4</td>\n","      <td>5.825449e-08</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>18</td>\n","      <td>0.328179</td>\n","      <td>1.008916e-05</td>\n","      <td>2.447661</td>\n","      <td>2</td>\n","      <td>2.173458e-08</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>19</td>\n","      <td>0.338409</td>\n","      <td>8.458262e-04</td>\n","      <td>5.752062</td>\n","      <td>16</td>\n","      <td>2.442458e-10</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>20</td>\n","      <td>0.328179</td>\n","      <td>4.315064e-07</td>\n","      <td>3.194330</td>\n","      <td>16</td>\n","      <td>3.950540e-06</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>21</td>\n","      <td>0.338409</td>\n","      <td>4.461499e-05</td>\n","      <td>3.987420</td>\n","      <td>16</td>\n","      <td>2.974428e-02</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>22</td>\n","      <td>0.626331</td>\n","      <td>1.355886e-05</td>\n","      <td>3.087744</td>\n","      <td>16</td>\n","      <td>1.605853e-03</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>23</td>\n","      <td>0.324698</td>\n","      <td>9.692696e-06</td>\n","      <td>2.955205</td>\n","      <td>16</td>\n","      <td>5.288769e-07</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>24</td>\n","      <td>0.328179</td>\n","      <td>1.748213e-04</td>\n","      <td>2.178480</td>\n","      <td>16</td>\n","      <td>4.042873e-10</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>25</td>\n","      <td>0.328179</td>\n","      <td>4.842497e-07</td>\n","      <td>4.587269</td>\n","      <td>16</td>\n","      <td>4.582443e-03</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>26</td>\n","      <td>0.332537</td>\n","      <td>4.763248e-06</td>\n","      <td>1.699574</td>\n","      <td>8</td>\n","      <td>4.632020e-05</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>27</td>\n","      <td>0.338409</td>\n","      <td>1.623991e-04</td>\n","      <td>2.218117</td>\n","      <td>4</td>\n","      <td>1.957650e-08</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>28</td>\n","      <td>0.603406</td>\n","      <td>2.327762e-05</td>\n","      <td>2.925687</td>\n","      <td>16</td>\n","      <td>2.397896e-10</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>29</td>\n","      <td>0.595174</td>\n","      <td>9.277448e-07</td>\n","      <td>6.819622</td>\n","      <td>2</td>\n","      <td>5.819704e-12</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>30</td>\n","      <td>0.328179</td>\n","      <td>1.913198e-03</td>\n","      <td>3.985043</td>\n","      <td>2</td>\n","      <td>1.895410e-07</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>31</td>\n","      <td>0.633179</td>\n","      <td>2.482092e-05</td>\n","      <td>3.344036</td>\n","      <td>16</td>\n","      <td>2.460109e-03</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>32</td>\n","      <td>0.626960</td>\n","      <td>1.089543e-05</td>\n","      <td>4.635266</td>\n","      <td>16</td>\n","      <td>3.079331e-03</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>33</td>\n","      <td>0.338409</td>\n","      <td>7.478728e-05</td>\n","      <td>4.824541</td>\n","      <td>16</td>\n","      <td>1.681594e-02</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>34</td>\n","      <td>0.614398</td>\n","      <td>5.630319e-06</td>\n","      <td>5.987619</td>\n","      <td>4</td>\n","      <td>3.377558e-01</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>35</td>\n","      <td>0.328179</td>\n","      <td>1.529388e-06</td>\n","      <td>3.798592</td>\n","      <td>16</td>\n","      <td>1.126955e-05</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>36</td>\n","      <td>0.328179</td>\n","      <td>4.279708e-06</td>\n","      <td>2.587962</td>\n","      <td>8</td>\n","      <td>6.474546e-04</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>37</td>\n","      <td>0.328179</td>\n","      <td>1.840175e-07</td>\n","      <td>4.438393</td>\n","      <td>16</td>\n","      <td>2.574053e-07</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>38</td>\n","      <td>0.605551</td>\n","      <td>1.712691e-05</td>\n","      <td>5.386316</td>\n","      <td>16</td>\n","      <td>1.727549e-06</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>39</td>\n","      <td>0.328179</td>\n","      <td>3.108127e-01</td>\n","      <td>6.248878</td>\n","      <td>4</td>\n","      <td>8.457392e-03</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>40</td>\n","      <td>0.338409</td>\n","      <td>1.159043e-04</td>\n","      <td>7.747279</td>\n","      <td>8</td>\n","      <td>6.291645e-02</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>41</td>\n","      <td>0.338409</td>\n","      <td>5.236121e-05</td>\n","      <td>3.113743</td>\n","      <td>16</td>\n","      <td>1.486562e-03</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>42</td>\n","      <td>0.612902</td>\n","      <td>1.579231e-05</td>\n","      <td>2.724342</td>\n","      <td>16</td>\n","      <td>5.872438e-04</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>43</td>\n","      <td>0.328179</td>\n","      <td>2.496945e-06</td>\n","      <td>3.582169</td>\n","      <td>16</td>\n","      <td>8.783547e-05</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>44</td>\n","      <td>0.328179</td>\n","      <td>4.204557e-04</td>\n","      <td>2.400666</td>\n","      <td>16</td>\n","      <td>1.056232e-03</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>45</td>\n","      <td>0.328179</td>\n","      <td>7.286118e-06</td>\n","      <td>4.331550</td>\n","      <td>16</td>\n","      <td>4.585683e-01</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>46</td>\n","      <td>0.328179</td>\n","      <td>2.451917e-06</td>\n","      <td>5.026653</td>\n","      <td>16</td>\n","      <td>1.122247e-09</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>47</td>\n","      <td>0.338409</td>\n","      <td>1.919748e-05</td>\n","      <td>3.221894</td>\n","      <td>2</td>\n","      <td>1.162791e-08</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>48</td>\n","      <td>0.338409</td>\n","      <td>2.944015e-04</td>\n","      <td>3.679597</td>\n","      <td>16</td>\n","      <td>2.388674e-05</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>49</td>\n","      <td>0.328179</td>\n","      <td>6.151214e-05</td>\n","      <td>2.821522</td>\n","      <td>16</td>\n","      <td>2.396018e-04</td>\n","      <td>PRUNED</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    number     value  ...  params_weight_decay     state\n","0        0  0.338409  ...         8.320151e-08  COMPLETE\n","1        1  0.338409  ...         4.132608e-02  COMPLETE\n","2        2  0.440629  ...         5.252612e-07  COMPLETE\n","3        3  0.634118  ...         3.509982e-09  COMPLETE\n","4        4  0.616366  ...         4.113694e-11  COMPLETE\n","5        5  0.328179  ...         4.119576e-11    PRUNED\n","6        6  0.328179  ...         1.188048e-01    PRUNED\n","7        7  0.338409  ...         2.285913e-04    PRUNED\n","8        8  0.625451  ...         3.480614e-03  COMPLETE\n","9        9  0.338409  ...         1.629541e-11    PRUNED\n","10      10  0.328179  ...         2.863498e-09    PRUNED\n","11      11  0.338409  ...         2.184359e-04    PRUNED\n","12      12  0.328179  ...         5.851830e-05    PRUNED\n","13      13  0.636639  ...         1.094642e-08  COMPLETE\n","14      14  0.328179  ...         4.924460e-09    PRUNED\n","15      15  0.328179  ...         1.850170e-09    PRUNED\n","16      16  0.328179  ...         5.339320e-06    PRUNED\n","17      17  0.338409  ...         5.825449e-08    PRUNED\n","18      18  0.328179  ...         2.173458e-08    PRUNED\n","19      19  0.338409  ...         2.442458e-10    PRUNED\n","20      20  0.328179  ...         3.950540e-06    PRUNED\n","21      21  0.338409  ...         2.974428e-02    PRUNED\n","22      22  0.626331  ...         1.605853e-03  COMPLETE\n","23      23  0.324698  ...         5.288769e-07    PRUNED\n","24      24  0.328179  ...         4.042873e-10    PRUNED\n","25      25  0.328179  ...         4.582443e-03    PRUNED\n","26      26  0.332537  ...         4.632020e-05    PRUNED\n","27      27  0.338409  ...         1.957650e-08    PRUNED\n","28      28  0.603406  ...         2.397896e-10    PRUNED\n","29      29  0.595174  ...         5.819704e-12    PRUNED\n","30      30  0.328179  ...         1.895410e-07    PRUNED\n","31      31  0.633179  ...         2.460109e-03  COMPLETE\n","32      32  0.626960  ...         3.079331e-03  COMPLETE\n","33      33  0.338409  ...         1.681594e-02    PRUNED\n","34      34  0.614398  ...         3.377558e-01    PRUNED\n","35      35  0.328179  ...         1.126955e-05    PRUNED\n","36      36  0.328179  ...         6.474546e-04    PRUNED\n","37      37  0.328179  ...         2.574053e-07    PRUNED\n","38      38  0.605551  ...         1.727549e-06  COMPLETE\n","39      39  0.328179  ...         8.457392e-03    PRUNED\n","40      40  0.338409  ...         6.291645e-02    PRUNED\n","41      41  0.338409  ...         1.486562e-03    PRUNED\n","42      42  0.612902  ...         5.872438e-04    PRUNED\n","43      43  0.328179  ...         8.783547e-05    PRUNED\n","44      44  0.328179  ...         1.056232e-03    PRUNED\n","45      45  0.328179  ...         4.585683e-01    PRUNED\n","46      46  0.328179  ...         1.122247e-09    PRUNED\n","47      47  0.338409  ...         1.162791e-08    PRUNED\n","48      48  0.338409  ...         2.388674e-05    PRUNED\n","49      49  0.328179  ...         2.396018e-04    PRUNED\n","\n","[50 rows x 7 columns]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"ef13tgLLtXxY","executionInfo":{"status":"ok","timestamp":1639338176869,"user_tz":-60,"elapsed":1014,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"82c00e66-a830-409f-d962-d0061e752857"},"source":["fig = optuna.visualization.plot_param_importances(study)\n","fig.show()"],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"6770ec4a-802a-4eef-96c0-a48c506396db\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"6770ec4a-802a-4eef-96c0-a48c506396db\")) {\n","                    Plotly.newPlot(\n","                        '6770ec4a-802a-4eef-96c0-a48c506396db',\n","                        [{\"cliponaxis\": false, \"hovertemplate\": [\"learning_rate (LogUniformDistribution): 0.026253995254071995<extra></extra>\", \"weight_decay (LogUniformDistribution): 0.029520444804895133<extra></extra>\", \"num_train_epochs (LogUniformDistribution): 0.33112498562507786<extra></extra>\", \"per_device_train_batch_size (CategoricalDistribution): 0.613100574315955<extra></extra>\"], \"marker\": {\"color\": \"rgb(66,146,198)\"}, \"orientation\": \"h\", \"text\": [\"0.026253995254071995\", \"0.029520444804895133\", \"0.33112498562507786\", \"0.613100574315955\"], \"textposition\": \"outside\", \"texttemplate\": \"%{text:.2f}\", \"type\": \"bar\", \"x\": [0.026253995254071995, 0.029520444804895133, 0.33112498562507786, 0.613100574315955], \"y\": [\"learning_rate\", \"weight_decay\", \"num_train_epochs\", \"per_device_train_batch_size\"]}],\n","                        {\"showlegend\": false, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Hyperparameter Importances\"}, \"xaxis\": {\"title\": {\"text\": \"Importance for Objective Value\"}}, \"yaxis\": {\"title\": {\"text\": \"Hyperparameter\"}}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('6770ec4a-802a-4eef-96c0-a48c506396db');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D8f8-e0Us-Yd","executionInfo":{"status":"ok","timestamp":1639338176871,"user_tz":-60,"elapsed":9,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"409285f4-7fad-425d-880f-86988f7c81b5"},"source":["best_run"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BestRun(run_id='13', objective=0.6366386554621848, hyperparameters={'learning_rate': 2.9136645854693335e-05, 'num_train_epochs': 3.9728281289117455, 'per_device_train_batch_size': 16, 'weight_decay': 1.0946416634758784e-08})"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"Hr5zSnMFnAfY","executionInfo":{"status":"ok","timestamp":1639338177211,"user_tz":-60,"elapsed":344,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"884b20e3-b9bf-4ac1-c39c-72721e15138a"},"source":["optuna.visualization.plot_intermediate_values(study)"],"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"afb99e8b-1e9a-4c40-a853-99698490dff5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"afb99e8b-1e9a-4c40-a853-99698490dff5\")) {\n","                    Plotly.newPlot(\n","                        'afb99e8b-1e9a-4c40-a853-99698490dff5',\n","                        [{\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial0\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7], \"y\": [0.3384094754653131, 0.3384094754653131, 0.3281786941580756, 0.3384094754653131, 0.3384094754653131, 0.3281786941580756, 0.3384094754653131, 0.3384094754653131]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial1\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5], \"y\": [0.3281786941580756, 0.3281786941580756, 0.3384094754653131, 0.3281786941580756, 0.3384094754653131, 0.3384094754653131]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial2\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.3337186699891249, 0.3487446525001753, 0.4406294706723891]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial3\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.3772683397683398, 0.5790365163723642, 0.6341179319054817]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial4\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.5245037741123846, 0.3765137867726282, 0.6163657770800628]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial5\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial6\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.3384094754653131, 0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial7\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.3384094754653131, 0.3384094754653131]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial8\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3], \"y\": [0.608439531093526, 0.6044975582960994, 0.6321053289555931, 0.6254513300420013]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial9\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3384094754653131]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial10\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial11\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3384094754653131]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial12\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial13\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3], \"y\": [0.4729467084639498, 0.6211764705882352, 0.646275348824924, 0.6366386554621848]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial14\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial15\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial16\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial17\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3384094754653131]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial18\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial19\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3384094754653131]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial20\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial21\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.5662291488451668, 0.3384094754653131, 0.3384094754653131]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial22\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3], \"y\": [0.6290686643331479, 0.6002559293689107, 0.6263305322128851, 0.6263305322128851]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial23\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.32469775474956825]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial24\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial25\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial26\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.33253670194605667]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial27\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3384094754653131]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial28\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.6085317732974734, 0.6063784288825576, 0.6034059363655777]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial29\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.5501891861338866, 0.368487972508591, 0.5951736221726664]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial30\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial31\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3], \"y\": [0.6123502183119398, 0.6291463071619668, 0.6338683862741321, 0.633179175475687]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial32\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4], \"y\": [0.5542288760999721, 0.6295803107091986, 0.6204560312209038, 0.6244453463203463, 0.6269599489409861]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial33\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3384094754653131]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial34\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.6201357466063349, 0.6184906971770745, 0.6143984220907298]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial35\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial36\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial37\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial38\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5], \"y\": [0.6101285751771188, 0.6368072635930713, 0.629948998370051, 0.6188859451149706, 0.6243996157540825, 0.6055505528209781]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial39\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial40\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3384094754653131]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial41\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3384094754653131]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial42\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.6203883495145631, 0.6010231043174461, 0.6129019483605258]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial43\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial44\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial45\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial46\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial47\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3384094754653131]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial48\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3384094754653131]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial49\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3281786941580756]}],\n","                        {\"showlegend\": false, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Intermediate Values Plot\"}, \"xaxis\": {\"title\": {\"text\": \"Step\"}}, \"yaxis\": {\"title\": {\"text\": \"Intermediate Value\"}}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('afb99e8b-1e9a-4c40-a853-99698490dff5');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"uFBmj8ysm9pb","executionInfo":{"status":"ok","timestamp":1639338177462,"user_tz":-60,"elapsed":261,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"e3ca7de4-7cd7-4258-d176-8df4ef5c27cf"},"source":["optuna.visualization.plot_parallel_coordinate(study)"],"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"05f57ed7-7c66-4078-a15b-8e96e7d8b136\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"05f57ed7-7c66-4078-a15b-8e96e7d8b136\")) {\n","                    Plotly.newPlot(\n","                        '05f57ed7-7c66-4078-a15b-8e96e7d8b136',\n","                        [{\"dimensions\": [{\"label\": \"Objective Value\", \"range\": [0.3384094754653131, 0.6366386554621848], \"values\": [0.3384094754653131, 0.3384094754653131, 0.6163657770800628, 0.4406294706723891, 0.6341179319054817, 0.6254513300420013, 0.6366386554621848, 0.6263305322128851, 0.633179175475687, 0.6269599489409861, 0.6055505528209781]}, {\"label\": \"learning_rate\", \"range\": [-5.6762445730562865, -0.37450472595400264], \"ticktext\": [\"2.11e-06\", \"1e-05\", \"0.0001\", \"0.001\", \"0.01\", \"0.1\", \"0.422\"], \"tickvals\": [-5.6762445730562865, -5, -4, -3, -2, -1, -0.37450472595400264], \"values\": [-4.970778489706839, -0.37450472595400264, -5.6762445730562865, -5.612604642630252, -4.966017045545438, -4.520392791596531, -4.535560444695069, -4.86777677861306, -4.605182155301724, -4.9627556235146555, -4.766320997987652]}, {\"label\": \"num_train_epochs\", \"range\": [0.41019894026245624, 0.8877662178364456], \"ticktext\": [\"2.57\", \"7.72\"], \"tickvals\": [0.41019894026245624, 0.8877662178364456], \"values\": [0.8877662178364456, 0.754964997394335, 0.41019894026245624, 0.4384218890943017, 0.47449538785679957, 0.5258098805733888, 0.5990997771671709, 0.4896413381993259, 0.5242708913795389, 0.6660746272711942, 0.7312918384663852]}, {\"label\": \"per_device_train_...\", \"range\": [0, 3], \"ticktext\": [2, 4, 8, 16], \"tickvals\": [0, 1, 2, 3], \"values\": [0, 1, 1, 2, 3, 3, 3, 3, 3, 3, 3]}, {\"label\": \"weight_decay\", \"range\": [-10.385768064286275, -1.3837757766956094], \"ticktext\": [\"4.11e-11\", \"1e-10\", \"1e-09\", \"1e-08\", \"1e-07\", \"1e-06\", \"1e-05\", \"0.0001\", \"0.001\", \"0.01\", \"0.0413\"], \"tickvals\": [-10.385768064286275, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1.3837757766956094], \"values\": [-7.079868805958513, -1.3837757766956094, -10.385768064286275, -6.279624681447139, -8.454695111119397, -2.458344176230719, -7.960728026069887, -2.794294326772992, -2.6090456048476813, -2.5115436573928527, -5.762569750122479]}], \"labelangle\": 30, \"labelside\": \"bottom\", \"line\": {\"color\": [0.3384094754653131, 0.3384094754653131, 0.6163657770800628, 0.4406294706723891, 0.6341179319054817, 0.6254513300420013, 0.6366386554621848, 0.6263305322128851, 0.633179175475687, 0.6269599489409861, 0.6055505528209781], \"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0.0, \"rgb(247,251,255)\"], [0.125, \"rgb(222,235,247)\"], [0.25, \"rgb(198,219,239)\"], [0.375, \"rgb(158,202,225)\"], [0.5, \"rgb(107,174,214)\"], [0.625, \"rgb(66,146,198)\"], [0.75, \"rgb(33,113,181)\"], [0.875, \"rgb(8,81,156)\"], [1.0, \"rgb(8,48,107)\"]], \"reversescale\": false, \"showscale\": true}, \"type\": \"parcoords\"}],\n","                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Parallel Coordinate Plot\"}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('05f57ed7-7c66-4078-a15b-8e96e7d8b136');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"I4B4vAEnDimm","executionInfo":{"status":"ok","timestamp":1639338177732,"user_tz":-60,"elapsed":270,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"eb0e3b99-d3b6-4431-add7-12ac4fadfe05"},"source":["optuna.visualization.plot_optimization_history(study)"],"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"08564596-c868-444c-9882-3a7a285ad923\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"08564596-c868-444c-9882-3a7a285ad923\")) {\n","                    Plotly.newPlot(\n","                        '08564596-c868-444c-9882-3a7a285ad923',\n","                        [{\"mode\": \"markers\", \"name\": \"Objective Value\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 8, 13, 22, 31, 32, 38], \"y\": [0.3384094754653131, 0.3384094754653131, 0.4406294706723891, 0.6341179319054817, 0.6163657770800628, 0.6254513300420013, 0.6366386554621848, 0.6263305322128851, 0.633179175475687, 0.6269599489409861, 0.6055505528209781]}, {\"name\": \"Best Value\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 8, 13, 22, 31, 32, 38], \"y\": [0.3384094754653131, 0.3384094754653131, 0.4406294706723891, 0.6341179319054817, 0.6341179319054817, 0.6341179319054817, 0.6366386554621848, 0.6366386554621848, 0.6366386554621848, 0.6366386554621848, 0.6366386554621848]}],\n","                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Optimization History Plot\"}, \"xaxis\": {\"title\": {\"text\": \"#Trials\"}}, \"yaxis\": {\"title\": {\"text\": \"Objective Value\"}}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('08564596-c868-444c-9882-3a7a285ad923');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"_S9z32VznLsH","executionInfo":{"status":"ok","timestamp":1639338178593,"user_tz":-60,"elapsed":862,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"33f46fed-cee7-459e-86ca-73df6497c1fe"},"source":["optuna.visualization.plot_contour(study)"],"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"2e96c2bc-85ea-4303-bcce-8df1041e35c2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"2e96c2bc-85ea-4303-bcce-8df1041e35c2\")) {\n","                    Plotly.newPlot(\n","                        '2e96c2bc-85ea-4303-bcce-8df1041e35c2',\n","                        [{\"type\": \"scatter\", \"xaxis\": \"x\", \"yaxis\": \"y\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": true, \"type\": \"contour\", \"x\": [1.1446387246708548e-06, 2.1074410097143765e-06, 2.4400310752316386e-06, 1.0696002857866855e-05, 1.0813915071256551e-05, 1.0895430041659685e-05, 1.3558861400977725e-05, 1.7126909465601443e-05, 2.482091827924726e-05, 2.9136645854693335e-05, 3.01722160191678e-05, 0.4221776849260434, 0.7772885430337508], \"xaxis\": \"x5\", \"y\": [2.4340000139535634, 2.571573491376362, 2.7442387285254384, 2.981915880251583, 3.087744367688028, 3.34403559358756, 3.355906718431561, 3.9728281289117455, 4.635265632779619, 5.38631612164996, 5.688070851486295, 7.722647604976535, 8.159143693652293], \"yaxis\": \"y5\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null], [null, 0.6163657770800628, null, null, null, null, null, null, null, null, null, null, null], [null, null, 0.4406294706723891, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, 0.6341179319054817, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, 0.6263305322128851, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, 0.633179175475687, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, 0.6254513300420013, null, null], [null, null, null, null, null, null, null, null, null, 0.6366386554621848, null, null, null], [null, null, null, null, null, 0.6269599489409861, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, 0.6055505528209781, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, 0.3384094754653131, null], [null, null, null, 0.3384094754653131, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [1.0696002857866855e-05, 0.4221776849260434, 2.4400310752316386e-06, 1.0813915071256551e-05, 2.1074410097143765e-06, 3.01722160191678e-05, 2.9136645854693335e-05, 1.3558861400977725e-05, 2.482091827924726e-05, 1.0895430041659685e-05, 1.7126909465601443e-05], \"xaxis\": \"x5\", \"y\": [7.722647604976535, 5.688070851486295, 2.7442387285254384, 2.981915880251583, 2.571573491376362, 3.355906718431561, 3.9728281289117455, 3.087744367688028, 3.34403559358756, 4.635265632779619, 5.38631612164996], \"yaxis\": \"y5\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.1446387246708548e-06, 2.1074410097143765e-06, 2.4400310752316386e-06, 1.0696002857866855e-05, 1.0813915071256551e-05, 1.0895430041659685e-05, 1.3558861400977725e-05, 1.7126909465601443e-05, 2.482091827924726e-05, 2.9136645854693335e-05, 3.01722160191678e-05, 0.4221776849260434, 0.7772885430337508], \"xaxis\": \"x9\", \"y\": [1.2999999999999998, 2, 4, 8, 16, 16.7], \"yaxis\": \"y9\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, 0.3384094754653131, null, null, null, null, null, null, null, null, null], [null, 0.6163657770800628, null, null, null, null, null, null, null, null, null, 0.3384094754653131, null], [null, null, 0.4406294706723891, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, 0.6341179319054817, 0.6269599489409861, 0.6263305322128851, 0.6055505528209781, 0.633179175475687, 0.6366386554621848, 0.6254513300420013, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [1.0696002857866855e-05, 0.4221776849260434, 2.4400310752316386e-06, 1.0813915071256551e-05, 2.1074410097143765e-06, 3.01722160191678e-05, 2.9136645854693335e-05, 1.3558861400977725e-05, 2.482091827924726e-05, 1.0895430041659685e-05, 1.7126909465601443e-05], \"xaxis\": \"x9\", \"y\": [2, 4, 8, 16, 4, 16, 16, 16, 16, 16, 16], \"yaxis\": \"y9\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.1446387246708548e-06, 2.1074410097143765e-06, 2.4400310752316386e-06, 1.0696002857866855e-05, 1.0813915071256551e-05, 1.0895430041659685e-05, 1.3558861400977725e-05, 1.7126909465601443e-05, 2.482091827924726e-05, 2.9136645854693335e-05, 3.01722160191678e-05, 0.4221776849260434, 0.7772885430337508], \"xaxis\": \"x13\", \"y\": [1.4592588011731288e-11, 4.113693549617035e-11, 3.5099819965382977e-09, 1.0946416634758784e-08, 8.320150727863287e-08, 5.252611964132717e-07, 1.7275485042756645e-06, 0.0016058525778814324, 0.0024601092568661046, 0.0030793307766418536, 0.0034806136879271246, 0.041326081068267045, 0.11649943997926332], \"yaxis\": \"y13\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null], [null, 0.6163657770800628, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, 0.6341179319054817, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, 0.6366386554621848, null, null, null], [null, null, null, 0.3384094754653131, null, null, null, null, null, null, null, null, null], [null, null, 0.4406294706723891, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, 0.6055505528209781, null, null, null, null, null], [null, null, null, null, null, null, 0.6263305322128851, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, 0.633179175475687, null, null, null, null], [null, null, null, null, null, 0.6269599489409861, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, 0.6254513300420013, null, null], [null, null, null, null, null, null, null, null, null, null, null, 0.3384094754653131, null], [null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [1.0696002857866855e-05, 0.4221776849260434, 2.4400310752316386e-06, 1.0813915071256551e-05, 2.1074410097143765e-06, 3.01722160191678e-05, 2.9136645854693335e-05, 1.3558861400977725e-05, 2.482091827924726e-05, 1.0895430041659685e-05, 1.7126909465601443e-05], \"xaxis\": \"x13\", \"y\": [8.320150727863287e-08, 0.041326081068267045, 5.252611964132717e-07, 3.5099819965382977e-09, 4.113693549617035e-11, 0.0034806136879271246, 1.0946416634758784e-08, 0.0016058525778814324, 0.0024601092568661046, 0.0030793307766418536, 1.7275485042756645e-06], \"yaxis\": \"y13\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [2.4340000139535634, 2.571573491376362, 2.7442387285254384, 2.981915880251583, 3.087744367688028, 3.34403559358756, 3.355906718431561, 3.9728281289117455, 4.635265632779619, 5.38631612164996, 5.688070851486295, 7.722647604976535, 8.159143693652293], \"xaxis\": \"x2\", \"y\": [1.1446387246708548e-06, 2.1074410097143765e-06, 2.4400310752316386e-06, 1.0696002857866855e-05, 1.0813915071256551e-05, 1.0895430041659685e-05, 1.3558861400977725e-05, 1.7126909465601443e-05, 2.482091827924726e-05, 2.9136645854693335e-05, 3.01722160191678e-05, 0.4221776849260434, 0.7772885430337508], \"yaxis\": \"y2\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null], [null, 0.6163657770800628, null, null, null, null, null, null, null, null, null, null, null], [null, null, 0.4406294706723891, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, 0.3384094754653131, null], [null, null, null, 0.6341179319054817, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, 0.6269599489409861, null, null, null, null], [null, null, null, null, 0.6263305322128851, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, 0.6055505528209781, null, null, null], [null, null, null, null, null, 0.633179175475687, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, 0.6366386554621848, null, null, null, null, null], [null, null, null, null, null, null, 0.6254513300420013, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, 0.3384094754653131, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [7.722647604976535, 5.688070851486295, 2.7442387285254384, 2.981915880251583, 2.571573491376362, 3.355906718431561, 3.9728281289117455, 3.087744367688028, 3.34403559358756, 4.635265632779619, 5.38631612164996], \"xaxis\": \"x2\", \"y\": [1.0696002857866855e-05, 0.4221776849260434, 2.4400310752316386e-06, 1.0813915071256551e-05, 2.1074410097143765e-06, 3.01722160191678e-05, 2.9136645854693335e-05, 1.3558861400977725e-05, 2.482091827924726e-05, 1.0895430041659685e-05, 1.7126909465601443e-05], \"yaxis\": \"y2\"}, {\"type\": \"scatter\", \"xaxis\": \"x6\", \"yaxis\": \"y6\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [2.4340000139535634, 2.571573491376362, 2.7442387285254384, 2.981915880251583, 3.087744367688028, 3.34403559358756, 3.355906718431561, 3.9728281289117455, 4.635265632779619, 5.38631612164996, 5.688070851486295, 7.722647604976535, 8.159143693652293], \"xaxis\": \"x10\", \"y\": [1.2999999999999998, 2, 4, 8, 16, 16.7], \"yaxis\": \"y10\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, 0.3384094754653131, null], [null, 0.6163657770800628, null, null, null, null, null, null, null, null, 0.3384094754653131, null, null], [null, null, 0.4406294706723891, null, null, null, null, null, null, null, null, null, null], [null, null, null, 0.6341179319054817, 0.6263305322128851, 0.633179175475687, 0.6254513300420013, 0.6366386554621848, 0.6269599489409861, 0.6055505528209781, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [7.722647604976535, 5.688070851486295, 2.7442387285254384, 2.981915880251583, 2.571573491376362, 3.355906718431561, 3.9728281289117455, 3.087744367688028, 3.34403559358756, 4.635265632779619, 5.38631612164996], \"xaxis\": \"x10\", \"y\": [2, 4, 8, 16, 4, 16, 16, 16, 16, 16, 16], \"yaxis\": \"y10\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [2.4340000139535634, 2.571573491376362, 2.7442387285254384, 2.981915880251583, 3.087744367688028, 3.34403559358756, 3.355906718431561, 3.9728281289117455, 4.635265632779619, 5.38631612164996, 5.688070851486295, 7.722647604976535, 8.159143693652293], \"xaxis\": \"x14\", \"y\": [1.4592588011731288e-11, 4.113693549617035e-11, 3.5099819965382977e-09, 1.0946416634758784e-08, 8.320150727863287e-08, 5.252611964132717e-07, 1.7275485042756645e-06, 0.0016058525778814324, 0.0024601092568661046, 0.0030793307766418536, 0.0034806136879271246, 0.041326081068267045, 0.11649943997926332], \"yaxis\": \"y14\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null], [null, 0.6163657770800628, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, 0.6341179319054817, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, 0.6366386554621848, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, 0.3384094754653131, null], [null, null, 0.4406294706723891, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, 0.6055505528209781, null, null, null], [null, null, null, null, 0.6263305322128851, null, null, null, null, null, null, null, null], [null, null, null, null, null, 0.633179175475687, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, 0.6269599489409861, null, null, null, null], [null, null, null, null, null, null, 0.6254513300420013, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, 0.3384094754653131, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [7.722647604976535, 5.688070851486295, 2.7442387285254384, 2.981915880251583, 2.571573491376362, 3.355906718431561, 3.9728281289117455, 3.087744367688028, 3.34403559358756, 4.635265632779619, 5.38631612164996], \"xaxis\": \"x14\", \"y\": [8.320150727863287e-08, 0.041326081068267045, 5.252611964132717e-07, 3.5099819965382977e-09, 4.113693549617035e-11, 0.0034806136879271246, 1.0946416634758784e-08, 0.0016058525778814324, 0.0024601092568661046, 0.0030793307766418536, 1.7275485042756645e-06], \"yaxis\": \"y14\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.2999999999999998, 2, 4, 8, 16, 16.7], \"xaxis\": \"x3\", \"y\": [1.1446387246708548e-06, 2.1074410097143765e-06, 2.4400310752316386e-06, 1.0696002857866855e-05, 1.0813915071256551e-05, 1.0895430041659685e-05, 1.3558861400977725e-05, 1.7126909465601443e-05, 2.482091827924726e-05, 2.9136645854693335e-05, 3.01722160191678e-05, 0.4221776849260434, 0.7772885430337508], \"yaxis\": \"y3\", \"z\": [[null, null, null, null, null, null], [null, null, 0.6163657770800628, null, null, null], [null, null, null, 0.4406294706723891, null, null], [null, 0.3384094754653131, null, null, null, null], [null, null, null, null, 0.6341179319054817, null], [null, null, null, null, 0.6269599489409861, null], [null, null, null, null, 0.6263305322128851, null], [null, null, null, null, 0.6055505528209781, null], [null, null, null, null, 0.633179175475687, null], [null, null, null, null, 0.6366386554621848, null], [null, null, null, null, 0.6254513300420013, null], [null, null, 0.3384094754653131, null, null, null], [null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [2, 4, 8, 16, 4, 16, 16, 16, 16, 16, 16], \"xaxis\": \"x3\", \"y\": [1.0696002857866855e-05, 0.4221776849260434, 2.4400310752316386e-06, 1.0813915071256551e-05, 2.1074410097143765e-06, 3.01722160191678e-05, 2.9136645854693335e-05, 1.3558861400977725e-05, 2.482091827924726e-05, 1.0895430041659685e-05, 1.7126909465601443e-05], \"yaxis\": \"y3\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.2999999999999998, 2, 4, 8, 16, 16.7], \"xaxis\": \"x7\", \"y\": [2.4340000139535634, 2.571573491376362, 2.7442387285254384, 2.981915880251583, 3.087744367688028, 3.34403559358756, 3.355906718431561, 3.9728281289117455, 4.635265632779619, 5.38631612164996, 5.688070851486295, 7.722647604976535, 8.159143693652293], \"yaxis\": \"y7\", \"z\": [[null, null, null, null, null, null], [null, null, 0.6163657770800628, null, null, null], [null, null, null, 0.4406294706723891, null, null], [null, null, null, null, 0.6341179319054817, null], [null, null, null, null, 0.6263305322128851, null], [null, null, null, null, 0.633179175475687, null], [null, null, null, null, 0.6254513300420013, null], [null, null, null, null, 0.6366386554621848, null], [null, null, null, null, 0.6269599489409861, null], [null, null, null, null, 0.6055505528209781, null], [null, null, 0.3384094754653131, null, null, null], [null, 0.3384094754653131, null, null, null, null], [null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [2, 4, 8, 16, 4, 16, 16, 16, 16, 16, 16], \"xaxis\": \"x7\", \"y\": [7.722647604976535, 5.688070851486295, 2.7442387285254384, 2.981915880251583, 2.571573491376362, 3.355906718431561, 3.9728281289117455, 3.087744367688028, 3.34403559358756, 4.635265632779619, 5.38631612164996], \"yaxis\": \"y7\"}, {\"type\": \"scatter\", \"xaxis\": \"x11\", \"yaxis\": \"y11\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.2999999999999998, 2, 4, 8, 16, 16.7], \"xaxis\": \"x15\", \"y\": [1.4592588011731288e-11, 4.113693549617035e-11, 3.5099819965382977e-09, 1.0946416634758784e-08, 8.320150727863287e-08, 5.252611964132717e-07, 1.7275485042756645e-06, 0.0016058525778814324, 0.0024601092568661046, 0.0030793307766418536, 0.0034806136879271246, 0.041326081068267045, 0.11649943997926332], \"yaxis\": \"y15\", \"z\": [[null, null, null, null, null, null], [null, null, 0.6163657770800628, null, null, null], [null, null, null, null, 0.6341179319054817, null], [null, null, null, null, 0.6366386554621848, null], [null, 0.3384094754653131, null, null, null, null], [null, null, null, 0.4406294706723891, null, null], [null, null, null, null, 0.6055505528209781, null], [null, null, null, null, 0.6263305322128851, null], [null, null, null, null, 0.633179175475687, null], [null, null, null, null, 0.6269599489409861, null], [null, null, null, null, 0.6254513300420013, null], [null, null, 0.3384094754653131, null, null, null], [null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [2, 4, 8, 16, 4, 16, 16, 16, 16, 16, 16], \"xaxis\": \"x15\", \"y\": [8.320150727863287e-08, 0.041326081068267045, 5.252611964132717e-07, 3.5099819965382977e-09, 4.113693549617035e-11, 0.0034806136879271246, 1.0946416634758784e-08, 0.0016058525778814324, 0.0024601092568661046, 0.0030793307766418536, 1.7275485042756645e-06], \"yaxis\": \"y15\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.4592588011731288e-11, 4.113693549617035e-11, 3.5099819965382977e-09, 1.0946416634758784e-08, 8.320150727863287e-08, 5.252611964132717e-07, 1.7275485042756645e-06, 0.0016058525778814324, 0.0024601092568661046, 0.0030793307766418536, 0.0034806136879271246, 0.041326081068267045, 0.11649943997926332], \"xaxis\": \"x4\", \"y\": [1.1446387246708548e-06, 2.1074410097143765e-06, 2.4400310752316386e-06, 1.0696002857866855e-05, 1.0813915071256551e-05, 1.0895430041659685e-05, 1.3558861400977725e-05, 1.7126909465601443e-05, 2.482091827924726e-05, 2.9136645854693335e-05, 3.01722160191678e-05, 0.4221776849260434, 0.7772885430337508], \"yaxis\": \"y4\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null], [null, 0.6163657770800628, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, 0.4406294706723891, null, null, null, null, null, null, null], [null, null, null, null, 0.3384094754653131, null, null, null, null, null, null, null, null], [null, null, 0.6341179319054817, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, 0.6269599489409861, null, null, null], [null, null, null, null, null, null, null, 0.6263305322128851, null, null, null, null, null], [null, null, null, null, null, null, 0.6055505528209781, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, 0.633179175475687, null, null, null, null], [null, null, null, 0.6366386554621848, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, 0.6254513300420013, null, null], [null, null, null, null, null, null, null, null, null, null, null, 0.3384094754653131, null], [null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [8.320150727863287e-08, 0.041326081068267045, 5.252611964132717e-07, 3.5099819965382977e-09, 4.113693549617035e-11, 0.0034806136879271246, 1.0946416634758784e-08, 0.0016058525778814324, 0.0024601092568661046, 0.0030793307766418536, 1.7275485042756645e-06], \"xaxis\": \"x4\", \"y\": [1.0696002857866855e-05, 0.4221776849260434, 2.4400310752316386e-06, 1.0813915071256551e-05, 2.1074410097143765e-06, 3.01722160191678e-05, 2.9136645854693335e-05, 1.3558861400977725e-05, 2.482091827924726e-05, 1.0895430041659685e-05, 1.7126909465601443e-05], \"yaxis\": \"y4\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.4592588011731288e-11, 4.113693549617035e-11, 3.5099819965382977e-09, 1.0946416634758784e-08, 8.320150727863287e-08, 5.252611964132717e-07, 1.7275485042756645e-06, 0.0016058525778814324, 0.0024601092568661046, 0.0030793307766418536, 0.0034806136879271246, 0.041326081068267045, 0.11649943997926332], \"xaxis\": \"x8\", \"y\": [2.4340000139535634, 2.571573491376362, 2.7442387285254384, 2.981915880251583, 3.087744367688028, 3.34403559358756, 3.355906718431561, 3.9728281289117455, 4.635265632779619, 5.38631612164996, 5.688070851486295, 7.722647604976535, 8.159143693652293], \"yaxis\": \"y8\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null], [null, 0.6163657770800628, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, 0.4406294706723891, null, null, null, null, null, null, null], [null, null, 0.6341179319054817, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, 0.6263305322128851, null, null, null, null, null], [null, null, null, null, null, null, null, null, 0.633179175475687, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, 0.6254513300420013, null, null], [null, null, null, 0.6366386554621848, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, 0.6269599489409861, null, null, null], [null, null, null, null, null, null, 0.6055505528209781, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, 0.3384094754653131, null], [null, null, null, null, 0.3384094754653131, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [8.320150727863287e-08, 0.041326081068267045, 5.252611964132717e-07, 3.5099819965382977e-09, 4.113693549617035e-11, 0.0034806136879271246, 1.0946416634758784e-08, 0.0016058525778814324, 0.0024601092568661046, 0.0030793307766418536, 1.7275485042756645e-06], \"xaxis\": \"x8\", \"y\": [7.722647604976535, 5.688070851486295, 2.7442387285254384, 2.981915880251583, 2.571573491376362, 3.355906718431561, 3.9728281289117455, 3.087744367688028, 3.34403559358756, 4.635265632779619, 5.38631612164996], \"yaxis\": \"y8\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.4592588011731288e-11, 4.113693549617035e-11, 3.5099819965382977e-09, 1.0946416634758784e-08, 8.320150727863287e-08, 5.252611964132717e-07, 1.7275485042756645e-06, 0.0016058525778814324, 0.0024601092568661046, 0.0030793307766418536, 0.0034806136879271246, 0.041326081068267045, 0.11649943997926332], \"xaxis\": \"x12\", \"y\": [1.2999999999999998, 2, 4, 8, 16, 16.7], \"yaxis\": \"y12\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, 0.3384094754653131, null, null, null, null, null, null, null, null], [null, 0.6163657770800628, null, null, null, null, null, null, null, null, null, 0.3384094754653131, null], [null, null, null, null, null, 0.4406294706723891, null, null, null, null, null, null, null], [null, null, 0.6341179319054817, 0.6366386554621848, null, null, 0.6055505528209781, 0.6263305322128851, 0.633179175475687, 0.6269599489409861, 0.6254513300420013, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [8.320150727863287e-08, 0.041326081068267045, 5.252611964132717e-07, 3.5099819965382977e-09, 4.113693549617035e-11, 0.0034806136879271246, 1.0946416634758784e-08, 0.0016058525778814324, 0.0024601092568661046, 0.0030793307766418536, 1.7275485042756645e-06], \"xaxis\": \"x12\", \"y\": [2, 4, 8, 16, 4, 16, 16, 16, 16, 16, 16], \"yaxis\": \"y12\"}, {\"type\": \"scatter\", \"xaxis\": \"x16\", \"yaxis\": \"y16\"}],\n","                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Contour Plot\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 0.2125], \"matches\": \"x13\", \"range\": [-5.9413315654114, -0.10941773359888843], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis10\": {\"anchor\": \"y10\", \"domain\": [0.2625, 0.475], \"matches\": \"x14\", \"range\": [0.3863205763837568, 0.9116445817151451], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis11\": {\"anchor\": \"y11\", \"domain\": [0.525, 0.7375], \"matches\": \"x15\", \"range\": [1.2999999999999998, 16.7], \"showticklabels\": false}, \"xaxis12\": {\"anchor\": \"y12\", \"domain\": [0.7875, 1.0], \"matches\": \"x16\", \"range\": [-10.835867678665808, -0.9336761623160761], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis13\": {\"anchor\": \"y13\", \"domain\": [0.0, 0.2125], \"range\": [-5.9413315654114, -0.10941773359888843], \"title\": {\"text\": \"learning_rate\"}, \"type\": \"log\"}, \"xaxis14\": {\"anchor\": \"y14\", \"domain\": [0.2625, 0.475], \"range\": [0.3863205763837568, 0.9116445817151451], \"title\": {\"text\": \"num_train_epochs\"}, \"type\": \"log\"}, \"xaxis15\": {\"anchor\": \"y15\", \"domain\": [0.525, 0.7375], \"range\": [1.2999999999999998, 16.7], \"title\": {\"text\": \"per_device_train_batch_size\"}}, \"xaxis16\": {\"anchor\": \"y16\", \"domain\": [0.7875, 1.0], \"range\": [-10.835867678665808, -0.9336761623160761], \"title\": {\"text\": \"weight_decay\"}, \"type\": \"log\"}, \"xaxis2\": {\"anchor\": \"y2\", \"domain\": [0.2625, 0.475], \"matches\": \"x14\", \"range\": [0.3863205763837568, 0.9116445817151451], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis3\": {\"anchor\": \"y3\", \"domain\": [0.525, 0.7375], \"matches\": \"x15\", \"range\": [1.2999999999999998, 16.7], \"showticklabels\": false}, \"xaxis4\": {\"anchor\": \"y4\", \"domain\": [0.7875, 1.0], \"matches\": \"x16\", \"range\": [-10.835867678665808, -0.9336761623160761], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis5\": {\"anchor\": \"y5\", \"domain\": [0.0, 0.2125], \"matches\": \"x13\", \"range\": [-5.9413315654114, -0.10941773359888843], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis6\": {\"anchor\": \"y6\", \"domain\": [0.2625, 0.475], \"matches\": \"x14\", \"range\": [0.3863205763837568, 0.9116445817151451], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis7\": {\"anchor\": \"y7\", \"domain\": [0.525, 0.7375], \"matches\": \"x15\", \"range\": [1.2999999999999998, 16.7], \"showticklabels\": false}, \"xaxis8\": {\"anchor\": \"y8\", \"domain\": [0.7875, 1.0], \"matches\": \"x16\", \"range\": [-10.835867678665808, -0.9336761623160761], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis9\": {\"anchor\": \"y9\", \"domain\": [0.0, 0.2125], \"matches\": \"x13\", \"range\": [-5.9413315654114, -0.10941773359888843], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.80625, 1.0], \"range\": [-5.9413315654114, -0.10941773359888843], \"title\": {\"text\": \"learning_rate\"}, \"type\": \"log\"}, \"yaxis10\": {\"anchor\": \"x10\", \"domain\": [0.26875, 0.4625], \"matches\": \"y9\", \"range\": [1.2999999999999998, 16.7], \"showticklabels\": false}, \"yaxis11\": {\"anchor\": \"x11\", \"domain\": [0.26875, 0.4625], \"matches\": \"y9\", \"range\": [1.2999999999999998, 16.7], \"showticklabels\": false}, \"yaxis12\": {\"anchor\": \"x12\", \"domain\": [0.26875, 0.4625], \"matches\": \"y9\", \"range\": [1.2999999999999998, 16.7], \"showticklabels\": false}, \"yaxis13\": {\"anchor\": \"x13\", \"domain\": [0.0, 0.19375], \"range\": [-10.835867678665808, -0.9336761623160761], \"title\": {\"text\": \"weight_decay\"}, \"type\": \"log\"}, \"yaxis14\": {\"anchor\": \"x14\", \"domain\": [0.0, 0.19375], \"matches\": \"y13\", \"range\": [-10.835867678665808, -0.9336761623160761], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis15\": {\"anchor\": \"x15\", \"domain\": [0.0, 0.19375], \"matches\": \"y13\", \"range\": [-10.835867678665808, -0.9336761623160761], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis16\": {\"anchor\": \"x16\", \"domain\": [0.0, 0.19375], \"matches\": \"y13\", \"range\": [-10.835867678665808, -0.9336761623160761], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis2\": {\"anchor\": \"x2\", \"domain\": [0.80625, 1.0], \"matches\": \"y\", \"range\": [-5.9413315654114, -0.10941773359888843], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis3\": {\"anchor\": \"x3\", \"domain\": [0.80625, 1.0], \"matches\": \"y\", \"range\": [-5.9413315654114, -0.10941773359888843], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis4\": {\"anchor\": \"x4\", \"domain\": [0.80625, 1.0], \"matches\": \"y\", \"range\": [-5.9413315654114, -0.10941773359888843], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis5\": {\"anchor\": \"x5\", \"domain\": [0.5375, 0.73125], \"range\": [0.3863205763837568, 0.9116445817151451], \"title\": {\"text\": \"num_train_epochs\"}, \"type\": \"log\"}, \"yaxis6\": {\"anchor\": \"x6\", \"domain\": [0.5375, 0.73125], \"matches\": \"y5\", \"range\": [0.3863205763837568, 0.9116445817151451], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis7\": {\"anchor\": \"x7\", \"domain\": [0.5375, 0.73125], \"matches\": \"y5\", \"range\": [0.3863205763837568, 0.9116445817151451], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis8\": {\"anchor\": \"x8\", \"domain\": [0.5375, 0.73125], \"matches\": \"y5\", \"range\": [0.3863205763837568, 0.9116445817151451], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis9\": {\"anchor\": \"x9\", \"domain\": [0.26875, 0.4625], \"range\": [1.2999999999999998, 16.7], \"title\": {\"text\": \"per_device_train_batch_size\"}}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('2e96c2bc-85ea-4303-bcce-8df1041e35c2');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"uklDrIbLDvMG","executionInfo":{"status":"ok","timestamp":1639338178987,"user_tz":-60,"elapsed":396,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"d2c96f1b-3768-42b4-88c6-1af61e96dd84"},"source":["optuna.visualization.plot_slice(study)"],"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"6936243b-48e4-4f61-aeb6-62c78653854f\" class=\"plotly-graph-div\" style=\"height:525px; width:1200px;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"6936243b-48e4-4f61-aeb6-62c78653854f\")) {\n","                    Plotly.newPlot(\n","                        '6936243b-48e4-4f61-aeb6-62c78653854f',\n","                        [{\"marker\": {\"color\": [0, 1, 2, 3, 4, 8, 13, 22, 31, 32, 38], \"colorbar\": {\"title\": {\"text\": \"#Trials\"}, \"x\": 1.0, \"xpad\": 40}, \"colorscale\": [[0.0, \"rgb(247,251,255)\"], [0.125, \"rgb(222,235,247)\"], [0.25, \"rgb(198,219,239)\"], [0.375, \"rgb(158,202,225)\"], [0.5, \"rgb(107,174,214)\"], [0.625, \"rgb(66,146,198)\"], [0.75, \"rgb(33,113,181)\"], [0.875, \"rgb(8,81,156)\"], [1.0, \"rgb(8,48,107)\"]], \"line\": {\"color\": \"Grey\", \"width\": 0.5}, \"showscale\": true}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [1.0696002857866855e-05, 0.4221776849260434, 2.4400310752316386e-06, 1.0813915071256551e-05, 2.1074410097143765e-06, 3.01722160191678e-05, 2.9136645854693335e-05, 1.3558861400977725e-05, 2.482091827924726e-05, 1.0895430041659685e-05, 1.7126909465601443e-05], \"xaxis\": \"x\", \"y\": [0.3384094754653131, 0.3384094754653131, 0.4406294706723891, 0.6341179319054817, 0.6163657770800628, 0.6254513300420013, 0.6366386554621848, 0.6263305322128851, 0.633179175475687, 0.6269599489409861, 0.6055505528209781], \"yaxis\": \"y\"}, {\"marker\": {\"color\": [0, 1, 2, 3, 4, 8, 13, 22, 31, 32, 38], \"colorbar\": {\"title\": {\"text\": \"#Trials\"}, \"x\": 1.0, \"xpad\": 40}, \"colorscale\": [[0.0, \"rgb(247,251,255)\"], [0.125, \"rgb(222,235,247)\"], [0.25, \"rgb(198,219,239)\"], [0.375, \"rgb(158,202,225)\"], [0.5, \"rgb(107,174,214)\"], [0.625, \"rgb(66,146,198)\"], [0.75, \"rgb(33,113,181)\"], [0.875, \"rgb(8,81,156)\"], [1.0, \"rgb(8,48,107)\"]], \"line\": {\"color\": \"Grey\", \"width\": 0.5}, \"showscale\": false}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [7.722647604976535, 5.688070851486295, 2.7442387285254384, 2.981915880251583, 2.571573491376362, 3.355906718431561, 3.9728281289117455, 3.087744367688028, 3.34403559358756, 4.635265632779619, 5.38631612164996], \"xaxis\": \"x2\", \"y\": [0.3384094754653131, 0.3384094754653131, 0.4406294706723891, 0.6341179319054817, 0.6163657770800628, 0.6254513300420013, 0.6366386554621848, 0.6263305322128851, 0.633179175475687, 0.6269599489409861, 0.6055505528209781], \"yaxis\": \"y2\"}, {\"marker\": {\"color\": [0, 1, 2, 3, 4, 8, 13, 22, 31, 32, 38], \"colorbar\": {\"title\": {\"text\": \"#Trials\"}, \"x\": 1.0, \"xpad\": 40}, \"colorscale\": [[0.0, \"rgb(247,251,255)\"], [0.125, \"rgb(222,235,247)\"], [0.25, \"rgb(198,219,239)\"], [0.375, \"rgb(158,202,225)\"], [0.5, \"rgb(107,174,214)\"], [0.625, \"rgb(66,146,198)\"], [0.75, \"rgb(33,113,181)\"], [0.875, \"rgb(8,81,156)\"], [1.0, \"rgb(8,48,107)\"]], \"line\": {\"color\": \"Grey\", \"width\": 0.5}, \"showscale\": false}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [2, 4, 8, 16, 4, 16, 16, 16, 16, 16, 16], \"xaxis\": \"x3\", \"y\": [0.3384094754653131, 0.3384094754653131, 0.4406294706723891, 0.6341179319054817, 0.6163657770800628, 0.6254513300420013, 0.6366386554621848, 0.6263305322128851, 0.633179175475687, 0.6269599489409861, 0.6055505528209781], \"yaxis\": \"y3\"}, {\"marker\": {\"color\": [0, 1, 2, 3, 4, 8, 13, 22, 31, 32, 38], \"colorbar\": {\"title\": {\"text\": \"#Trials\"}, \"x\": 1.0, \"xpad\": 40}, \"colorscale\": [[0.0, \"rgb(247,251,255)\"], [0.125, \"rgb(222,235,247)\"], [0.25, \"rgb(198,219,239)\"], [0.375, \"rgb(158,202,225)\"], [0.5, \"rgb(107,174,214)\"], [0.625, \"rgb(66,146,198)\"], [0.75, \"rgb(33,113,181)\"], [0.875, \"rgb(8,81,156)\"], [1.0, \"rgb(8,48,107)\"]], \"line\": {\"color\": \"Grey\", \"width\": 0.5}, \"showscale\": false}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [8.320150727863287e-08, 0.041326081068267045, 5.252611964132717e-07, 3.5099819965382977e-09, 4.113693549617035e-11, 0.0034806136879271246, 1.0946416634758784e-08, 0.0016058525778814324, 0.0024601092568661046, 0.0030793307766418536, 1.7275485042756645e-06], \"xaxis\": \"x4\", \"y\": [0.3384094754653131, 0.3384094754653131, 0.4406294706723891, 0.6341179319054817, 0.6163657770800628, 0.6254513300420013, 0.6366386554621848, 0.6263305322128851, 0.633179175475687, 0.6269599489409861, 0.6055505528209781], \"yaxis\": \"y4\"}],\n","                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Slice Plot\"}, \"width\": 1200, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 0.2125], \"title\": {\"text\": \"learning_rate\"}, \"type\": \"log\"}, \"xaxis2\": {\"anchor\": \"y2\", \"domain\": [0.2625, 0.475], \"title\": {\"text\": \"num_train_epochs\"}, \"type\": \"log\"}, \"xaxis3\": {\"anchor\": \"y3\", \"domain\": [0.525, 0.7375], \"title\": {\"text\": \"per_device_train_batch_size\"}}, \"xaxis4\": {\"anchor\": \"y4\", \"domain\": [0.7875, 1.0], \"title\": {\"text\": \"weight_decay\"}, \"type\": \"log\"}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Objective Value\"}}, \"yaxis2\": {\"anchor\": \"x2\", \"domain\": [0.0, 1.0], \"matches\": \"y\", \"showticklabels\": false}, \"yaxis3\": {\"anchor\": \"x3\", \"domain\": [0.0, 1.0], \"matches\": \"y\", \"showticklabels\": false}, \"yaxis4\": {\"anchor\": \"x4\", \"domain\": [0.0, 1.0], \"matches\": \"y\", \"showticklabels\": false}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('6936243b-48e4-4f61-aeb6-62c78653854f');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"PnOWxc8TD52A","executionInfo":{"status":"ok","timestamp":1639338178989,"user_tz":-60,"elapsed":6,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"08c4f469-3d8f-4acb-8f0c-9e3570315635"},"source":["optuna.visualization.plot_edf(study)"],"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"f3ce6c70-6ad6-4762-81e7-c2f593059300\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"f3ce6c70-6ad6-4762-81e7-c2f593059300\")) {\n","                    Plotly.newPlot(\n","                        'f3ce6c70-6ad6-4762-81e7-c2f593059300',\n","                        [{\"mode\": \"lines\", \"name\": \"RoBERTa_MASK_NS\", \"type\": \"scatter\", \"x\": [0.3384094754653131, 0.34142189142487744, 0.3444343073844418, 0.34744672334400617, 0.35045913930357053, 0.3534715552631349, 0.35648397122269926, 0.3594963871822636, 0.362508803141828, 0.36552121910139235, 0.3685336350609567, 0.3715460510205211, 0.37455846698008544, 0.3775708829396498, 0.38058329889921416, 0.38359571485877847, 0.38660813081834283, 0.3896205467779072, 0.39263296273747156, 0.3956453786970359, 0.3986577946566003, 0.40167021061616465, 0.404682626575729, 0.4076950425352934, 0.41070745849485774, 0.4137198744544221, 0.41673229041398646, 0.4197447063735508, 0.4227571223331152, 0.42576953829267955, 0.4287819542522439, 0.4317943702118083, 0.43480678617137264, 0.437819202130937, 0.44083161809050136, 0.4438440340500657, 0.44685645000963004, 0.4498688659691944, 0.45288128192875876, 0.4558936978883231, 0.4589061138478875, 0.46191852980745185, 0.4649309457670162, 0.4679433617265806, 0.47095577768614494, 0.4739681936457093, 0.47698060960527366, 0.479993025564838, 0.4830054415244024, 0.48601785748396675, 0.4890302734435311, 0.4920426894030955, 0.49505510536265984, 0.4980675213222242, 0.5010799372817886, 0.5040923532413529, 0.5071047692009173, 0.5101171851604817, 0.513129601120046, 0.5161420170796104, 0.5191544330391747, 0.5221668489987391, 0.5251792649583035, 0.5281916809178678, 0.5312040968774322, 0.5342165128369966, 0.5372289287965609, 0.5402413447561253, 0.5432537607156896, 0.546266176675254, 0.5492785926348184, 0.5522910085943826, 0.555303424553947, 0.5583158405135114, 0.5613282564730757, 0.5643406724326401, 0.5673530883922044, 0.5703655043517688, 0.5733779203113332, 0.5763903362708975, 0.5794027522304619, 0.5824151681900263, 0.5854275841495906, 0.588440000109155, 0.5914524160687193, 0.5944648320282837, 0.5974772479878481, 0.6004896639474124, 0.6035020799069768, 0.6065144958665412, 0.6095269118261055, 0.6125393277856699, 0.6155517437452342, 0.6185641597047986, 0.621576575664363, 0.6245889916239273, 0.6276014075834917, 0.6306138235430561, 0.6336262395026204, 0.6366386554621848], \"y\": [0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.18181818181818182, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.2727272727272727, 0.36363636363636365, 0.36363636363636365, 0.36363636363636365, 0.36363636363636365, 0.45454545454545453, 0.45454545454545453, 0.45454545454545453, 0.7272727272727273, 0.7272727272727273, 0.8181818181818182, 1.0]}],\n","                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Empirical Distribution Function Plot\"}, \"xaxis\": {\"title\": {\"text\": \"Objective Value\"}}, \"yaxis\": {\"range\": [0, 1], \"title\": {\"text\": \"Cumulative Probability\"}}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('f3ce6c70-6ad6-4762-81e7-c2f593059300');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]}]}