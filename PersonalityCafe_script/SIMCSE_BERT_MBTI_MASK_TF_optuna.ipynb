{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"SIMCSE_MBTI_MASK_TF_optuna.ipynb","provenance":[{"file_id":"1GViGI-xQToPJ6TlIhvkfgKwhSCBhG4Ht","timestamp":1621319032244}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6ba089b230034d59ade48314026b5327":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6fa1c9cd07d2408c85bf84d35033ed2c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bfd8a5d72ff944aeb50cae55f8369d3e","IPY_MODEL_d94bec22a26d4206aaef9ad2bb3308a5","IPY_MODEL_b22466bcaf7240118ec198e2428064c4"]}},"6fa1c9cd07d2408c85bf84d35033ed2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bfd8a5d72ff944aeb50cae55f8369d3e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_944931df6aa342238870918c0f6d06da","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e30422f88fb64a63a30ab08f39906812"}},"d94bec22a26d4206aaef9ad2bb3308a5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7349a70be54543b3b5b9e21c10c3e9f4","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":252,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":252,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_71d25114aa1746e29b68e7d46cab0cc9"}},"b22466bcaf7240118ec198e2428064c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c45a9727eb424f86aae9870c223c9c9c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 252/252 [00:00&lt;00:00, 5.36kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a53e208bb1dd4cab9892a38c6d3d1f3d"}},"944931df6aa342238870918c0f6d06da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e30422f88fb64a63a30ab08f39906812":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7349a70be54543b3b5b9e21c10c3e9f4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"71d25114aa1746e29b68e7d46cab0cc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c45a9727eb424f86aae9870c223c9c9c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a53e208bb1dd4cab9892a38c6d3d1f3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"948a93593ad14d378cdd2d50510699f8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5abb85f428bb40da8f09f21d8726b7ba","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2d10ee70ed004029ab37e2bc373ae905","IPY_MODEL_e9de5395e0b2426a9aab6fbe9ce1c28e","IPY_MODEL_9ea377cf66834c7485b609c7a86e34c3"]}},"5abb85f428bb40da8f09f21d8726b7ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2d10ee70ed004029ab37e2bc373ae905":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bcc3b7062a4844068ea54bb891e5ede0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_10968c1cf1da48e59af06f638124c29a"}},"e9de5395e0b2426a9aab6fbe9ce1c28e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7641f4c6b6ab481c974b56368c2b72b5","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":689,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":689,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_faa2bcf1b6d5494fa44185caa9baf5da"}},"9ea377cf66834c7485b609c7a86e34c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_23b22e7d1ca3474d9869d804f4a14b9f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 689/689 [00:00&lt;00:00, 19.5kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e953f7eac466499f8680ef5180e4ad78"}},"bcc3b7062a4844068ea54bb891e5ede0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"10968c1cf1da48e59af06f638124c29a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7641f4c6b6ab481c974b56368c2b72b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"faa2bcf1b6d5494fa44185caa9baf5da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"23b22e7d1ca3474d9869d804f4a14b9f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e953f7eac466499f8680ef5180e4ad78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"df4f2004fb964016b8b092a68488b4d7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6efa65cc8bde4be7a7e6efefe6497def","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f104e7e12df44f898917800ab121ff97","IPY_MODEL_3ae6633034d84ea08072f9ca4def5b69","IPY_MODEL_7b1a8412fccd42059405952cad04bb43"]}},"6efa65cc8bde4be7a7e6efefe6497def":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f104e7e12df44f898917800ab121ff97":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ce6943f163f2456ba4b2eed6ca1ef87f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7c8ba96f0214422195e2d67c527ccc5b"}},"3ae6633034d84ea08072f9ca4def5b69":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_50c416bdafab49ec928f2b70e5999a06","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5825fbaf7a6d4daba07b2767c2911fe8"}},"7b1a8412fccd42059405952cad04bb43":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8471d8301aa14877a6e00ea103f134e6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 226k/226k [00:00&lt;00:00, 717kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ab3f2581871547a98928242e308b414f"}},"ce6943f163f2456ba4b2eed6ca1ef87f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7c8ba96f0214422195e2d67c527ccc5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"50c416bdafab49ec928f2b70e5999a06":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5825fbaf7a6d4daba07b2767c2911fe8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8471d8301aa14877a6e00ea103f134e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ab3f2581871547a98928242e308b414f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3a21402fa05949118a01cface5830784":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fef5d7d328964af1bc729d012c004c7e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f524ecea8bec4077aa7cd2a18d222653","IPY_MODEL_574f0121adce400785c1dd43f1f05f03","IPY_MODEL_541ee17717be44e59344536a461a804b"]}},"fef5d7d328964af1bc729d012c004c7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f524ecea8bec4077aa7cd2a18d222653":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9f90634e1d4349ceaa72163043cc33dd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_29d09b205fce43f8b5317691639ad508"}},"574f0121adce400785c1dd43f1f05f03":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5502bf57a57644ac9103dbcee43e6599","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":112,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":112,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a4986d5a56f6488cb066d47c032ff77b"}},"541ee17717be44e59344536a461a804b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d39c6e70feb54b1684973977f9845fe3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 112/112 [00:00&lt;00:00, 2.84kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_612294ec895a4f59a18371ee7d5582be"}},"9f90634e1d4349ceaa72163043cc33dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"29d09b205fce43f8b5317691639ad508":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5502bf57a57644ac9103dbcee43e6599":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a4986d5a56f6488cb066d47c032ff77b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d39c6e70feb54b1684973977f9845fe3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"612294ec895a4f59a18371ee7d5582be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fb8423f797f5446e8eddf04786bab07e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b97f351793744d80b496292623673db4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e8d5c237622340ebbf32432fc8cd1e00","IPY_MODEL_e313bbbd461944559304bea053a31792","IPY_MODEL_11f7439103564572bf97355a0ebf3da8"]}},"b97f351793744d80b496292623673db4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e8d5c237622340ebbf32432fc8cd1e00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cd4eabbb7c80490d9e0665f8f5d8e93c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_be6da4a09d194c95b08d66c7ed60b354"}},"e313bbbd461944559304bea053a31792":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3e7da8d2b6004e918e67977cdc9388d2","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":6,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":6,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_642a3cb906ea4abd8b52e61693b2ea48"}},"11f7439103564572bf97355a0ebf3da8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_68049fb4d5254e02af22060766b703a7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 6/6 [00:18&lt;00:00,  2.87s/ba]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c6ea7e672ce94cae90df0e47935a9e93"}},"cd4eabbb7c80490d9e0665f8f5d8e93c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"be6da4a09d194c95b08d66c7ed60b354":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3e7da8d2b6004e918e67977cdc9388d2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"642a3cb906ea4abd8b52e61693b2ea48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"68049fb4d5254e02af22060766b703a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c6ea7e672ce94cae90df0e47935a9e93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f8f60a3a3820423d9877f313a46c1792":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7c9ee06eec384713b10947760f7eaf80","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_76d076b986184d779918884419670f82","IPY_MODEL_6aabba713c404a1698b064379d7c6c97","IPY_MODEL_25c2b07946b0443da6d5f80e1832d2d6"]}},"7c9ee06eec384713b10947760f7eaf80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"76d076b986184d779918884419670f82":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ffe3de3639e74bbe88070a3c74a8ee3c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_928a06ee5b6c47e6b76a62d784693643"}},"6aabba713c404a1698b064379d7c6c97":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_bd306f2a3f1d4b72b54509ba42884148","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_15e8d3dc074544bd8cc326cb50f770ef"}},"25c2b07946b0443da6d5f80e1832d2d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1bbfb7474e8d4f0981b21ab0ddd11779","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:04&lt;00:00,  2.20s/ba]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eea5947cb10e4025bb8d309b0ffa5626"}},"ffe3de3639e74bbe88070a3c74a8ee3c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"928a06ee5b6c47e6b76a62d784693643":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bd306f2a3f1d4b72b54509ba42884148":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"15e8d3dc074544bd8cc326cb50f770ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1bbfb7474e8d4f0981b21ab0ddd11779":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"eea5947cb10e4025bb8d309b0ffa5626":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3f20383f99b5426aa4994f7da83f8423":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_01b99d09a6b24a71b62cba4e85f7e2d8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f8a104cc6821421eb7d58fc0588590ef","IPY_MODEL_aa6b80bd063c4858bf6786adc2e17499","IPY_MODEL_23098e4b285d41b99a2fc18bb49d8f2e"]}},"01b99d09a6b24a71b62cba4e85f7e2d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f8a104cc6821421eb7d58fc0588590ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a7a3841a5be84849bd191e0befbd0811","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_080e15655375441daf6e47cdef527467"}},"aa6b80bd063c4858bf6786adc2e17499":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_184fa1bea73849cf98cdc85f7a256ee2","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":437998343,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":437998343,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9b60f4b002794446b17484f9dea8e679"}},"23098e4b285d41b99a2fc18bb49d8f2e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3b4a3a84fc5042eca89749dcbd64a37f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 418M/418M [00:13&lt;00:00, 25.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dcac6b758b7e495da3d70609c64292c2"}},"a7a3841a5be84849bd191e0befbd0811":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"080e15655375441daf6e47cdef527467":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"184fa1bea73849cf98cdc85f7a256ee2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9b60f4b002794446b17484f9dea8e679":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3b4a3a84fc5042eca89749dcbd64a37f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dcac6b758b7e495da3d70609c64292c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"-9igszFADeud"},"source":["# Initiliation"]},{"cell_type":"code","metadata":{"id":"4EOaUe7B1xDa","executionInfo":{"status":"ok","timestamp":1638912329396,"user_tz":-60,"elapsed":14184,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"57ba436e-b540-4da6-ae3b-016906a5b70e"},"source":["!pip install transformers datasets --quiet"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 3.1 MB 11.3 MB/s \n","\u001b[K     |████████████████████████████████| 298 kB 6.6 MB/s \n","\u001b[K     |████████████████████████████████| 61 kB 279 kB/s \n","\u001b[K     |████████████████████████████████| 3.3 MB 42.3 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 46.2 MB/s \n","\u001b[K     |████████████████████████████████| 895 kB 46.3 MB/s \n","\u001b[K     |████████████████████████████████| 243 kB 51.2 MB/s \n","\u001b[K     |████████████████████████████████| 132 kB 25.9 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 48.4 MB/s \n","\u001b[K     |████████████████████████████████| 192 kB 42.7 MB/s \n","\u001b[K     |████████████████████████████████| 271 kB 49.1 MB/s \n","\u001b[K     |████████████████████████████████| 160 kB 44.8 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","metadata":{"id":"8KpxUoNWQDGZ","executionInfo":{"status":"ok","timestamp":1638912338755,"user_tz":-60,"elapsed":9366,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}}},"source":["from transformers import TrainingArguments\n","from transformers import Trainer\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoTokenizer\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report, confusion_matrix\n","from datasets import Dataset\n","from datasets import load_metric\n","\n","import numpy as np\n","import math\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from google.colab import drive"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KCQCqALkqtIM"},"source":["# Data Preparation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bv6QdmgkwMsZ","executionInfo":{"status":"ok","timestamp":1638912361298,"user_tz":-60,"elapsed":22559,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"b2b83030-0878-4020-a6d4-30f51f385ce4"},"source":["drive.mount('/content/drive/')"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f9Z1U4B7zvOi","executionInfo":{"status":"ok","timestamp":1638912362685,"user_tz":-60,"elapsed":1395,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"830227b9-33ae-4cad-b50b-ba764ef0848a"},"source":["%cd 'drive/MyDrive/Masterarbeit/Colab Notebooks/OVERVIEW MBTI/Datasets/URL_Balanced_MASK'"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1aHXlqhpj1STohhfU4gn53D4whaLH__Jz/Masterarbeit/Colab Notebooks/OVERVIEW MBTI/Datasets/URL_Balanced_MASK\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"QYahF3fduAU4","executionInfo":{"status":"ok","timestamp":1638912365680,"user_tz":-60,"elapsed":2464,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"d8d85d7a-332a-4458-cfdc-00467a61dd95"},"source":["dfTF = pd.read_csv('MBTI_TF_URL_MASK.csv', sep=\",\", error_bad_lines=False)\n","dfTF"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>i like that you are kind as [MASK] i find that...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>oh my you are right who really talks like tha...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>yep yep yep especially the last one yep agree ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>things that are generalizable to the entire po...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>work student hobbies studying gaming reading d...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>6935</th>\n","      <td>well mostly i dont like avocado but the primar...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6936</th>\n","      <td>during an argument rather than trying to valid...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6937</th>\n","      <td>cigarettes are like hamsters perfectly harmles...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6938</th>\n","      <td>bookshelf porn as a nonamerican please excuse ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6939</th>\n","      <td>i respect your opinion i dont really know any ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6940 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                   text  label\n","0     i like that you are kind as [MASK] i find that...      1\n","1      oh my you are right who really talks like tha...      1\n","2     yep yep yep especially the last one yep agree ...      1\n","3     things that are generalizable to the entire po...      1\n","4     work student hobbies studying gaming reading d...      0\n","...                                                 ...    ...\n","6935  well mostly i dont like avocado but the primar...      1\n","6936  during an argument rather than trying to valid...      1\n","6937  cigarettes are like hamsters perfectly harmles...      1\n","6938  bookshelf porn as a nonamerican please excuse ...      0\n","6939  i respect your opinion i dont really know any ...      1\n","\n","[6940 rows x 2 columns]"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"38B9v6_0sP3h"},"source":["# Model Training"]},{"cell_type":"code","metadata":{"id":"2ewqnPtVdCgV","executionInfo":{"status":"ok","timestamp":1638912365681,"user_tz":-60,"elapsed":7,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}}},"source":["modeltype = \"princeton-nlp/sup-simcse-bert-base-uncased\""],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297,"referenced_widgets":["6ba089b230034d59ade48314026b5327","6fa1c9cd07d2408c85bf84d35033ed2c","bfd8a5d72ff944aeb50cae55f8369d3e","d94bec22a26d4206aaef9ad2bb3308a5","b22466bcaf7240118ec198e2428064c4","944931df6aa342238870918c0f6d06da","e30422f88fb64a63a30ab08f39906812","7349a70be54543b3b5b9e21c10c3e9f4","71d25114aa1746e29b68e7d46cab0cc9","c45a9727eb424f86aae9870c223c9c9c","a53e208bb1dd4cab9892a38c6d3d1f3d","948a93593ad14d378cdd2d50510699f8","5abb85f428bb40da8f09f21d8726b7ba","2d10ee70ed004029ab37e2bc373ae905","e9de5395e0b2426a9aab6fbe9ce1c28e","9ea377cf66834c7485b609c7a86e34c3","bcc3b7062a4844068ea54bb891e5ede0","10968c1cf1da48e59af06f638124c29a","7641f4c6b6ab481c974b56368c2b72b5","faa2bcf1b6d5494fa44185caa9baf5da","23b22e7d1ca3474d9869d804f4a14b9f","e953f7eac466499f8680ef5180e4ad78","df4f2004fb964016b8b092a68488b4d7","6efa65cc8bde4be7a7e6efefe6497def","f104e7e12df44f898917800ab121ff97","3ae6633034d84ea08072f9ca4def5b69","7b1a8412fccd42059405952cad04bb43","ce6943f163f2456ba4b2eed6ca1ef87f","7c8ba96f0214422195e2d67c527ccc5b","50c416bdafab49ec928f2b70e5999a06","5825fbaf7a6d4daba07b2767c2911fe8","8471d8301aa14877a6e00ea103f134e6","ab3f2581871547a98928242e308b414f","3a21402fa05949118a01cface5830784","fef5d7d328964af1bc729d012c004c7e","f524ecea8bec4077aa7cd2a18d222653","574f0121adce400785c1dd43f1f05f03","541ee17717be44e59344536a461a804b","9f90634e1d4349ceaa72163043cc33dd","29d09b205fce43f8b5317691639ad508","5502bf57a57644ac9103dbcee43e6599","a4986d5a56f6488cb066d47c032ff77b","d39c6e70feb54b1684973977f9845fe3","612294ec895a4f59a18371ee7d5582be","fb8423f797f5446e8eddf04786bab07e","b97f351793744d80b496292623673db4","e8d5c237622340ebbf32432fc8cd1e00","e313bbbd461944559304bea053a31792","11f7439103564572bf97355a0ebf3da8","cd4eabbb7c80490d9e0665f8f5d8e93c","be6da4a09d194c95b08d66c7ed60b354","3e7da8d2b6004e918e67977cdc9388d2","642a3cb906ea4abd8b52e61693b2ea48","68049fb4d5254e02af22060766b703a7","c6ea7e672ce94cae90df0e47935a9e93","f8f60a3a3820423d9877f313a46c1792","7c9ee06eec384713b10947760f7eaf80","76d076b986184d779918884419670f82","6aabba713c404a1698b064379d7c6c97","25c2b07946b0443da6d5f80e1832d2d6","ffe3de3639e74bbe88070a3c74a8ee3c","928a06ee5b6c47e6b76a62d784693643","bd306f2a3f1d4b72b54509ba42884148","15e8d3dc074544bd8cc326cb50f770ef","1bbfb7474e8d4f0981b21ab0ddd11779","eea5947cb10e4025bb8d309b0ffa5626","3f20383f99b5426aa4994f7da83f8423","01b99d09a6b24a71b62cba4e85f7e2d8","f8a104cc6821421eb7d58fc0588590ef","aa6b80bd063c4858bf6786adc2e17499","23098e4b285d41b99a2fc18bb49d8f2e","a7a3841a5be84849bd191e0befbd0811","080e15655375441daf6e47cdef527467","184fa1bea73849cf98cdc85f7a256ee2","9b60f4b002794446b17484f9dea8e679","3b4a3a84fc5042eca89749dcbd64a37f","dcac6b758b7e495da3d70609c64292c2"]},"id":"kB1ZJyhlp4O8","executionInfo":{"status":"ok","timestamp":1638912410989,"user_tz":-60,"elapsed":45314,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"34d5d309-a67c-4495-cbd2-34352467c835"},"source":["train, test = train_test_split(dfTF, test_size=0.2, random_state=0, stratify=dfTF.label)\n","\n","train = Dataset.from_pandas(train)\n","test = Dataset.from_pandas(test)\n","\n","tokenizer = AutoTokenizer.from_pretrained(modeltype)\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n","\n","tokenized_train = train.map(tokenize_function, batched=True)\n","tokenized_test = test.map(tokenize_function, batched=True)\n","\n","full_train_dataset = tokenized_train\n","full_eval_dataset = tokenized_test\n","\n","model = AutoModelForSequenceClassification.from_pretrained(modeltype, num_labels=2)\n","\n","training_args = TrainingArguments(\n","    \"SIMCSE_BERT_TF_MASK\", \n","    evaluation_strategy=\"epoch\",\n","    save_strategy = 'no',\n","    save_steps = 100000,\n","    save_total_limit = 1,\n","    metric_for_best_model=\"eval_f1\")\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n","    acc = accuracy_score(labels, preds)\n","    print(classification_report(labels, preds, labels=[0,1]))\n","    print(confusion_matrix(labels,preds))\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ba089b230034d59ade48314026b5327","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/252 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"948a93593ad14d378cdd2d50510699f8","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/689 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df4f2004fb964016b8b092a68488b4d7","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a21402fa05949118a01cface5830784","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb8423f797f5446e8eddf04786bab07e","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/6 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f8f60a3a3820423d9877f313a46c1792","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f20383f99b5426aa4994f7da83f8423","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/418M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","metadata":{"id":"W_PFsTTOqm4a"},"source":["# Hyperparameter Optimization"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N9iWNo8V7gby","executionInfo":{"status":"ok","timestamp":1638912418530,"user_tz":-60,"elapsed":7561,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"592cc627-89c7-4a0d-a622-40538e5fddd2"},"source":["! pip install optuna --quiet"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 308 kB 10.8 MB/s \n","\u001b[K     |████████████████████████████████| 209 kB 48.3 MB/s \n","\u001b[K     |████████████████████████████████| 80 kB 8.3 MB/s \n","\u001b[K     |████████████████████████████████| 75 kB 4.4 MB/s \n","\u001b[K     |████████████████████████████████| 49 kB 5.5 MB/s \n","\u001b[K     |████████████████████████████████| 149 kB 49.4 MB/s \n","\u001b[K     |████████████████████████████████| 112 kB 47.5 MB/s \n","\u001b[?25h  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","metadata":{"id":"y4BgLFRH7kVg","executionInfo":{"status":"ok","timestamp":1638912418531,"user_tz":-60,"elapsed":6,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}}},"source":["def model_init():\n","    return AutoModelForSequenceClassification.from_pretrained(modeltype, num_labels=2)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mpa3mq0u7sN3","executionInfo":{"status":"ok","timestamp":1638912432539,"user_tz":-60,"elapsed":14012,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"db09214e-cc08-4d13-da77-1c21da552150"},"source":["trainer = Trainer(\n","      model_init=model_init,\n","      args=training_args, \n","      train_dataset=full_train_dataset, \n","      eval_dataset=full_eval_dataset,\n","      compute_metrics=compute_metrics \n","  )"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"pjNfFgAH7voa","executionInfo":{"status":"ok","timestamp":1638949302079,"user_tz":-60,"elapsed":36869555,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"261b12d8-cb82-4a08-b7e9-38ca3798edf7"},"source":["import sklearn.metrics as metrics\n","import optuna\n","import sys\n","import logging\n","\n","def objective (metrics):\n","  return metrics['eval_f1']\n","\n","def hyperparameter_space(trial):\n","\n","    return {\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 5e-8, 5e-1, log=True),\n","        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [2, 4, 8, 16]),\n","        \"weight_decay\": trial.suggest_float(\"weight_decay\", 5e-12, 5e-1, log=True),\n","        \"num_train_epochs\": trial.suggest_float(\"num_train_epochs\",1,8,log=True),\n","        #\"adam_epsilon\": trial.suggest_float(\"adam_epsilon\", 1e-10, 1e-6, log=True),\n","        #\"seed\" : trial.suggest_float(\"seed\",10,60,log=True)\n","        }\n","\n","optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n","study_name = \"SIMCSE_BERT_MASK_TF\"  # Unique identifier of the study.\n","storage_name = \"sqlite:///{}.db\".format(study_name)\n","\n","best_run = trainer.hyperparameter_search(hp_space=hyperparameter_space,compute_objective=objective, n_trials=50, direction=\"maximize\",study_name=study_name, storage=storage_name )\n","\n","study = optuna.create_study()"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-07 21:27:13,913]\u001b[0m A new study created in RDB with name: SIMCSE_BERT_MASK_TF\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["A new study created in RDB with name: SIMCSE_BERT_MASK_TF\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4793\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4793' max='4793' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4793/4793 41:34, Epoch 6/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.677400</td>\n","      <td>0.662608</td>\n","      <td>0.623919</td>\n","      <td>0.577061</td>\n","      <td>0.647011</td>\n","      <td>0.598964</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.657000</td>\n","      <td>0.625932</td>\n","      <td>0.663545</td>\n","      <td>0.637052</td>\n","      <td>0.680015</td>\n","      <td>0.644552</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.605400</td>\n","      <td>0.595617</td>\n","      <td>0.698847</td>\n","      <td>0.689882</td>\n","      <td>0.700235</td>\n","      <td>0.689158</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.583300</td>\n","      <td>0.582110</td>\n","      <td>0.698847</td>\n","      <td>0.686449</td>\n","      <td>0.704520</td>\n","      <td>0.686562</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.578400</td>\n","      <td>0.570028</td>\n","      <td>0.708934</td>\n","      <td>0.701433</td>\n","      <td>0.709630</td>\n","      <td>0.700363</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.561300</td>\n","      <td>0.566542</td>\n","      <td>0.708213</td>\n","      <td>0.699901</td>\n","      <td>0.709775</td>\n","      <td>0.698923</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.554800</td>\n","      <td>0.565216</td>\n","      <td>0.709654</td>\n","      <td>0.701530</td>\n","      <td>0.711131</td>\n","      <td>0.700506</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.69      0.32      0.44       632\n","           1       0.61      0.88      0.72       756\n","\n","    accuracy                           0.62      1388\n","   macro avg       0.65      0.60      0.58      1388\n","weighted avg       0.64      0.62      0.59      1388\n","\n","[[202 430]\n"," [ 92 664]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.72      0.43      0.54       632\n","           1       0.64      0.86      0.74       756\n","\n","    accuracy                           0.66      1388\n","   macro avg       0.68      0.64      0.64      1388\n","weighted avg       0.68      0.66      0.65      1388\n","\n","[[273 359]\n"," [108 648]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.71      0.58      0.64       632\n","           1       0.69      0.80      0.74       756\n","\n","    accuracy                           0.70      1388\n","   macro avg       0.70      0.69      0.69      1388\n","weighted avg       0.70      0.70      0.69      1388\n","\n","[[367 265]\n"," [153 603]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.72      0.55      0.62       632\n","           1       0.69      0.82      0.75       756\n","\n","    accuracy                           0.70      1388\n","   macro avg       0.70      0.69      0.69      1388\n","weighted avg       0.70      0.70      0.69      1388\n","\n","[[347 285]\n"," [133 623]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.71      0.60      0.65       632\n","           1       0.71      0.80      0.75       756\n","\n","    accuracy                           0.71      1388\n","   macro avg       0.71      0.70      0.70      1388\n","weighted avg       0.71      0.71      0.71      1388\n","\n","[[382 250]\n"," [154 602]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.72      0.59      0.65       632\n","           1       0.70      0.80      0.75       756\n","\n","    accuracy                           0.71      1388\n","   macro avg       0.71      0.70      0.70      1388\n","weighted avg       0.71      0.71      0.70      1388\n","\n","[[376 256]\n"," [149 607]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.72      0.60      0.65       632\n","           1       0.70      0.80      0.75       756\n","\n","    accuracy                           0.71      1388\n","   macro avg       0.71      0.70      0.70      1388\n","weighted avg       0.71      0.71      0.71      1388\n","\n","[[378 254]\n"," [149 607]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-07 22:08:51,352]\u001b[0m Trial 0 finished with value: 0.7015297451634195 and parameters: {'learning_rate': 5.113164637727514e-07, 'per_device_train_batch_size': 8, 'weight_decay': 0.0004337243841065953, 'num_train_epochs': 6.90519349381888}. Best is trial 0 with value: 0.7015297451634195.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 0 finished with value: 0.7015297451634195 and parameters: {'learning_rate': 5.113164637727514e-07, 'per_device_train_batch_size': 8, 'weight_decay': 0.0004337243841065953, 'num_train_epochs': 6.90519349381888}. Best is trial 0 with value: 0.7015297451634195.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2800\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2800' max='2800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2800/2800 24:41, Epoch 4/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.610300</td>\n","      <td>0.588189</td>\n","      <td>0.684438</td>\n","      <td>0.677471</td>\n","      <td>0.736268</td>\n","      <td>0.703180</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.513800</td>\n","      <td>0.531387</td>\n","      <td>0.742795</td>\n","      <td>0.742391</td>\n","      <td>0.743271</td>\n","      <td>0.745203</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.365400</td>\n","      <td>0.757042</td>\n","      <td>0.737032</td>\n","      <td>0.735339</td>\n","      <td>0.735050</td>\n","      <td>0.735759</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.233200</td>\n","      <td>1.089246</td>\n","      <td>0.734870</td>\n","      <td>0.730232</td>\n","      <td>0.734147</td>\n","      <td>0.728974</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.233200</td>\n","      <td>1.090853</td>\n","      <td>0.731988</td>\n","      <td>0.727609</td>\n","      <td>0.730881</td>\n","      <td>0.726458</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.60      0.91      0.72       632\n","           1       0.87      0.49      0.63       756\n","\n","    accuracy                           0.68      1388\n","   macro avg       0.74      0.70      0.68      1388\n","weighted avg       0.75      0.68      0.67      1388\n","\n","[[577  55]\n"," [383 373]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.70      0.77      0.73       632\n","           1       0.79      0.72      0.75       756\n","\n","    accuracy                           0.74      1388\n","   macro avg       0.74      0.75      0.74      1388\n","weighted avg       0.75      0.74      0.74      1388\n","\n","[[488 144]\n"," [213 543]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.71      0.72      0.71       632\n","           1       0.76      0.75      0.76       756\n","\n","    accuracy                           0.74      1388\n","   macro avg       0.74      0.74      0.74      1388\n","weighted avg       0.74      0.74      0.74      1388\n","\n","[[456 176]\n"," [189 567]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.73      0.66      0.69       632\n","           1       0.74      0.79      0.77       756\n","\n","    accuracy                           0.73      1388\n","   macro avg       0.73      0.73      0.73      1388\n","weighted avg       0.73      0.73      0.73      1388\n","\n","[[419 213]\n"," [155 601]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.72      0.66      0.69       632\n","           1       0.74      0.79      0.76       756\n","\n","    accuracy                           0.73      1388\n","   macro avg       0.73      0.73      0.73      1388\n","weighted avg       0.73      0.73      0.73      1388\n","\n","[[420 212]\n"," [160 596]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-07 22:33:35,880]\u001b[0m Trial 1 finished with value: 0.7276088222633006 and parameters: {'learning_rate': 2.8957120638078643e-05, 'per_device_train_batch_size': 8, 'weight_decay': 1.1108012743093335e-09, 'num_train_epochs': 4.03344948971642}. Best is trial 1 with value: 0.7276088222633006.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 1 finished with value: 0.7276088222633006 and parameters: {'learning_rate': 2.8957120638078643e-05, 'per_device_train_batch_size': 8, 'weight_decay': 1.1108012743093335e-09, 'num_train_epochs': 4.03344948971642}. Best is trial 1 with value: 0.7276088222633006.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 753\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='753' max='753' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [753/753 12:32, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.612012</td>\n","      <td>0.659942</td>\n","      <td>0.646774</td>\n","      <td>0.734117</td>\n","      <td>0.682640</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.575000</td>\n","      <td>0.550747</td>\n","      <td>0.753602</td>\n","      <td>0.751214</td>\n","      <td>0.751701</td>\n","      <td>0.750841</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.575000</td>\n","      <td>0.580998</td>\n","      <td>0.754323</td>\n","      <td>0.752182</td>\n","      <td>0.752362</td>\n","      <td>0.752022</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      0.94      0.71       632\n","           1       0.89      0.43      0.58       756\n","\n","    accuracy                           0.66      1388\n","   macro avg       0.73      0.68      0.65      1388\n","weighted avg       0.75      0.66      0.64      1388\n","\n","[[592  40]\n"," [432 324]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.73      0.72      0.73       632\n","           1       0.77      0.78      0.78       756\n","\n","    accuracy                           0.75      1388\n","   macro avg       0.75      0.75      0.75      1388\n","weighted avg       0.75      0.75      0.75      1388\n","\n","[[455 177]\n"," [165 591]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.73      0.73      0.73       632\n","           1       0.77      0.78      0.78       756\n","\n","    accuracy                           0.75      1388\n","   macro avg       0.75      0.75      0.75      1388\n","weighted avg       0.75      0.75      0.75      1388\n","\n","[[459 173]\n"," [168 588]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-07 22:46:12,286]\u001b[0m Trial 2 finished with value: 0.7521821788855245 and parameters: {'learning_rate': 0.00010383242911913873, 'per_device_train_batch_size': 16, 'weight_decay': 4.717749467497328e-07, 'num_train_epochs': 2.169265655030074}. Best is trial 2 with value: 0.7521821788855245.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 2 finished with value: 0.7521821788855245 and parameters: {'learning_rate': 0.00010383242911913873, 'per_device_train_batch_size': 16, 'weight_decay': 4.717749467497328e-07, 'num_train_epochs': 2.169265655030074}. Best is trial 2 with value: 0.7521821788855245.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4373\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4373' max='4373' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4373/4373 21:45, Epoch 3/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>7.682800</td>\n","      <td>1.960315</td>\n","      <td>0.544669</td>\n","      <td>0.352612</td>\n","      <td>0.272334</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>5.647900</td>\n","      <td>3.962965</td>\n","      <td>0.455331</td>\n","      <td>0.312871</td>\n","      <td>0.227666</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>2.765600</td>\n","      <td>2.521621</td>\n","      <td>0.544669</td>\n","      <td>0.352612</td>\n","      <td>0.272334</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>2.765600</td>\n","      <td>0.742753</td>\n","      <td>0.455331</td>\n","      <td>0.312871</td>\n","      <td>0.227666</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       632\n","           1       0.54      1.00      0.71       756\n","\n","    accuracy                           0.54      1388\n","   macro avg       0.27      0.50      0.35      1388\n","weighted avg       0.30      0.54      0.38      1388\n","\n","[[  0 632]\n"," [  0 756]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.46      1.00      0.63       632\n","           1       0.00      0.00      0.00       756\n","\n","    accuracy                           0.46      1388\n","   macro avg       0.23      0.50      0.31      1388\n","weighted avg       0.21      0.46      0.28      1388\n","\n","[[632   0]\n"," [756   0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       632\n","           1       0.54      1.00      0.71       756\n","\n","    accuracy                           0.54      1388\n","   macro avg       0.27      0.50      0.35      1388\n","weighted avg       0.30      0.54      0.38      1388\n","\n","[[  0 632]\n"," [  0 756]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.46      1.00      0.63       632\n","           1       0.00      0.00      0.00       756\n","\n","    accuracy                           0.46      1388\n","   macro avg       0.23      0.50      0.31      1388\n","weighted avg       0.21      0.46      0.28      1388\n","\n","[[632   0]\n"," [756   0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-07 23:08:00,670]\u001b[0m Trial 3 finished with value: 0.31287128712871287 and parameters: {'learning_rate': 0.06032012701524992, 'per_device_train_batch_size': 4, 'weight_decay': 5.597111728168741e-09, 'num_train_epochs': 3.150463025055146}. Best is trial 2 with value: 0.7521821788855245.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 3 finished with value: 0.31287128712871287 and parameters: {'learning_rate': 0.06032012701524992, 'per_device_train_batch_size': 4, 'weight_decay': 5.597111728168741e-09, 'num_train_epochs': 3.150463025055146}. Best is trial 2 with value: 0.7521821788855245.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3704\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3704' max='3704' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3704/3704 32:35, Epoch 5/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.716700</td>\n","      <td>0.700985</td>\n","      <td>0.455331</td>\n","      <td>0.312871</td>\n","      <td>0.227666</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.699600</td>\n","      <td>0.689237</td>\n","      <td>0.544669</td>\n","      <td>0.352612</td>\n","      <td>0.272334</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.693700</td>\n","      <td>0.689379</td>\n","      <td>0.544669</td>\n","      <td>0.352612</td>\n","      <td>0.272334</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.692700</td>\n","      <td>0.689170</td>\n","      <td>0.544669</td>\n","      <td>0.352612</td>\n","      <td>0.272334</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.689100</td>\n","      <td>0.689151</td>\n","      <td>0.544669</td>\n","      <td>0.352612</td>\n","      <td>0.272334</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.689700</td>\n","      <td>0.689181</td>\n","      <td>0.544669</td>\n","      <td>0.352612</td>\n","      <td>0.272334</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.46      1.00      0.63       632\n","           1       0.00      0.00      0.00       756\n","\n","    accuracy                           0.46      1388\n","   macro avg       0.23      0.50      0.31      1388\n","weighted avg       0.21      0.46      0.28      1388\n","\n","[[632   0]\n"," [756   0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       632\n","           1       0.54      1.00      0.71       756\n","\n","    accuracy                           0.54      1388\n","   macro avg       0.27      0.50      0.35      1388\n","weighted avg       0.30      0.54      0.38      1388\n","\n","[[  0 632]\n"," [  0 756]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       632\n","           1       0.54      1.00      0.71       756\n","\n","    accuracy                           0.54      1388\n","   macro avg       0.27      0.50      0.35      1388\n","weighted avg       0.30      0.54      0.38      1388\n","\n","[[  0 632]\n"," [  0 756]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       632\n","           1       0.54      1.00      0.71       756\n","\n","    accuracy                           0.54      1388\n","   macro avg       0.27      0.50      0.35      1388\n","weighted avg       0.30      0.54      0.38      1388\n","\n","[[  0 632]\n"," [  0 756]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       632\n","           1       0.54      1.00      0.71       756\n","\n","    accuracy                           0.54      1388\n","   macro avg       0.27      0.50      0.35      1388\n","weighted avg       0.30      0.54      0.38      1388\n","\n","[[  0 632]\n"," [  0 756]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       632\n","           1       0.54      1.00      0.71       756\n","\n","    accuracy                           0.54      1388\n","   macro avg       0.27      0.50      0.35      1388\n","weighted avg       0.30      0.54      0.38      1388\n","\n","[[  0 632]\n"," [  0 756]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-07 23:40:39,478]\u001b[0m Trial 4 finished with value: 0.35261194029850745 and parameters: {'learning_rate': 0.00034161359596541536, 'per_device_train_batch_size': 8, 'weight_decay': 0.019259178689961923, 'num_train_epochs': 5.336965770180979}. Best is trial 2 with value: 0.7521821788855245.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 4 finished with value: 0.35261194029850745 and parameters: {'learning_rate': 0.00034161359596541536, 'per_device_train_batch_size': 8, 'weight_decay': 0.019259178689961923, 'num_train_epochs': 5.336965770180979}. Best is trial 2 with value: 0.7521821788855245.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2647\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1389' max='2647' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1389/2647 06:15 < 05:40, 3.69 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.677200</td>\n","      <td>0.669966</td>\n","      <td>0.585735</td>\n","      <td>0.499189</td>\n","      <td>0.617037</td>\n","      <td>0.553011</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.66      0.19      0.29       632\n","           1       0.57      0.92      0.71       756\n","\n","    accuracy                           0.59      1388\n","   macro avg       0.62      0.55      0.50      1388\n","weighted avg       0.61      0.59      0.52      1388\n","\n","[[118 514]\n"," [ 61 695]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-07 23:47:26,245]\u001b[0m Trial 5 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 5 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3888\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1389' max='3888' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1389/3888 06:17 < 11:19, 3.68 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>14.806700</td>\n","      <td>15.145612</td>\n","      <td>0.455331</td>\n","      <td>0.312871</td>\n","      <td>0.227666</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.46      1.00      0.63       632\n","           1       0.00      0.00      0.00       756\n","\n","    accuracy                           0.46      1388\n","   macro avg       0.23      0.50      0.31      1388\n","weighted avg       0.21      0.46      0.28      1388\n","\n","[[632   0]\n"," [756   0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-07 23:54:14,529]\u001b[0m Trial 6 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 6 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2576\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='695' max='2576' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 695/2576 05:34 < 15:08, 2.07 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.328000</td>\n","      <td>1.779015</td>\n","      <td>0.455331</td>\n","      <td>0.312871</td>\n","      <td>0.227666</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.46      1.00      0.63       632\n","           1       0.00      0.00      0.00       756\n","\n","    accuracy                           0.46      1388\n","   macro avg       0.23      0.50      0.31      1388\n","weighted avg       0.21      0.46      0.28      1388\n","\n","[[632   0]\n"," [756   0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-08 00:00:20,883]\u001b[0m Trial 7 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 7 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1446\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='348' max='1446' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 348/1446 05:08 < 16:18, 1.12 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.686017</td>\n","      <td>0.560519</td>\n","      <td>0.476520</td>\n","      <td>0.558224</td>\n","      <td>0.528954</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.56      0.18      0.27       632\n","           1       0.56      0.88      0.69       756\n","\n","    accuracy                           0.56      1388\n","   macro avg       0.56      0.53      0.48      1388\n","weighted avg       0.56      0.56      0.50      1388\n","\n","[[111 521]\n"," [ 89 667]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-08 00:06:01,099]\u001b[0m Trial 8 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 8 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 8\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 21799\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='21799' max='21799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [21799/21799 1:04:16, Epoch 7/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.805200</td>\n","      <td>0.737043</td>\n","      <td>0.718300</td>\n","      <td>0.717976</td>\n","      <td>0.731014</td>\n","      <td>0.727128</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.866000</td>\n","      <td>0.857907</td>\n","      <td>0.744957</td>\n","      <td>0.743831</td>\n","      <td>0.743481</td>\n","      <td>0.744981</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.783200</td>\n","      <td>1.058639</td>\n","      <td>0.745677</td>\n","      <td>0.734483</td>\n","      <td>0.757958</td>\n","      <td>0.733185</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.689300</td>\n","      <td>1.190879</td>\n","      <td>0.729107</td>\n","      <td>0.710936</td>\n","      <td>0.753802</td>\n","      <td>0.712394</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.636400</td>\n","      <td>1.094498</td>\n","      <td>0.750000</td>\n","      <td>0.746055</td>\n","      <td>0.749231</td>\n","      <td>0.744809</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.501900</td>\n","      <td>1.338237</td>\n","      <td>0.733429</td>\n","      <td>0.719624</td>\n","      <td>0.748865</td>\n","      <td>0.719346</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.486200</td>\n","      <td>1.300148</td>\n","      <td>0.743516</td>\n","      <td>0.740955</td>\n","      <td>0.741521</td>\n","      <td>0.740544</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.465600</td>\n","      <td>1.350096</td>\n","      <td>0.743516</td>\n","      <td>0.739129</td>\n","      <td>0.742944</td>\n","      <td>0.737819</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.65      0.83      0.73       632\n","           1       0.81      0.63      0.71       756\n","\n","    accuracy                           0.72      1388\n","   macro avg       0.73      0.73      0.72      1388\n","weighted avg       0.74      0.72      0.72      1388\n","\n","[[522 110]\n"," [281 475]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.71      0.75      0.73       632\n","           1       0.78      0.74      0.76       756\n","\n","    accuracy                           0.74      1388\n","   macro avg       0.74      0.74      0.74      1388\n","weighted avg       0.75      0.74      0.75      1388\n","\n","[[471 161]\n"," [193 563]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.80      0.59      0.68       632\n","           1       0.72      0.87      0.79       756\n","\n","    accuracy                           0.75      1388\n","   macro avg       0.76      0.73      0.73      1388\n","weighted avg       0.75      0.75      0.74      1388\n","\n","[[375 257]\n"," [ 96 660]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.81      0.53      0.64       632\n","           1       0.69      0.90      0.78       756\n","\n","    accuracy                           0.73      1388\n","   macro avg       0.75      0.71      0.71      1388\n","weighted avg       0.75      0.73      0.72      1388\n","\n","[[332 300]\n"," [ 76 680]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.74      0.69      0.71       632\n","           1       0.75      0.80      0.78       756\n","\n","    accuracy                           0.75      1388\n","   macro avg       0.75      0.74      0.75      1388\n","weighted avg       0.75      0.75      0.75      1388\n","\n","[[434 198]\n"," [149 607]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.79      0.56      0.66       632\n","           1       0.71      0.88      0.78       756\n","\n","    accuracy                           0.73      1388\n","   macro avg       0.75      0.72      0.72      1388\n","weighted avg       0.74      0.73      0.73      1388\n","\n","[[355 277]\n"," [ 93 663]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.72      0.71      0.72       632\n","           1       0.76      0.77      0.77       756\n","\n","    accuracy                           0.74      1388\n","   macro avg       0.74      0.74      0.74      1388\n","weighted avg       0.74      0.74      0.74      1388\n","\n","[[447 185]\n"," [171 585]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.74      0.67      0.71       632\n","           1       0.75      0.80      0.77       756\n","\n","    accuracy                           0.74      1388\n","   macro avg       0.74      0.74      0.74      1388\n","weighted avg       0.74      0.74      0.74      1388\n","\n","[[426 206]\n"," [150 606]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-08 01:10:20,139]\u001b[0m Trial 9 finished with value: 0.7391285984592513 and parameters: {'learning_rate': 3.515249330255268e-06, 'per_device_train_batch_size': 2, 'weight_decay': 0.00015865248205273314, 'num_train_epochs': 7.852372823525647}. Best is trial 2 with value: 0.7521821788855245.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 9 finished with value: 0.7391285984592513 and parameters: {'learning_rate': 3.515249330255268e-06, 'per_device_train_batch_size': 2, 'weight_decay': 0.00015865248205273314, 'num_train_epochs': 7.852372823525647}. Best is trial 2 with value: 0.7521821788855245.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 349\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='348' max='349' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [348/349 05:08 < 00:00, 1.12 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.689222</td>\n","      <td>0.544669</td>\n","      <td>0.352612</td>\n","      <td>0.272334</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       632\n","           1       0.54      1.00      0.71       756\n","\n","    accuracy                           0.54      1388\n","   macro avg       0.27      0.50      0.35      1388\n","weighted avg       0.30      0.54      0.38      1388\n","\n","[[  0 632]\n"," [  0 756]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-08 01:16:00,522]\u001b[0m Trial 10 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 10 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4987\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4987' max='4987' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4987/4987 14:46, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.795600</td>\n","      <td>0.810653</td>\n","      <td>0.737752</td>\n","      <td>0.737488</td>\n","      <td>0.750417</td>\n","      <td>0.746542</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.751100</td>\n","      <td>0.995395</td>\n","      <td>0.757925</td>\n","      <td>0.754908</td>\n","      <td>0.756511</td>\n","      <td>0.754031</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.67      0.84      0.75       632\n","           1       0.83      0.65      0.73       756\n","\n","    accuracy                           0.74      1388\n","   macro avg       0.75      0.75      0.74      1388\n","weighted avg       0.76      0.74      0.74      1388\n","\n","[[534  98]\n"," [266 490]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.75      0.71      0.73       632\n","           1       0.77      0.80      0.78       756\n","\n","    accuracy                           0.76      1388\n","   macro avg       0.76      0.75      0.75      1388\n","weighted avg       0.76      0.76      0.76      1388\n","\n","[[449 183]\n"," [153 603]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-08 01:30:49,806]\u001b[0m Trial 11 finished with value: 0.754907958049808 and parameters: {'learning_rate': 3.558787916816092e-05, 'per_device_train_batch_size': 2, 'weight_decay': 2.6453705505426204e-05, 'num_train_epochs': 1.796459751947201}. Best is trial 11 with value: 0.754907958049808.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 11 finished with value: 0.754907958049808 and parameters: {'learning_rate': 3.558787916816092e-05, 'per_device_train_batch_size': 2, 'weight_decay': 2.6453705505426204e-05, 'num_train_epochs': 1.796459751947201}. Best is trial 11 with value: 0.754907958049808.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4867\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4867' max='4867' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4867/4867 14:25, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.863600</td>\n","      <td>0.672961</td>\n","      <td>0.750000</td>\n","      <td>0.749688</td>\n","      <td>0.750925</td>\n","      <td>0.752855</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.732300</td>\n","      <td>1.015488</td>\n","      <td>0.750720</td>\n","      <td>0.745649</td>\n","      <td>0.751471</td>\n","      <td>0.744043</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.70      0.78      0.74       632\n","           1       0.80      0.72      0.76       756\n","\n","    accuracy                           0.75      1388\n","   macro avg       0.75      0.75      0.75      1388\n","weighted avg       0.76      0.75      0.75      1388\n","\n","[[496 136]\n"," [211 545]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.76      0.67      0.71       632\n","           1       0.75      0.82      0.78       756\n","\n","    accuracy                           0.75      1388\n","   macro avg       0.75      0.74      0.75      1388\n","weighted avg       0.75      0.75      0.75      1388\n","\n","[[423 209]\n"," [137 619]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-08 01:45:18,139]\u001b[0m Trial 12 finished with value: 0.7456486000949089 and parameters: {'learning_rate': 2.879490739702494e-05, 'per_device_train_batch_size': 2, 'weight_decay': 1.2764023216744377e-11, 'num_train_epochs': 1.7529057098780358}. Best is trial 11 with value: 0.754907958049808.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 12 finished with value: 0.7456486000949089 and parameters: {'learning_rate': 2.879490739702494e-05, 'per_device_train_batch_size': 2, 'weight_decay': 1.2764023216744377e-11, 'num_train_epochs': 1.7529057098780358}. Best is trial 11 with value: 0.754907958049808.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 705\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='705' max='705' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [705/705 11:52, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.537811</td>\n","      <td>0.721182</td>\n","      <td>0.720677</td>\n","      <td>0.736148</td>\n","      <td>0.730812</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.550400</td>\n","      <td>0.511596</td>\n","      <td>0.762248</td>\n","      <td>0.759046</td>\n","      <td>0.761148</td>\n","      <td>0.757999</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.550400</td>\n","      <td>0.512572</td>\n","      <td>0.760807</td>\n","      <td>0.757336</td>\n","      <td>0.759951</td>\n","      <td>0.756157</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.65      0.84      0.73       632\n","           1       0.82      0.62      0.71       756\n","\n","    accuracy                           0.72      1388\n","   macro avg       0.74      0.73      0.72      1388\n","weighted avg       0.74      0.72      0.72      1388\n","\n","[[530 102]\n"," [285 471]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.75      0.71      0.73       632\n","           1       0.77      0.81      0.79       756\n","\n","    accuracy                           0.76      1388\n","   macro avg       0.76      0.76      0.76      1388\n","weighted avg       0.76      0.76      0.76      1388\n","\n","[[449 183]\n"," [147 609]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.75      0.70      0.73       632\n","           1       0.77      0.81      0.79       756\n","\n","    accuracy                           0.76      1388\n","   macro avg       0.76      0.76      0.76      1388\n","weighted avg       0.76      0.76      0.76      1388\n","\n","[[445 187]\n"," [145 611]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-08 01:57:14,046]\u001b[0m Trial 13 finished with value: 0.7573360126551616 and parameters: {'learning_rate': 2.6781315018468085e-05, 'per_device_train_batch_size': 16, 'weight_decay': 3.4450729101456406e-06, 'num_train_epochs': 2.03163161021332}. Best is trial 13 with value: 0.7573360126551616.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 13 finished with value: 0.7573360126551616 and parameters: {'learning_rate': 2.6781315018468085e-05, 'per_device_train_batch_size': 16, 'weight_decay': 3.4450729101456406e-06, 'num_train_epochs': 2.03163161021332}. Best is trial 13 with value: 0.7573360126551616.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3833\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3834' max='3833' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3833/3833 11:03, Epoch 1.38/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.809800</td>\n","      <td>0.739764</td>\n","      <td>0.718300</td>\n","      <td>0.717948</td>\n","      <td>0.719213</td>\n","      <td>0.720899</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.746500</td>\n","      <td>0.841105</td>\n","      <td>0.739193</td>\n","      <td>0.733997</td>\n","      <td>0.739356</td>\n","      <td>0.732553</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.67      0.75      0.71       632\n","           1       0.77      0.69      0.73       756\n","\n","    accuracy                           0.72      1388\n","   macro avg       0.72      0.72      0.72      1388\n","weighted avg       0.72      0.72      0.72      1388\n","\n","[[474 158]\n"," [233 523]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.74      0.66      0.70       632\n","           1       0.74      0.81      0.77       756\n","\n","    accuracy                           0.74      1388\n","   macro avg       0.74      0.73      0.73      1388\n","weighted avg       0.74      0.74      0.74      1388\n","\n","[[416 216]\n"," [146 610]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-08 02:08:48,727]\u001b[0m Trial 14 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 14 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3635\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2777' max='3635' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2777/3635 07:41 < 02:22, 6.01 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.930900</td>\n","      <td>0.847542</td>\n","      <td>0.455331</td>\n","      <td>0.312871</td>\n","      <td>0.227666</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.46      1.00      0.63       632\n","           1       0.00      0.00      0.00       756\n","\n","    accuracy                           0.46      1388\n","   macro avg       0.23      0.50      0.31      1388\n","weighted avg       0.21      0.46      0.28      1388\n","\n","[[632   0]\n"," [756   0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-08 02:17:01,248]\u001b[0m Trial 15 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 15 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 849\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='348' max='849' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [348/849 05:08 < 07:26, 1.12 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.617877</td>\n","      <td>0.682997</td>\n","      <td>0.669749</td>\n","      <td>0.687232</td>\n","      <td>0.670455</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.70      0.53      0.60       632\n","           1       0.67      0.81      0.74       756\n","\n","    accuracy                           0.68      1388\n","   macro avg       0.69      0.67      0.67      1388\n","weighted avg       0.69      0.68      0.68      1388\n","\n","[[335 297]\n"," [143 613]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-08 02:22:41,755]\u001b[0m Trial 16 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 16 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4210\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4210' max='4210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4210/4210 12:35, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.846000</td>\n","      <td>0.712204</td>\n","      <td>0.751441</td>\n","      <td>0.750928</td>\n","      <td>0.751377</td>\n","      <td>0.753399</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.769800</td>\n","      <td>0.948782</td>\n","      <td>0.754323</td>\n","      <td>0.750168</td>\n","      <td>0.754012</td>\n","      <td>0.748778</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.71      0.78      0.74       632\n","           1       0.80      0.73      0.76       756\n","\n","    accuracy                           0.75      1388\n","   macro avg       0.75      0.75      0.75      1388\n","weighted avg       0.76      0.75      0.75      1388\n","\n","[[490 142]\n"," [203 553]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.75      0.69      0.72       632\n","           1       0.76      0.81      0.78       756\n","\n","    accuracy                           0.75      1388\n","   macro avg       0.75      0.75      0.75      1388\n","weighted avg       0.75      0.75      0.75      1388\n","\n","[[434 198]\n"," [143 613]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-08 02:35:19,567]\u001b[0m Trial 17 finished with value: 0.7501677220885898 and parameters: {'learning_rate': 2.1025362834713015e-05, 'per_device_train_batch_size': 2, 'weight_decay': 5.167420942207734e-06, 'num_train_epochs': 1.5165589271751867}. Best is trial 13 with value: 0.7573360126551616.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 17 finished with value: 0.7501677220885898 and parameters: {'learning_rate': 2.1025362834713015e-05, 'per_device_train_batch_size': 2, 'weight_decay': 5.167420942207734e-06, 'num_train_epochs': 1.5165589271751867}. Best is trial 13 with value: 0.7573360126551616.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 405\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='348' max='405' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [348/405 05:08 < 00:50, 1.12 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.692968</td>\n","      <td>0.544669</td>\n","      <td>0.352612</td>\n","      <td>0.272334</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       632\n","           1       0.54      1.00      0.71       756\n","\n","    accuracy                           0.54      1388\n","   macro avg       0.27      0.50      0.35      1388\n","weighted avg       0.30      0.54      0.38      1388\n","\n","[[  0 632]\n"," [  0 756]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-08 02:40:59,724]\u001b[0m Trial 18 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 18 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 658\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='348' max='658' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [348/658 05:08 < 04:36, 1.12 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.726750</td>\n","      <td>0.455331</td>\n","      <td>0.312871</td>\n","      <td>0.227666</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.46      1.00      0.63       632\n","           1       0.00      0.00      0.00       756\n","\n","    accuracy                           0.46      1388\n","   macro avg       0.23      0.50      0.31      1388\n","weighted avg       0.21      0.46      0.28      1388\n","\n","[[632   0]\n"," [756   0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-08 02:46:39,807]\u001b[0m Trial 19 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 19 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6639\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6639' max='6639' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6639/6639 19:38, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.859800</td>\n","      <td>0.740463</td>\n","      <td>0.729827</td>\n","      <td>0.729803</td>\n","      <td>0.737427</td>\n","      <td>0.736542</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.822300</td>\n","      <td>0.943497</td>\n","      <td>0.755043</td>\n","      <td>0.749192</td>\n","      <td>0.757414</td>\n","      <td>0.747363</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.682000</td>\n","      <td>1.018036</td>\n","      <td>0.764409</td>\n","      <td>0.761356</td>\n","      <td>0.763227</td>\n","      <td>0.760373</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.67      0.81      0.73       632\n","           1       0.81      0.66      0.73       756\n","\n","    accuracy                           0.73      1388\n","   macro avg       0.74      0.74      0.73      1388\n","weighted avg       0.74      0.73      0.73      1388\n","\n","[[513 119]\n"," [256 500]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.77      0.66      0.71       632\n","           1       0.75      0.83      0.79       756\n","\n","    accuracy                           0.76      1388\n","   macro avg       0.76      0.75      0.75      1388\n","weighted avg       0.76      0.76      0.75      1388\n","\n","[[418 214]\n"," [126 630]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.75      0.72      0.73       632\n","           1       0.77      0.81      0.79       756\n","\n","    accuracy                           0.76      1388\n","   macro avg       0.76      0.76      0.76      1388\n","weighted avg       0.76      0.76      0.76      1388\n","\n","[[452 180]\n"," [147 609]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-08 03:06:20,868]\u001b[0m Trial 20 finished with value: 0.7613559108152659 and parameters: {'learning_rate': 9.550187523162385e-06, 'per_device_train_batch_size': 2, 'weight_decay': 7.344308135535765e-11, 'num_train_epochs': 2.391488658266402}. Best is trial 20 with value: 0.7613559108152659.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 20 finished with value: 0.7613559108152659 and parameters: {'learning_rate': 9.550187523162385e-06, 'per_device_train_batch_size': 2, 'weight_decay': 7.344308135535765e-11, 'num_train_epochs': 2.391488658266402}. Best is trial 20 with value: 0.7613559108152659.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 7329\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='7329' max='7329' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7329/7329 21:43, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.858600</td>\n","      <td>0.737413</td>\n","      <td>0.733429</td>\n","      <td>0.733374</td>\n","      <td>0.737001</td>\n","      <td>0.738032</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.794600</td>\n","      <td>0.927440</td>\n","      <td>0.757205</td>\n","      <td>0.753283</td>\n","      <td>0.756738</td>\n","      <td>0.751942</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.699100</td>\n","      <td>1.024544</td>\n","      <td>0.754323</td>\n","      <td>0.750354</td>\n","      <td>0.753778</td>\n","      <td>0.749037</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.68      0.79      0.73       632\n","           1       0.80      0.69      0.74       756\n","\n","    accuracy                           0.73      1388\n","   macro avg       0.74      0.74      0.73      1388\n","weighted avg       0.74      0.73      0.73      1388\n","\n","[[499 133]\n"," [237 519]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.75      0.69      0.72       632\n","           1       0.76      0.81      0.78       756\n","\n","    accuracy                           0.76      1388\n","   macro avg       0.76      0.75      0.75      1388\n","weighted avg       0.76      0.76      0.76      1388\n","\n","[[438 194]\n"," [143 613]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.75      0.69      0.72       632\n","           1       0.76      0.81      0.78       756\n","\n","    accuracy                           0.75      1388\n","   macro avg       0.75      0.75      0.75      1388\n","weighted avg       0.75      0.75      0.75      1388\n","\n","[[436 196]\n"," [145 611]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-08 03:28:07,512]\u001b[0m Trial 21 finished with value: 0.7503543136600246 and parameters: {'learning_rate': 1.1044057137355722e-05, 'per_device_train_batch_size': 2, 'weight_decay': 2.470141657816699e-11, 'num_train_epochs': 2.640014269888514}. Best is trial 20 with value: 0.7613559108152659.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 21 finished with value: 0.7503543136600246 and parameters: {'learning_rate': 1.1044057137355722e-05, 'per_device_train_batch_size': 2, 'weight_decay': 2.470141657816699e-11, 'num_train_epochs': 2.640014269888514}. Best is trial 20 with value: 0.7613559108152659.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6207\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2777' max='6207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2777/6207 07:45 < 09:35, 5.96 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.696900</td>\n","      <td>0.694502</td>\n","      <td>0.455331</td>\n","      <td>0.312871</td>\n","      <td>0.227666</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.46      1.00      0.63       632\n","           1       0.00      0.00      0.00       756\n","\n","    accuracy                           0.46      1388\n","   macro avg       0.23      0.50      0.31      1388\n","weighted avg       0.21      0.46      0.28      1388\n","\n","[[632   0]\n"," [756   0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-08 03:36:24,319]\u001b[0m Trial 22 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 22 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4503\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2777' max='4503' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2777/4503 07:47 < 04:50, 5.93 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.622600</td>\n","      <td>0.605707</td>\n","      <td>0.685879</td>\n","      <td>0.674637</td>\n","      <td>0.688281</td>\n","      <td>0.674658</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.70      0.55      0.61       632\n","           1       0.68      0.80      0.74       756\n","\n","    accuracy                           0.69      1388\n","   macro avg       0.69      0.67      0.67      1388\n","weighted avg       0.69      0.69      0.68      1388\n","\n","[[347 285]\n"," [151 605]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-08 03:44:43,533]\u001b[0m Trial 23 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 23 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 8963\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2777' max='8963' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2777/8963 07:48 < 17:24, 5.92 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.702300</td>\n","      <td>0.694631</td>\n","      <td>0.455331</td>\n","      <td>0.312871</td>\n","      <td>0.227666</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.46      1.00      0.63       632\n","           1       0.00      0.00      0.00       756\n","\n","    accuracy                           0.46      1388\n","   macro avg       0.23      0.50      0.31      1388\n","weighted avg       0.21      0.46      0.28      1388\n","\n","[[632   0]\n"," [756   0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-08 03:53:03,485]\u001b[0m Trial 24 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 24 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5978\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5978' max='5978' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5978/5978 18:14, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.845300</td>\n","      <td>0.745943</td>\n","      <td>0.732709</td>\n","      <td>0.732705</td>\n","      <td>0.739251</td>\n","      <td>0.738928</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.811300</td>\n","      <td>0.944119</td>\n","      <td>0.755764</td>\n","      <td>0.749873</td>\n","      <td>0.758271</td>\n","      <td>0.748024</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.811300</td>\n","      <td>0.946383</td>\n","      <td>0.752882</td>\n","      <td>0.748797</td>\n","      <td>0.752411</td>\n","      <td>0.747455</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.67      0.81      0.73       632\n","           1       0.81      0.67      0.73       756\n","\n","    accuracy                           0.73      1388\n","   macro avg       0.74      0.74      0.73      1388\n","weighted avg       0.75      0.73      0.73      1388\n","\n","[[511 121]\n"," [250 506]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.77      0.66      0.71       632\n","           1       0.75      0.83      0.79       756\n","\n","    accuracy                           0.76      1388\n","   macro avg       0.76      0.75      0.75      1388\n","weighted avg       0.76      0.76      0.75      1388\n","\n","[[418 214]\n"," [125 631]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.75      0.69      0.72       632\n","           1       0.76      0.81      0.78       756\n","\n","    accuracy                           0.75      1388\n","   macro avg       0.75      0.75      0.75      1388\n","weighted avg       0.75      0.75      0.75      1388\n","\n","[[434 198]\n"," [145 611]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-08 04:11:21,032]\u001b[0m Trial 25 finished with value: 0.7487968383534321 and parameters: {'learning_rate': 8.39487800112118e-06, 'per_device_train_batch_size': 2, 'weight_decay': 1.8248832708908834e-06, 'num_train_epochs': 2.153328431655919}. Best is trial 20 with value: 0.7613559108152659.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 25 finished with value: 0.7487968383534321 and parameters: {'learning_rate': 8.39487800112118e-06, 'per_device_train_batch_size': 2, 'weight_decay': 1.8248832708908834e-06, 'num_train_epochs': 2.153328431655919}. Best is trial 20 with value: 0.7613559108152659.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 679\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='348' max='679' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [348/679 05:08 < 04:55, 1.12 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.691977</td>\n","      <td>0.544669</td>\n","      <td>0.352612</td>\n","      <td>0.272334</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       632\n","           1       0.54      1.00      0.71       756\n","\n","    accuracy                           0.54      1388\n","   macro avg       0.27      0.50      0.35      1388\n","weighted avg       0.30      0.54      0.38      1388\n","\n","[[  0 632]\n"," [  0 756]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-08 04:17:02,020]\u001b[0m Trial 26 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 26 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2167\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1389' max='2167' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1389/2167 06:18 < 03:32, 3.66 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.685600</td>\n","      <td>0.685836</td>\n","      <td>0.556916</td>\n","      <td>0.468917</td>\n","      <td>0.551700</td>\n","      <td>0.524739</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.54      0.16      0.25       632\n","           1       0.56      0.88      0.69       756\n","\n","    accuracy                           0.56      1388\n","   macro avg       0.55      0.52      0.47      1388\n","weighted avg       0.55      0.56      0.49      1388\n","\n","[[104 528]\n"," [ 87 669]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-08 04:23:52,402]\u001b[0m Trial 27 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 27 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6856\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2777' max='6856' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2777/6856 07:42 < 11:20, 6.00 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.338800</td>\n","      <td>0.699171</td>\n","      <td>0.544669</td>\n","      <td>0.352612</td>\n","      <td>0.272334</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       632\n","           1       0.54      1.00      0.71       756\n","\n","    accuracy                           0.54      1388\n","   macro avg       0.27      0.50      0.35      1388\n","weighted avg       0.30      0.54      0.38      1388\n","\n","[[  0 632]\n"," [  0 756]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-08 04:32:06,552]\u001b[0m Trial 28 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 28 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3662\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='695' max='3662' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 695/3662 05:35 < 23:58, 2.06 it/s, Epoch 1/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.674800</td>\n","      <td>0.653137</td>\n","      <td>0.638329</td>\n","      <td>0.608689</td>\n","      <td>0.649522</td>\n","      <td>0.618679</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.67      0.40      0.50       632\n","           1       0.63      0.84      0.72       756\n","\n","    accuracy                           0.64      1388\n","   macro avg       0.65      0.62      0.61      1388\n","weighted avg       0.65      0.64      0.62      1388\n","\n","[[252 380]\n"," [122 634]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-08 04:38:14,029]\u001b[0m Trial 29 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 29 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 414\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='414' max='414' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [414/414 07:05, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.525045</td>\n","      <td>0.755764</td>\n","      <td>0.751249</td>\n","      <td>0.756024</td>\n","      <td>0.749711</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.518691</td>\n","      <td>0.747839</td>\n","      <td>0.746726</td>\n","      <td>0.746368</td>\n","      <td>0.747886</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.76      0.68      0.72       632\n","           1       0.75      0.82      0.78       756\n","\n","    accuracy                           0.76      1388\n","   macro avg       0.76      0.75      0.75      1388\n","weighted avg       0.76      0.76      0.75      1388\n","\n","[[431 201]\n"," [138 618]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.71      0.75      0.73       632\n","           1       0.78      0.75      0.76       756\n","\n","    accuracy                           0.75      1388\n","   macro avg       0.75      0.75      0.75      1388\n","weighted avg       0.75      0.75      0.75      1388\n","\n","[[473 159]\n"," [191 565]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-08 04:45:23,038]\u001b[0m Trial 30 finished with value: 0.746725892559226 and parameters: {'learning_rate': 4.519860073508858e-05, 'per_device_train_batch_size': 16, 'weight_decay': 2.2169028347942e-06, 'num_train_epochs': 1.191412553838028}. Best is trial 20 with value: 0.7613559108152659.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 30 finished with value: 0.746725892559226 and parameters: {'learning_rate': 4.519860073508858e-05, 'per_device_train_batch_size': 16, 'weight_decay': 2.2169028347942e-06, 'num_train_epochs': 1.191412553838028}. Best is trial 20 with value: 0.7613559108152659.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 734\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='348' max='734' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [348/734 05:08 < 05:44, 1.12 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.690739</td>\n","      <td>0.544669</td>\n","      <td>0.352612</td>\n","      <td>0.272334</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       632\n","           1       0.54      1.00      0.71       756\n","\n","    accuracy                           0.54      1388\n","   macro avg       0.27      0.50      0.35      1388\n","weighted avg       0.30      0.54      0.38      1388\n","\n","[[  0 632]\n"," [  0 756]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-08 04:51:03,629]\u001b[0m Trial 31 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 31 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1128\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='348' max='1128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 348/1128 05:08 < 11:35, 1.12 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.534934</td>\n","      <td>0.714697</td>\n","      <td>0.714198</td>\n","      <td>0.729276</td>\n","      <td>0.724211</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.64      0.83      0.73       632\n","           1       0.81      0.62      0.70       756\n","\n","    accuracy                           0.71      1388\n","   macro avg       0.73      0.72      0.71      1388\n","weighted avg       0.74      0.71      0.71      1388\n","\n","[[525 107]\n"," [289 467]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-08 04:56:43,835]\u001b[0m Trial 32 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 32 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 780\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='695' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [695/780 10:46 < 01:19, 1.07 it/s, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.532987</td>\n","      <td>0.728386</td>\n","      <td>0.728047</td>\n","      <td>0.729303</td>\n","      <td>0.731067</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.561700</td>\n","      <td>0.521655</td>\n","      <td>0.750000</td>\n","      <td>0.744645</td>\n","      <td>0.751127</td>\n","      <td>0.742993</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.68      0.76      0.72       632\n","           1       0.78      0.70      0.74       756\n","\n","    accuracy                           0.73      1388\n","   macro avg       0.73      0.73      0.73      1388\n","weighted avg       0.73      0.73      0.73      1388\n","\n","[[481 151]\n"," [226 530]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.76      0.66      0.71       632\n","           1       0.75      0.82      0.78       756\n","\n","    accuracy                           0.75      1388\n","   macro avg       0.75      0.74      0.74      1388\n","weighted avg       0.75      0.75      0.75      1388\n","\n","[[420 212]\n"," [135 621]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-08 05:08:01,995]\u001b[0m Trial 33 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 33 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1269\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='695' max='1269' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 695/1269 05:34 < 04:36, 2.07 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.649600</td>\n","      <td>0.587607</td>\n","      <td>0.690922</td>\n","      <td>0.676916</td>\n","      <td>0.697435</td>\n","      <td>0.677730</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.72      0.53      0.61       632\n","           1       0.68      0.83      0.74       756\n","\n","    accuracy                           0.69      1388\n","   macro avg       0.70      0.68      0.68      1388\n","weighted avg       0.70      0.69      0.68      1388\n","\n","[[335 297]\n"," [132 624]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-08 05:14:07,514]\u001b[0m Trial 34 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 34 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1061\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='348' max='1061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 348/1061 05:08 < 10:35, 1.12 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.693082</td>\n","      <td>0.544669</td>\n","      <td>0.352612</td>\n","      <td>0.272334</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       632\n","           1       0.54      1.00      0.71       756\n","\n","    accuracy                           0.54      1388\n","   macro avg       0.27      0.50      0.35      1388\n","weighted avg       0.30      0.54      0.38      1388\n","\n","[[  0 632]\n"," [  0 756]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-08 05:19:47,568]\u001b[0m Trial 35 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 35 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3670\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3670' max='3670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3670/3670 17:56, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.578300</td>\n","      <td>0.539279</td>\n","      <td>0.727666</td>\n","      <td>0.727312</td>\n","      <td>0.741155</td>\n","      <td>0.736764</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.517800</td>\n","      <td>0.547909</td>\n","      <td>0.753602</td>\n","      <td>0.750531</td>\n","      <td>0.752111</td>\n","      <td>0.749673</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.517500</td>\n","      <td>0.585910</td>\n","      <td>0.755043</td>\n","      <td>0.751990</td>\n","      <td>0.753578</td>\n","      <td>0.751126</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.66      0.84      0.74       632\n","           1       0.82      0.63      0.72       756\n","\n","    accuracy                           0.73      1388\n","   macro avg       0.74      0.74      0.73      1388\n","weighted avg       0.75      0.73      0.73      1388\n","\n","[[530 102]\n"," [276 480]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.74      0.71      0.72       632\n","           1       0.76      0.79      0.78       756\n","\n","    accuracy                           0.75      1388\n","   macro avg       0.75      0.75      0.75      1388\n","weighted avg       0.75      0.75      0.75      1388\n","\n","[[446 186]\n"," [156 600]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.74      0.71      0.72       632\n","           1       0.76      0.79      0.78       756\n","\n","    accuracy                           0.76      1388\n","   macro avg       0.75      0.75      0.75      1388\n","weighted avg       0.75      0.76      0.75      1388\n","\n","[[447 185]\n"," [155 601]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-08 05:37:46,953]\u001b[0m Trial 36 finished with value: 0.7519901956456392 and parameters: {'learning_rate': 5.933575828730239e-06, 'per_device_train_batch_size': 4, 'weight_decay': 0.00031550587547210827, 'num_train_epochs': 2.64380532657309}. Best is trial 20 with value: 0.7613559108152659.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 36 finished with value: 0.7519901956456392 and parameters: {'learning_rate': 5.933575828730239e-06, 'per_device_train_batch_size': 4, 'weight_decay': 0.00031550587547210827, 'num_train_epochs': 2.64380532657309}. Best is trial 20 with value: 0.7613559108152659.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2557\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='695' max='2557' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 695/2557 05:34 < 14:59, 2.07 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.648200</td>\n","      <td>0.576452</td>\n","      <td>0.703170</td>\n","      <td>0.699662</td>\n","      <td>0.739130</td>\n","      <td>0.718560</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.89      0.73       632\n","           1       0.86      0.55      0.67       756\n","\n","    accuracy                           0.70      1388\n","   macro avg       0.74      0.72      0.70      1388\n","weighted avg       0.75      0.70      0.70      1388\n","\n","[[563  69]\n"," [343 413]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-08 05:43:53,267]\u001b[0m Trial 37 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 37 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2309\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1389' max='2309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1389/2309 06:14 < 04:08, 3.70 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.680200</td>\n","      <td>0.676245</td>\n","      <td>0.567723</td>\n","      <td>0.453754</td>\n","      <td>0.596379</td>\n","      <td>0.531156</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.63      0.12      0.20       632\n","           1       0.56      0.94      0.70       756\n","\n","    accuracy                           0.57      1388\n","   macro avg       0.60      0.53      0.45      1388\n","weighted avg       0.59      0.57      0.48      1388\n","\n","[[ 77 555]\n"," [ 45 711]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-08 05:50:39,170]\u001b[0m Trial 38 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 38 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 708\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='348' max='708' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [348/708 05:08 < 05:21, 1.12 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.689174</td>\n","      <td>0.544669</td>\n","      <td>0.352612</td>\n","      <td>0.272334</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       632\n","           1       0.54      1.00      0.71       756\n","\n","    accuracy                           0.54      1388\n","   macro avg       0.27      0.50      0.35      1388\n","weighted avg       0.30      0.54      0.38      1388\n","\n","[[  0 632]\n"," [  0 756]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-08 05:56:19,673]\u001b[0m Trial 39 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 39 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6467\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2777' max='6467' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2777/6467 07:38 < 10:10, 6.05 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.683000</td>\n","      <td>0.659126</td>\n","      <td>0.716859</td>\n","      <td>0.716561</td>\n","      <td>0.718084</td>\n","      <td>0.719706</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.67      0.75      0.71       632\n","           1       0.77      0.69      0.73       756\n","\n","    accuracy                           0.72      1388\n","   macro avg       0.72      0.72      0.72      1388\n","weighted avg       0.72      0.72      0.72      1388\n","\n","[[475 157]\n"," [236 520]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-08 06:04:29,674]\u001b[0m Trial 40 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 40 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3879\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1389' max='3879' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1389/3879 06:15 < 11:13, 3.70 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>64.202000</td>\n","      <td>7.347014</td>\n","      <td>0.544669</td>\n","      <td>0.352612</td>\n","      <td>0.272334</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       632\n","           1       0.54      1.00      0.71       756\n","\n","    accuracy                           0.54      1388\n","   macro avg       0.27      0.50      0.35      1388\n","weighted avg       0.30      0.54      0.38      1388\n","\n","[[  0 632]\n"," [  0 756]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-08 06:11:16,350]\u001b[0m Trial 41 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 41 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3714\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3714' max='3714' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3714/3714 18:08, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.609100</td>\n","      <td>0.537753</td>\n","      <td>0.744236</td>\n","      <td>0.744236</td>\n","      <td>0.750354</td>\n","      <td>0.750289</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.595400</td>\n","      <td>0.712441</td>\n","      <td>0.762968</td>\n","      <td>0.760279</td>\n","      <td>0.761419</td>\n","      <td>0.759569</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.507000</td>\n","      <td>0.881253</td>\n","      <td>0.746398</td>\n","      <td>0.741658</td>\n","      <td>0.746410</td>\n","      <td>0.740205</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.68      0.82      0.74       632\n","           1       0.82      0.68      0.74       756\n","\n","    accuracy                           0.74      1388\n","   macro avg       0.75      0.75      0.74      1388\n","weighted avg       0.76      0.74      0.74      1388\n","\n","[[517 115]\n"," [240 516]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.75      0.72      0.73       632\n","           1       0.77      0.80      0.79       756\n","\n","    accuracy                           0.76      1388\n","   macro avg       0.76      0.76      0.76      1388\n","weighted avg       0.76      0.76      0.76      1388\n","\n","[[456 176]\n"," [153 603]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.75      0.67      0.71       632\n","           1       0.75      0.81      0.78       756\n","\n","    accuracy                           0.75      1388\n","   macro avg       0.75      0.74      0.74      1388\n","weighted avg       0.75      0.75      0.74      1388\n","\n","[[424 208]\n"," [144 612]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-08 06:29:27,399]\u001b[0m Trial 42 finished with value: 0.7416582064297801 and parameters: {'learning_rate': 2.7603341256578127e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.0197974863223102, 'num_train_epochs': 2.6753126014096016}. Best is trial 20 with value: 0.7613559108152659.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 42 finished with value: 0.7416582064297801 and parameters: {'learning_rate': 2.7603341256578127e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.0197974863223102, 'num_train_epochs': 2.6753126014096016}. Best is trial 20 with value: 0.7613559108152659.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6354\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1389' max='6354' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1389/6354 06:14 < 22:19, 3.71 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.580800</td>\n","      <td>0.536114</td>\n","      <td>0.723343</td>\n","      <td>0.722716</td>\n","      <td>0.739737</td>\n","      <td>0.733445</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.65      0.85      0.74       632\n","           1       0.83      0.62      0.71       756\n","\n","    accuracy                           0.72      1388\n","   macro avg       0.74      0.73      0.72      1388\n","weighted avg       0.75      0.72      0.72      1388\n","\n","[[535  97]\n"," [287 469]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-08 06:36:12,796]\u001b[0m Trial 43 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 43 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3409\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2777' max='3409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2777/3409 13:00 < 02:57, 3.56 it/s, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.576900</td>\n","      <td>0.536753</td>\n","      <td>0.732709</td>\n","      <td>0.732622</td>\n","      <td>0.735761</td>\n","      <td>0.736982</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.525200</td>\n","      <td>0.548988</td>\n","      <td>0.737032</td>\n","      <td>0.730690</td>\n","      <td>0.738609</td>\n","      <td>0.729142</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.68      0.78      0.73       632\n","           1       0.79      0.69      0.74       756\n","\n","    accuracy                           0.73      1388\n","   macro avg       0.74      0.74      0.73      1388\n","weighted avg       0.74      0.73      0.73      1388\n","\n","[[496 136]\n"," [235 521]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.75      0.64      0.69       632\n","           1       0.73      0.82      0.77       756\n","\n","    accuracy                           0.74      1388\n","   macro avg       0.74      0.73      0.73      1388\n","weighted avg       0.74      0.74      0.73      1388\n","\n","[[405 227]\n"," [138 618]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-08 06:49:44,577]\u001b[0m Trial 44 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 44 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4104\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4104' max='4104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4104/4104 19:55, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.593600</td>\n","      <td>0.530298</td>\n","      <td>0.744957</td>\n","      <td>0.744914</td>\n","      <td>0.753474</td>\n","      <td>0.752118</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.573400</td>\n","      <td>0.645246</td>\n","      <td>0.766571</td>\n","      <td>0.764871</td>\n","      <td>0.764690</td>\n","      <td>0.765082</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.455400</td>\n","      <td>0.902964</td>\n","      <td>0.757205</td>\n","      <td>0.753551</td>\n","      <td>0.756413</td>\n","      <td>0.752332</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.68      0.83      0.75       632\n","           1       0.83      0.67      0.74       756\n","\n","    accuracy                           0.74      1388\n","   macro avg       0.75      0.75      0.74      1388\n","weighted avg       0.76      0.74      0.74      1388\n","\n","[[526 106]\n"," [248 508]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.74      0.75      0.74       632\n","           1       0.79      0.78      0.78       756\n","\n","    accuracy                           0.77      1388\n","   macro avg       0.76      0.77      0.76      1388\n","weighted avg       0.77      0.77      0.77      1388\n","\n","[[473 159]\n"," [165 591]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.75      0.70      0.72       632\n","           1       0.76      0.81      0.78       756\n","\n","    accuracy                           0.76      1388\n","   macro avg       0.76      0.75      0.75      1388\n","weighted avg       0.76      0.76      0.76      1388\n","\n","[[441 191]\n"," [146 610]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-08 07:09:42,610]\u001b[0m Trial 45 finished with value: 0.7535510065158645 and parameters: {'learning_rate': 1.6165531171201104e-05, 'per_device_train_batch_size': 4, 'weight_decay': 6.308241593768865e-05, 'num_train_epochs': 2.9562366228147448}. Best is trial 20 with value: 0.7613559108152659.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 45 finished with value: 0.7535510065158645 and parameters: {'learning_rate': 1.6165531171201104e-05, 'per_device_train_batch_size': 4, 'weight_decay': 6.308241593768865e-05, 'num_train_epochs': 2.9562366228147448}. Best is trial 20 with value: 0.7613559108152659.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4011\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4012' max='4011' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4011/4011 11:32, Epoch 1.44/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.847100</td>\n","      <td>0.741917</td>\n","      <td>0.752161</td>\n","      <td>0.748739</td>\n","      <td>0.750935</td>\n","      <td>0.747702</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.791400</td>\n","      <td>0.929156</td>\n","      <td>0.746398</td>\n","      <td>0.740572</td>\n","      <td>0.747975</td>\n","      <td>0.738907</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.74      0.70      0.72       632\n","           1       0.76      0.80      0.78       756\n","\n","    accuracy                           0.75      1388\n","   macro avg       0.75      0.75      0.75      1388\n","weighted avg       0.75      0.75      0.75      1388\n","\n","[[441 191]\n"," [153 603]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.76      0.66      0.70       632\n","           1       0.74      0.82      0.78       756\n","\n","    accuracy                           0.75      1388\n","   macro avg       0.75      0.74      0.74      1388\n","weighted avg       0.75      0.75      0.74      1388\n","\n","[[414 218]\n"," [134 622]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-08 07:21:46,112]\u001b[0m Trial 46 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 46 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 605\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='348' max='605' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [348/605 05:08 < 03:49, 1.12 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.546162</td>\n","      <td>0.709654</td>\n","      <td>0.707431</td>\n","      <td>0.737980</td>\n","      <td>0.723214</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.63      0.88      0.73       632\n","           1       0.85      0.57      0.68       756\n","\n","    accuracy                           0.71      1388\n","   macro avg       0.74      0.72      0.71      1388\n","weighted avg       0.75      0.71      0.71      1388\n","\n","[[553  79]\n"," [324 432]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-08 07:27:26,961]\u001b[0m Trial 47 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 47 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2518\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='695' max='2518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 695/2518 05:34 < 14:40, 2.07 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.623000</td>\n","      <td>0.557966</td>\n","      <td>0.715418</td>\n","      <td>0.713586</td>\n","      <td>0.741564</td>\n","      <td>0.728376</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.64      0.87      0.74       632\n","           1       0.85      0.58      0.69       756\n","\n","    accuracy                           0.72      1388\n","   macro avg       0.74      0.73      0.71      1388\n","weighted avg       0.75      0.72      0.71      1388\n","\n","[[552  80]\n"," [315 441]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-08 07:33:33,247]\u001b[0m Trial 48 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 48 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running training *****\n","  Num examples = 5552\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 8221\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2777' max='8221' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2777/8221 07:37 < 14:58, 6.06 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.701600</td>\n","      <td>0.693869</td>\n","      <td>0.455331</td>\n","      <td>0.312871</td>\n","      <td>0.227666</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 1388\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.46      1.00      0.63       632\n","           1       0.00      0.00      0.00       756\n","\n","    accuracy                           0.46      1388\n","   macro avg       0.23      0.50      0.31      1388\n","weighted avg       0.21      0.46      0.28      1388\n","\n","[[632   0]\n"," [756   0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-08 07:41:42,399]\u001b[0m Trial 49 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 49 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-08 07:41:42,450]\u001b[0m A new study created in memory with name: no-name-ce91c0fc-4eae-400e-9f62-3be4727526a8\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["A new study created in memory with name: no-name-ce91c0fc-4eae-400e-9f62-3be4727526a8\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"rHREpYlgsNkL","executionInfo":{"status":"ok","timestamp":1638949302083,"user_tz":-60,"elapsed":31,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"eb9d772c-3ee5-4523-be2e-89b8671608aa"},"source":["storage_name"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'sqlite:///SIMCSE_BERT_MASK_TF.db'"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"vlqXIjF6sPqH","executionInfo":{"status":"ok","timestamp":1638949302086,"user_tz":-60,"elapsed":30,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"35c7681d-06c1-4642-a9fa-60f1fdabfa99"},"source":["study_name"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'SIMCSE_BERT_MASK_TF'"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mXXqQglwpuZy","executionInfo":{"status":"ok","timestamp":1638949302088,"user_tz":-60,"elapsed":31,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"ce2d1d4f-c297-41a4-942d-28fc8bf513b8"},"source":["study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True, direction=\"maximize\")\n","df = study.trials_dataframe(attrs=(\"number\", \"value\", \"params\", \"state\"))"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-08 07:41:42,572]\u001b[0m Using an existing study with name 'SIMCSE_BERT_MASK_TF' instead of creating a new one.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Using an existing study with name 'SIMCSE_BERT_MASK_TF' instead of creating a new one.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"JzU25SO_tKCP","executionInfo":{"status":"ok","timestamp":1638949302091,"user_tz":-60,"elapsed":30,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"7525e1fb-835b-4647-fa72-60a6d0c3aaa3"},"source":["df"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>number</th>\n","      <th>value</th>\n","      <th>params_learning_rate</th>\n","      <th>params_num_train_epochs</th>\n","      <th>params_per_device_train_batch_size</th>\n","      <th>params_weight_decay</th>\n","      <th>state</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.701530</td>\n","      <td>5.113165e-07</td>\n","      <td>6.905193</td>\n","      <td>8</td>\n","      <td>4.337244e-04</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.727609</td>\n","      <td>2.895712e-05</td>\n","      <td>4.033449</td>\n","      <td>8</td>\n","      <td>1.110801e-09</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.752182</td>\n","      <td>1.038324e-04</td>\n","      <td>2.169266</td>\n","      <td>16</td>\n","      <td>4.717749e-07</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.312871</td>\n","      <td>6.032013e-02</td>\n","      <td>3.150463</td>\n","      <td>4</td>\n","      <td>5.597112e-09</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.352612</td>\n","      <td>3.416136e-04</td>\n","      <td>5.336966</td>\n","      <td>8</td>\n","      <td>1.925918e-02</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>0.499189</td>\n","      <td>3.405867e-07</td>\n","      <td>1.906998</td>\n","      <td>4</td>\n","      <td>1.597199e-09</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>0.312871</td>\n","      <td>1.309907e-01</td>\n","      <td>2.800622</td>\n","      <td>4</td>\n","      <td>1.297187e-04</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>0.312871</td>\n","      <td>1.109630e-02</td>\n","      <td>3.710493</td>\n","      <td>8</td>\n","      <td>3.778855e-08</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>0.476520</td>\n","      <td>9.091989e-08</td>\n","      <td>4.166444</td>\n","      <td>16</td>\n","      <td>5.388756e-07</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>0.739129</td>\n","      <td>3.515249e-06</td>\n","      <td>7.852373</td>\n","      <td>2</td>\n","      <td>1.586525e-04</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>0.352612</td>\n","      <td>1.491741e-03</td>\n","      <td>1.004228</td>\n","      <td>16</td>\n","      <td>4.547035e-01</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>0.754908</td>\n","      <td>3.558788e-05</td>\n","      <td>1.796460</td>\n","      <td>2</td>\n","      <td>2.645371e-05</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>0.745649</td>\n","      <td>2.879491e-05</td>\n","      <td>1.752906</td>\n","      <td>2</td>\n","      <td>1.276402e-11</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>0.757336</td>\n","      <td>2.678132e-05</td>\n","      <td>2.031632</td>\n","      <td>16</td>\n","      <td>3.445073e-06</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>0.733997</td>\n","      <td>4.643864e-06</td>\n","      <td>1.380664</td>\n","      <td>2</td>\n","      <td>1.122795e-05</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>15</td>\n","      <td>0.312871</td>\n","      <td>1.987430e-03</td>\n","      <td>1.309104</td>\n","      <td>2</td>\n","      <td>1.091819e-05</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>0.669749</td>\n","      <td>1.815471e-06</td>\n","      <td>2.445082</td>\n","      <td>16</td>\n","      <td>6.580780e-03</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>0.750168</td>\n","      <td>2.102536e-05</td>\n","      <td>1.516559</td>\n","      <td>2</td>\n","      <td>5.167421e-06</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>18</td>\n","      <td>0.352612</td>\n","      <td>3.018965e-04</td>\n","      <td>1.164955</td>\n","      <td>16</td>\n","      <td>9.316864e-08</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>19</td>\n","      <td>0.312871</td>\n","      <td>1.937929e-03</td>\n","      <td>1.893761</td>\n","      <td>16</td>\n","      <td>2.263450e-03</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>20</td>\n","      <td>0.761356</td>\n","      <td>9.550188e-06</td>\n","      <td>2.391489</td>\n","      <td>2</td>\n","      <td>7.344308e-11</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>21</td>\n","      <td>0.750354</td>\n","      <td>1.104406e-05</td>\n","      <td>2.640014</td>\n","      <td>2</td>\n","      <td>2.470142e-11</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>22</td>\n","      <td>0.312871</td>\n","      <td>1.287870e-04</td>\n","      <td>2.235863</td>\n","      <td>2</td>\n","      <td>2.391096e-10</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>23</td>\n","      <td>0.674637</td>\n","      <td>7.449268e-07</td>\n","      <td>1.621961</td>\n","      <td>2</td>\n","      <td>4.188383e-08</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>24</td>\n","      <td>0.312871</td>\n","      <td>7.368042e-05</td>\n","      <td>3.228603</td>\n","      <td>2</td>\n","      <td>4.121708e-05</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>25</td>\n","      <td>0.748797</td>\n","      <td>8.394878e-06</td>\n","      <td>2.153328</td>\n","      <td>2</td>\n","      <td>1.824883e-06</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>26</td>\n","      <td>0.352612</td>\n","      <td>7.336536e-04</td>\n","      <td>1.954084</td>\n","      <td>16</td>\n","      <td>9.581098e-11</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>27</td>\n","      <td>0.468917</td>\n","      <td>5.792002e-08</td>\n","      <td>1.560646</td>\n","      <td>4</td>\n","      <td>2.590070e-07</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>28</td>\n","      <td>0.352612</td>\n","      <td>7.450962e-03</td>\n","      <td>2.469655</td>\n","      <td>2</td>\n","      <td>9.983281e-09</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>29</td>\n","      <td>0.608689</td>\n","      <td>6.583111e-07</td>\n","      <td>5.276021</td>\n","      <td>8</td>\n","      <td>5.294645e-04</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>30</td>\n","      <td>0.746726</td>\n","      <td>4.519860e-05</td>\n","      <td>1.191413</td>\n","      <td>16</td>\n","      <td>2.216903e-06</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>31</td>\n","      <td>0.352612</td>\n","      <td>1.568519e-04</td>\n","      <td>2.113790</td>\n","      <td>16</td>\n","      <td>4.473112e-05</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>32</td>\n","      <td>0.714198</td>\n","      <td>1.487106e-05</td>\n","      <td>3.249779</td>\n","      <td>16</td>\n","      <td>5.108201e-07</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>33</td>\n","      <td>0.744645</td>\n","      <td>5.535629e-05</td>\n","      <td>2.245908</td>\n","      <td>16</td>\n","      <td>5.163913e-10</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>34</td>\n","      <td>0.676916</td>\n","      <td>1.929700e-06</td>\n","      <td>1.827462</td>\n","      <td>8</td>\n","      <td>7.938809e-02</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>35</td>\n","      <td>0.352612</td>\n","      <td>2.886958e-04</td>\n","      <td>3.055032</td>\n","      <td>16</td>\n","      <td>4.637514e-09</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>36</td>\n","      <td>0.751990</td>\n","      <td>5.933576e-06</td>\n","      <td>2.643805</td>\n","      <td>4</td>\n","      <td>3.155059e-04</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>37</td>\n","      <td>0.699662</td>\n","      <td>9.171945e-05</td>\n","      <td>3.683552</td>\n","      <td>8</td>\n","      <td>3.567091e-05</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>38</td>\n","      <td>0.453754</td>\n","      <td>2.456995e-07</td>\n","      <td>1.662861</td>\n","      <td>4</td>\n","      <td>1.232092e-03</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>39</td>\n","      <td>0.352612</td>\n","      <td>6.163463e-04</td>\n","      <td>2.039712</td>\n","      <td>16</td>\n","      <td>1.996360e-07</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>40</td>\n","      <td>0.716561</td>\n","      <td>2.162009e-06</td>\n","      <td>2.329389</td>\n","      <td>2</td>\n","      <td>1.929283e-09</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>41</td>\n","      <td>0.352612</td>\n","      <td>4.699684e-01</td>\n","      <td>2.794539</td>\n","      <td>4</td>\n","      <td>1.919650e-04</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>42</td>\n","      <td>0.741658</td>\n","      <td>2.760334e-05</td>\n","      <td>2.675313</td>\n","      <td>4</td>\n","      <td>1.979749e-02</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>43</td>\n","      <td>0.722716</td>\n","      <td>6.763867e-06</td>\n","      <td>4.577676</td>\n","      <td>4</td>\n","      <td>9.147783e-06</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>44</td>\n","      <td>0.730690</td>\n","      <td>3.437545e-06</td>\n","      <td>2.455530</td>\n","      <td>4</td>\n","      <td>3.356167e-04</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>45</td>\n","      <td>0.753551</td>\n","      <td>1.616553e-05</td>\n","      <td>2.956237</td>\n","      <td>4</td>\n","      <td>6.308242e-05</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>46</td>\n","      <td>0.740572</td>\n","      <td>1.640859e-05</td>\n","      <td>1.444877</td>\n","      <td>2</td>\n","      <td>7.522174e-07</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>47</td>\n","      <td>0.707431</td>\n","      <td>7.550336e-05</td>\n","      <td>1.743056</td>\n","      <td>16</td>\n","      <td>5.856648e-05</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>48</td>\n","      <td>0.713586</td>\n","      <td>4.541060e-05</td>\n","      <td>3.626942</td>\n","      <td>8</td>\n","      <td>4.433123e-06</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>49</td>\n","      <td>0.312871</td>\n","      <td>2.029749e-04</td>\n","      <td>2.961430</td>\n","      <td>2</td>\n","      <td>1.715312e-05</td>\n","      <td>PRUNED</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    number     value  ...  params_weight_decay     state\n","0        0  0.701530  ...         4.337244e-04  COMPLETE\n","1        1  0.727609  ...         1.110801e-09  COMPLETE\n","2        2  0.752182  ...         4.717749e-07  COMPLETE\n","3        3  0.312871  ...         5.597112e-09  COMPLETE\n","4        4  0.352612  ...         1.925918e-02  COMPLETE\n","5        5  0.499189  ...         1.597199e-09    PRUNED\n","6        6  0.312871  ...         1.297187e-04    PRUNED\n","7        7  0.312871  ...         3.778855e-08    PRUNED\n","8        8  0.476520  ...         5.388756e-07    PRUNED\n","9        9  0.739129  ...         1.586525e-04  COMPLETE\n","10      10  0.352612  ...         4.547035e-01    PRUNED\n","11      11  0.754908  ...         2.645371e-05  COMPLETE\n","12      12  0.745649  ...         1.276402e-11  COMPLETE\n","13      13  0.757336  ...         3.445073e-06  COMPLETE\n","14      14  0.733997  ...         1.122795e-05    PRUNED\n","15      15  0.312871  ...         1.091819e-05    PRUNED\n","16      16  0.669749  ...         6.580780e-03    PRUNED\n","17      17  0.750168  ...         5.167421e-06  COMPLETE\n","18      18  0.352612  ...         9.316864e-08    PRUNED\n","19      19  0.312871  ...         2.263450e-03    PRUNED\n","20      20  0.761356  ...         7.344308e-11  COMPLETE\n","21      21  0.750354  ...         2.470142e-11  COMPLETE\n","22      22  0.312871  ...         2.391096e-10    PRUNED\n","23      23  0.674637  ...         4.188383e-08    PRUNED\n","24      24  0.312871  ...         4.121708e-05    PRUNED\n","25      25  0.748797  ...         1.824883e-06  COMPLETE\n","26      26  0.352612  ...         9.581098e-11    PRUNED\n","27      27  0.468917  ...         2.590070e-07    PRUNED\n","28      28  0.352612  ...         9.983281e-09    PRUNED\n","29      29  0.608689  ...         5.294645e-04    PRUNED\n","30      30  0.746726  ...         2.216903e-06  COMPLETE\n","31      31  0.352612  ...         4.473112e-05    PRUNED\n","32      32  0.714198  ...         5.108201e-07    PRUNED\n","33      33  0.744645  ...         5.163913e-10    PRUNED\n","34      34  0.676916  ...         7.938809e-02    PRUNED\n","35      35  0.352612  ...         4.637514e-09    PRUNED\n","36      36  0.751990  ...         3.155059e-04  COMPLETE\n","37      37  0.699662  ...         3.567091e-05    PRUNED\n","38      38  0.453754  ...         1.232092e-03    PRUNED\n","39      39  0.352612  ...         1.996360e-07    PRUNED\n","40      40  0.716561  ...         1.929283e-09    PRUNED\n","41      41  0.352612  ...         1.919650e-04    PRUNED\n","42      42  0.741658  ...         1.979749e-02  COMPLETE\n","43      43  0.722716  ...         9.147783e-06    PRUNED\n","44      44  0.730690  ...         3.356167e-04    PRUNED\n","45      45  0.753551  ...         6.308242e-05  COMPLETE\n","46      46  0.740572  ...         7.522174e-07    PRUNED\n","47      47  0.707431  ...         5.856648e-05    PRUNED\n","48      48  0.713586  ...         4.433123e-06    PRUNED\n","49      49  0.312871  ...         1.715312e-05    PRUNED\n","\n","[50 rows x 7 columns]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"ef13tgLLtXxY","executionInfo":{"status":"ok","timestamp":1638949304241,"user_tz":-60,"elapsed":2178,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"91bbf6d0-9add-4801-aa39-548643caf86a"},"source":["fig = optuna.visualization.plot_param_importances(study)\n","fig.show()"],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"a215c1ec-9157-43fe-9e90-285ba962458e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"a215c1ec-9157-43fe-9e90-285ba962458e\")) {\n","                    Plotly.newPlot(\n","                        'a215c1ec-9157-43fe-9e90-285ba962458e',\n","                        [{\"cliponaxis\": false, \"hovertemplate\": [\"weight_decay (LogUniformDistribution): 0.06189888884806436<extra></extra>\", \"num_train_epochs (LogUniformDistribution): 0.18654616739674498<extra></extra>\", \"per_device_train_batch_size (CategoricalDistribution): 0.2666712709166536<extra></extra>\", \"learning_rate (LogUniformDistribution): 0.4848836728385371<extra></extra>\"], \"marker\": {\"color\": \"rgb(66,146,198)\"}, \"orientation\": \"h\", \"text\": [\"0.06189888884806436\", \"0.18654616739674498\", \"0.2666712709166536\", \"0.4848836728385371\"], \"textposition\": \"outside\", \"texttemplate\": \"%{text:.2f}\", \"type\": \"bar\", \"x\": [0.06189888884806436, 0.18654616739674498, 0.2666712709166536, 0.4848836728385371], \"y\": [\"weight_decay\", \"num_train_epochs\", \"per_device_train_batch_size\", \"learning_rate\"]}],\n","                        {\"showlegend\": false, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Hyperparameter Importances\"}, \"xaxis\": {\"title\": {\"text\": \"Importance for Objective Value\"}}, \"yaxis\": {\"title\": {\"text\": \"Hyperparameter\"}}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('a215c1ec-9157-43fe-9e90-285ba962458e');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D8f8-e0Us-Yd","executionInfo":{"status":"ok","timestamp":1638949304242,"user_tz":-60,"elapsed":9,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"7e3a83e9-7128-4223-844b-97859482e853"},"source":["best_run"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BestRun(run_id='20', objective=0.7613559108152659, hyperparameters={'learning_rate': 9.550187523162385e-06, 'num_train_epochs': 2.391488658266402, 'per_device_train_batch_size': 2, 'weight_decay': 7.344308135535765e-11})"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"Hr5zSnMFnAfY","executionInfo":{"status":"ok","timestamp":1638949304586,"user_tz":-60,"elapsed":351,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"45373e25-e505-4588-cf9d-4fbf01d5f04c"},"source":["optuna.visualization.plot_intermediate_values(study)"],"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"3ba95058-1235-45c0-8636-3bb4bb1f6553\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"3ba95058-1235-45c0-8636-3bb4bb1f6553\")) {\n","                    Plotly.newPlot(\n","                        '3ba95058-1235-45c0-8636-3bb4bb1f6553',\n","                        [{\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial0\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6], \"y\": [0.5770614675150312, 0.6370518483761021, 0.6898818076081008, 0.6864493981737687, 0.7014329039732843, 0.6999011842409417, 0.7015297451634195]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial1\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4], \"y\": [0.677470969713432, 0.7423908991800965, 0.7353390905866244, 0.7302320668420108, 0.7276088222633006]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial2\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.6467736369910282, 0.7512138059419888, 0.7521821788855245]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial3\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3], \"y\": [0.35261194029850745, 0.31287128712871287, 0.35261194029850745, 0.31287128712871287]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial4\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5], \"y\": [0.31287128712871287, 0.35261194029850745, 0.35261194029850745, 0.35261194029850745, 0.35261194029850745, 0.35261194029850745]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial5\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.4991889509072141]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial6\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.31287128712871287]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial7\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.31287128712871287]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial8\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.47652045742323523]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial9\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7], \"y\": [0.7179763398784514, 0.7438313313313314, 0.7344827642266729, 0.7109358383551931, 0.7460549452577354, 0.7196235150244583, 0.7409551769331586, 0.7391285984592513]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial10\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.35261194029850745]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial11\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.7374883612662942, 0.754907958049808]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial12\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.7496880429365177, 0.7456486000949089]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial13\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.7206768577004499, 0.7590460318662727, 0.7573360126551616]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial14\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.7179481982368254, 0.7339965736817251]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial15\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.31287128712871287]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial16\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.6697489806733505]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial17\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.7509277921537522, 0.7501677220885898]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial18\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.35261194029850745]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial19\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.31287128712871287]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial20\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.7298033871909675, 0.7491921768707482, 0.7613559108152659]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial21\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.7333740364167995, 0.7532827088077074, 0.7503543136600246]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial22\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.31287128712871287]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial23\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.6746373616920611]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial24\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.31287128712871287]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial25\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.7327054651420515, 0.7498733504325754, 0.7487968383534321]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial26\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.35261194029850745]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial27\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.4689168733773445]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial28\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.35261194029850745]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial29\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.6086891082881244]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial30\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.7512485627056817, 0.746725892559226]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial31\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.35261194029850745]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial32\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.7141983589679592]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial33\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.728047239732182, 0.7446450242638017]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial34\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.6769155892248768]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial35\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.35261194029850745]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial36\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.7273118494171651, 0.7505313144435545, 0.7519901956456392]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial37\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.6996624027596]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial38\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.4537540633928641]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial39\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.35261194029850745]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial40\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.7165608653383]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial41\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.35261194029850745]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial42\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.7442361784813523, 0.7602794846018368, 0.7416582064297801]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial43\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.7227159882384033]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial44\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.7326221923144224, 0.7306895955984956]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial45\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.7449138727663507, 0.764871223766352, 0.7535510065158645]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial46\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.7487386202178603, 0.7405717684040609]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial47\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.7074307640084251]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial48\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.7135861391279908]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial49\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.31287128712871287]}],\n","                        {\"showlegend\": false, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Intermediate Values Plot\"}, \"xaxis\": {\"title\": {\"text\": \"Step\"}}, \"yaxis\": {\"title\": {\"text\": \"Intermediate Value\"}}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('3ba95058-1235-45c0-8636-3bb4bb1f6553');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"uFBmj8ysm9pb","executionInfo":{"status":"ok","timestamp":1638949304587,"user_tz":-60,"elapsed":13,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"bd364bb4-7a5b-41d4-9041-7bb1827349f0"},"source":["optuna.visualization.plot_parallel_coordinate(study)"],"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"abc92d2c-8007-4b82-9752-accb091462ef\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"abc92d2c-8007-4b82-9752-accb091462ef\")) {\n","                    Plotly.newPlot(\n","                        'abc92d2c-8007-4b82-9752-accb091462ef',\n","                        [{\"dimensions\": [{\"label\": \"Objective Value\", \"range\": [0.31287128712871287, 0.7613559108152659], \"values\": [0.7391285984592513, 0.754907958049808, 0.7456486000949089, 0.7501677220885898, 0.7613559108152659, 0.7503543136600246, 0.7487968383534321, 0.31287128712871287, 0.7519901956456392, 0.7416582064297801, 0.7535510065158645, 0.7015297451634195, 0.7276088222633006, 0.35261194029850745, 0.7521821788855245, 0.7573360126551616, 0.746725892559226]}, {\"label\": \"learning_rate\", \"range\": [-6.2913102232903935, -1.219537752649488], \"ticktext\": [\"5.11e-07\", \"1e-06\", \"1e-05\", \"0.0001\", \"0.001\", \"0.01\", \"0.0603\"], \"tickvals\": [-6.2913102232903935, -6, -5, -4, -3, -2, -1.219537752649488], \"values\": [-5.454043865832267, -4.448697892665143, -4.540684313801057, -4.677256500844959, -5.019988100722528, -4.956871355168041, -5.075985610880889, -1.219537752649488, -5.226683503153343, -4.559038345417857, -4.791410020657646, -6.2913102232903935, -4.538244624559259, -3.4664648530437643, -3.983666985717524, -4.572168102034882, -4.344875009932781]}, {\"label\": \"num_train_epochs\", \"range\": [0.07606217192016865, 0.8950009113248176], \"ticktext\": [\"1.19\", \"7.85\"], \"tickvals\": [0.07606217192016865, 0.8950009113248176], \"values\": [0.8950009113248176, 0.2544174916753509, 0.2437585556939065, 0.18085928985262814, 0.3786683255533742, 0.4216062743384258, 0.3331102746807424, 0.49837438694789227, 0.4222294732388256, 0.4273745351928737, 0.4707391928722854, 0.839175852664171, 0.6056766227083147, 0.7272944173681013, 0.33631274032628355, 0.3078449614091776, 0.07606217192016865]}, {\"label\": \"per_device_train_...\", \"range\": [0, 3], \"ticktext\": [2, 4, 8, 16], \"tickvals\": [0, 1, 2, 3], \"values\": [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3]}, {\"label\": \"weight_decay\", \"range\": [-10.894012414530643, -1.7033899484077544], \"ticktext\": [\"1.28e-11\", \"1e-10\", \"1e-09\", \"1e-08\", \"1e-07\", \"1e-06\", \"1e-05\", \"0.0001\", \"0.001\", \"0.01\", \"0.0198\"], \"tickvals\": [-10.894012414530643, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1.7033899484077544], \"values\": [-3.7995531291521365, -4.577513485525055, -10.894012414530643, -5.286726159040408, -10.134049110298708, -10.607278140082837, -5.738764910072868, -8.25203602372703, -3.500992548745185, -1.7033899484077544, -4.2000916823641425, -3.362786161007217, -8.954363630699604, -1.715362237358053, -6.326265125716722, -5.462801582424321, -5.654253341273179]}], \"labelangle\": 30, \"labelside\": \"bottom\", \"line\": {\"color\": [0.7391285984592513, 0.754907958049808, 0.7456486000949089, 0.7501677220885898, 0.7613559108152659, 0.7503543136600246, 0.7487968383534321, 0.31287128712871287, 0.7519901956456392, 0.7416582064297801, 0.7535510065158645, 0.7015297451634195, 0.7276088222633006, 0.35261194029850745, 0.7521821788855245, 0.7573360126551616, 0.746725892559226], \"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0.0, \"rgb(247,251,255)\"], [0.125, \"rgb(222,235,247)\"], [0.25, \"rgb(198,219,239)\"], [0.375, \"rgb(158,202,225)\"], [0.5, \"rgb(107,174,214)\"], [0.625, \"rgb(66,146,198)\"], [0.75, \"rgb(33,113,181)\"], [0.875, \"rgb(8,81,156)\"], [1.0, \"rgb(8,48,107)\"]], \"reversescale\": false, \"showscale\": true}, \"type\": \"parcoords\"}],\n","                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Parallel Coordinate Plot\"}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('abc92d2c-8007-4b82-9752-accb091462ef');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"I4B4vAEnDimm","executionInfo":{"status":"ok","timestamp":1638949305468,"user_tz":-60,"elapsed":892,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"3316e8e1-4456-42f0-a0e5-6cae62d37f4f"},"source":["optuna.visualization.plot_optimization_history(study)"],"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"a8d0663e-3257-442c-aec0-ac29b25d61ed\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"a8d0663e-3257-442c-aec0-ac29b25d61ed\")) {\n","                    Plotly.newPlot(\n","                        'a8d0663e-3257-442c-aec0-ac29b25d61ed',\n","                        [{\"mode\": \"markers\", \"name\": \"Objective Value\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 9, 11, 12, 13, 17, 20, 21, 25, 30, 36, 42, 45], \"y\": [0.7015297451634195, 0.7276088222633006, 0.7521821788855245, 0.31287128712871287, 0.35261194029850745, 0.7391285984592513, 0.754907958049808, 0.7456486000949089, 0.7573360126551616, 0.7501677220885898, 0.7613559108152659, 0.7503543136600246, 0.7487968383534321, 0.746725892559226, 0.7519901956456392, 0.7416582064297801, 0.7535510065158645]}, {\"name\": \"Best Value\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 9, 11, 12, 13, 17, 20, 21, 25, 30, 36, 42, 45], \"y\": [0.7015297451634195, 0.7276088222633006, 0.7521821788855245, 0.7521821788855245, 0.7521821788855245, 0.7521821788855245, 0.754907958049808, 0.754907958049808, 0.7573360126551616, 0.7573360126551616, 0.7613559108152659, 0.7613559108152659, 0.7613559108152659, 0.7613559108152659, 0.7613559108152659, 0.7613559108152659, 0.7613559108152659]}],\n","                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Optimization History Plot\"}, \"xaxis\": {\"title\": {\"text\": \"#Trials\"}}, \"yaxis\": {\"title\": {\"text\": \"Objective Value\"}}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('a8d0663e-3257-442c-aec0-ac29b25d61ed');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"_S9z32VznLsH","executionInfo":{"status":"ok","timestamp":1638949306460,"user_tz":-60,"elapsed":1000,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"dea0baf4-aa08-4419-df6c-37623c9f85b5"},"source":["optuna.visualization.plot_contour(study)"],"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"16cf00f4-f609-4604-b401-1bf27f2041a1\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"16cf00f4-f609-4604-b401-1bf27f2041a1\")) {\n","                    Plotly.newPlot(\n","                        '16cf00f4-f609-4604-b401-1bf27f2041a1',\n","                        [{\"type\": \"scatter\", \"xaxis\": \"x\", \"yaxis\": \"y\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": true, \"type\": \"contour\", \"x\": [2.85168238633715e-07, 5.113164637727514e-07, 3.515249330255268e-06, 5.933575828730239e-06, 8.39487800112118e-06, 9.550187523162385e-06, 1.1044057137355722e-05, 1.6165531171201104e-05, 2.1025362834713015e-05, 2.6781315018468085e-05, 2.7603341256578127e-05, 2.879490739702494e-05, 2.8957120638078643e-05, 3.558787916816092e-05, 4.519860073508858e-05, 0.00010383242911913873, 0.00034161359596541536, 0.06032012701524992, 0.10815606319810658], \"xaxis\": \"x5\", \"y\": [1.0842145594819437, 1.191412553838028, 1.5165589271751867, 1.7529057098780358, 1.796459751947201, 2.03163161021332, 2.153328431655919, 2.169265655030074, 2.391488658266402, 2.640014269888514, 2.64380532657309, 2.6753126014096016, 2.9562366228147448, 3.150463025055146, 4.03344948971642, 5.336965770180979, 6.90519349381888, 7.852372823525647, 8.628749242986736], \"yaxis\": \"y5\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.746725892559226, null, null, null, null], [null, null, null, null, null, null, null, null, 0.7501677220885898, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, 0.7456486000949089, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, 0.754907958049808, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, 0.7573360126551616, null, null, null, null, null, null, null, null, null], [null, null, null, null, 0.7487968383534321, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.7521821788855245, null, null, null], [null, null, null, null, null, 0.7613559108152659, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, 0.7503543136600246, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, 0.7519901956456392, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, 0.7416582064297801, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, 0.7535510065158645, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.31287128712871287, null], [null, null, null, null, null, null, null, null, null, null, null, null, 0.7276088222633006, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.35261194029850745, null, null], [null, 0.7015297451634195, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, 0.7391285984592513, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [5.113164637727514e-07, 2.8957120638078643e-05, 0.00010383242911913873, 0.06032012701524992, 0.00034161359596541536, 3.515249330255268e-06, 3.558787916816092e-05, 2.879490739702494e-05, 2.6781315018468085e-05, 2.1025362834713015e-05, 9.550187523162385e-06, 1.1044057137355722e-05, 8.39487800112118e-06, 4.519860073508858e-05, 5.933575828730239e-06, 2.7603341256578127e-05, 1.6165531171201104e-05], \"xaxis\": \"x5\", \"y\": [6.90519349381888, 4.03344948971642, 2.169265655030074, 3.150463025055146, 5.336965770180979, 7.852372823525647, 1.796459751947201, 1.7529057098780358, 2.03163161021332, 1.5165589271751867, 2.391488658266402, 2.640014269888514, 2.153328431655919, 1.191412553838028, 2.64380532657309, 2.6753126014096016, 2.9562366228147448], \"yaxis\": \"y5\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [2.85168238633715e-07, 5.113164637727514e-07, 3.515249330255268e-06, 5.933575828730239e-06, 8.39487800112118e-06, 9.550187523162385e-06, 1.1044057137355722e-05, 1.6165531171201104e-05, 2.1025362834713015e-05, 2.6781315018468085e-05, 2.7603341256578127e-05, 2.879490739702494e-05, 2.8957120638078643e-05, 3.558787916816092e-05, 4.519860073508858e-05, 0.00010383242911913873, 0.00034161359596541536, 0.06032012701524992, 0.10815606319810658], \"xaxis\": \"x9\", \"y\": [1.2999999999999998, 2, 4, 8, 16, 16.7], \"yaxis\": \"y9\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, 0.7391285984592513, null, 0.7487968383534321, 0.7613559108152659, 0.7503543136600246, null, 0.7501677220885898, null, null, 0.7456486000949089, null, 0.754907958049808, null, null, null, null, null], [null, null, null, 0.7519901956456392, null, null, null, 0.7535510065158645, null, null, 0.7416582064297801, null, null, null, null, null, null, 0.31287128712871287, null], [null, 0.7015297451634195, null, null, null, null, null, null, null, null, null, null, 0.7276088222633006, null, null, null, 0.35261194029850745, null, null], [null, null, null, null, null, null, null, null, null, 0.7573360126551616, null, null, null, null, 0.746725892559226, 0.7521821788855245, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [5.113164637727514e-07, 2.8957120638078643e-05, 0.00010383242911913873, 0.06032012701524992, 0.00034161359596541536, 3.515249330255268e-06, 3.558787916816092e-05, 2.879490739702494e-05, 2.6781315018468085e-05, 2.1025362834713015e-05, 9.550187523162385e-06, 1.1044057137355722e-05, 8.39487800112118e-06, 4.519860073508858e-05, 5.933575828730239e-06, 2.7603341256578127e-05, 1.6165531171201104e-05], \"xaxis\": \"x9\", \"y\": [8, 8, 16, 4, 8, 2, 2, 2, 16, 2, 2, 2, 2, 16, 4, 4, 4], \"yaxis\": \"y9\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [2.85168238633715e-07, 5.113164637727514e-07, 3.515249330255268e-06, 5.933575828730239e-06, 8.39487800112118e-06, 9.550187523162385e-06, 1.1044057137355722e-05, 1.6165531171201104e-05, 2.1025362834713015e-05, 2.6781315018468085e-05, 2.7603341256578127e-05, 2.879490739702494e-05, 2.8957120638078643e-05, 3.558787916816092e-05, 4.519860073508858e-05, 0.00010383242911913873, 0.00034161359596541536, 0.06032012701524992, 0.10815606319810658], \"xaxis\": \"x13\", \"y\": [4.4305379630856e-12, 1.2764023216744377e-11, 2.470141657816699e-11, 7.344308135535765e-11, 1.1108012743093335e-09, 5.597111728168741e-09, 4.717749467497328e-07, 1.8248832708908834e-06, 2.2169028347942e-06, 3.4450729101456406e-06, 5.167420942207734e-06, 2.6453705505426204e-05, 6.308241593768865e-05, 0.00015865248205273314, 0.00031550587547210827, 0.0004337243841065953, 0.019259178689961923, 0.0197974863223102, 0.057034964412122906], \"yaxis\": \"y13\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, 0.7456486000949089, null, null, null, null, null, null, null], [null, null, null, null, null, null, 0.7503543136600246, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, 0.7613559108152659, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, 0.7276088222633006, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.31287128712871287, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.7521821788855245, null, null, null], [null, null, null, null, 0.7487968383534321, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.746725892559226, null, null, null, null], [null, null, null, null, null, null, null, null, null, 0.7573360126551616, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, 0.7501677220885898, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, 0.754907958049808, null, null, null, null, null], [null, null, null, null, null, null, null, 0.7535510065158645, null, null, null, null, null, null, null, null, null, null, null], [null, null, 0.7391285984592513, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, 0.7519901956456392, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, 0.7015297451634195, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.35261194029850745, null, null], [null, null, null, null, null, null, null, null, null, null, 0.7416582064297801, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [5.113164637727514e-07, 2.8957120638078643e-05, 0.00010383242911913873, 0.06032012701524992, 0.00034161359596541536, 3.515249330255268e-06, 3.558787916816092e-05, 2.879490739702494e-05, 2.6781315018468085e-05, 2.1025362834713015e-05, 9.550187523162385e-06, 1.1044057137355722e-05, 8.39487800112118e-06, 4.519860073508858e-05, 5.933575828730239e-06, 2.7603341256578127e-05, 1.6165531171201104e-05], \"xaxis\": \"x13\", \"y\": [0.0004337243841065953, 1.1108012743093335e-09, 4.717749467497328e-07, 5.597111728168741e-09, 0.019259178689961923, 0.00015865248205273314, 2.6453705505426204e-05, 1.2764023216744377e-11, 3.4450729101456406e-06, 5.167420942207734e-06, 7.344308135535765e-11, 2.470141657816699e-11, 1.8248832708908834e-06, 2.2169028347942e-06, 0.00031550587547210827, 0.0197974863223102, 6.308241593768865e-05], \"yaxis\": \"y13\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.0842145594819437, 1.191412553838028, 1.5165589271751867, 1.7529057098780358, 1.796459751947201, 2.03163161021332, 2.153328431655919, 2.169265655030074, 2.391488658266402, 2.640014269888514, 2.64380532657309, 2.6753126014096016, 2.9562366228147448, 3.150463025055146, 4.03344948971642, 5.336965770180979, 6.90519349381888, 7.852372823525647, 8.628749242986736], \"xaxis\": \"x2\", \"y\": [2.85168238633715e-07, 5.113164637727514e-07, 3.515249330255268e-06, 5.933575828730239e-06, 8.39487800112118e-06, 9.550187523162385e-06, 1.1044057137355722e-05, 1.6165531171201104e-05, 2.1025362834713015e-05, 2.6781315018468085e-05, 2.7603341256578127e-05, 2.879490739702494e-05, 2.8957120638078643e-05, 3.558787916816092e-05, 4.519860073508858e-05, 0.00010383242911913873, 0.00034161359596541536, 0.06032012701524992, 0.10815606319810658], \"yaxis\": \"y2\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.7015297451634195, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.7391285984592513, null], [null, null, null, null, null, null, null, null, null, null, 0.7519901956456392, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, 0.7487968383534321, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, 0.7613559108152659, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, 0.7503543136600246, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, 0.7535510065158645, null, null, null, null, null, null], [null, null, 0.7501677220885898, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, 0.7573360126551616, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, 0.7416582064297801, null, null, null, null, null, null, null], [null, null, null, 0.7456486000949089, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.7276088222633006, null, null, null, null], [null, null, null, null, 0.754907958049808, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, 0.746725892559226, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, 0.7521821788855245, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.35261194029850745, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, 0.31287128712871287, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [6.90519349381888, 4.03344948971642, 2.169265655030074, 3.150463025055146, 5.336965770180979, 7.852372823525647, 1.796459751947201, 1.7529057098780358, 2.03163161021332, 1.5165589271751867, 2.391488658266402, 2.640014269888514, 2.153328431655919, 1.191412553838028, 2.64380532657309, 2.6753126014096016, 2.9562366228147448], \"xaxis\": \"x2\", \"y\": [5.113164637727514e-07, 2.8957120638078643e-05, 0.00010383242911913873, 0.06032012701524992, 0.00034161359596541536, 3.515249330255268e-06, 3.558787916816092e-05, 2.879490739702494e-05, 2.6781315018468085e-05, 2.1025362834713015e-05, 9.550187523162385e-06, 1.1044057137355722e-05, 8.39487800112118e-06, 4.519860073508858e-05, 5.933575828730239e-06, 2.7603341256578127e-05, 1.6165531171201104e-05], \"yaxis\": \"y2\"}, {\"type\": \"scatter\", \"xaxis\": \"x6\", \"yaxis\": \"y6\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.0842145594819437, 1.191412553838028, 1.5165589271751867, 1.7529057098780358, 1.796459751947201, 2.03163161021332, 2.153328431655919, 2.169265655030074, 2.391488658266402, 2.640014269888514, 2.64380532657309, 2.6753126014096016, 2.9562366228147448, 3.150463025055146, 4.03344948971642, 5.336965770180979, 6.90519349381888, 7.852372823525647, 8.628749242986736], \"xaxis\": \"x10\", \"y\": [1.2999999999999998, 2, 4, 8, 16, 16.7], \"yaxis\": \"y10\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, 0.7501677220885898, 0.7456486000949089, 0.754907958049808, null, 0.7487968383534321, null, 0.7613559108152659, 0.7503543136600246, null, null, null, null, null, null, null, 0.7391285984592513, null], [null, null, null, null, null, null, null, null, null, null, 0.7519901956456392, 0.7416582064297801, 0.7535510065158645, 0.31287128712871287, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.7276088222633006, 0.35261194029850745, 0.7015297451634195, null, null], [null, 0.746725892559226, null, null, null, 0.7573360126551616, null, 0.7521821788855245, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [6.90519349381888, 4.03344948971642, 2.169265655030074, 3.150463025055146, 5.336965770180979, 7.852372823525647, 1.796459751947201, 1.7529057098780358, 2.03163161021332, 1.5165589271751867, 2.391488658266402, 2.640014269888514, 2.153328431655919, 1.191412553838028, 2.64380532657309, 2.6753126014096016, 2.9562366228147448], \"xaxis\": \"x10\", \"y\": [8, 8, 16, 4, 8, 2, 2, 2, 16, 2, 2, 2, 2, 16, 4, 4, 4], \"yaxis\": \"y10\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.0842145594819437, 1.191412553838028, 1.5165589271751867, 1.7529057098780358, 1.796459751947201, 2.03163161021332, 2.153328431655919, 2.169265655030074, 2.391488658266402, 2.640014269888514, 2.64380532657309, 2.6753126014096016, 2.9562366228147448, 3.150463025055146, 4.03344948971642, 5.336965770180979, 6.90519349381888, 7.852372823525647, 8.628749242986736], \"xaxis\": \"x14\", \"y\": [4.4305379630856e-12, 1.2764023216744377e-11, 2.470141657816699e-11, 7.344308135535765e-11, 1.1108012743093335e-09, 5.597111728168741e-09, 4.717749467497328e-07, 1.8248832708908834e-06, 2.2169028347942e-06, 3.4450729101456406e-06, 5.167420942207734e-06, 2.6453705505426204e-05, 6.308241593768865e-05, 0.00015865248205273314, 0.00031550587547210827, 0.0004337243841065953, 0.019259178689961923, 0.0197974863223102, 0.057034964412122906], \"yaxis\": \"y14\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, 0.7456486000949089, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, 0.7503543136600246, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, 0.7613559108152659, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.7276088222633006, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, 0.31287128712871287, null, null, null, null, null], [null, null, null, null, null, null, null, 0.7521821788855245, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, 0.7487968383534321, null, null, null, null, null, null, null, null, null, null, null, null], [null, 0.746725892559226, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, 0.7573360126551616, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, 0.7501677220885898, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, 0.754907958049808, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, 0.7535510065158645, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.7391285984592513, null], [null, null, null, null, null, null, null, null, null, null, 0.7519901956456392, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.7015297451634195, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.35261194029850745, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, 0.7416582064297801, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [6.90519349381888, 4.03344948971642, 2.169265655030074, 3.150463025055146, 5.336965770180979, 7.852372823525647, 1.796459751947201, 1.7529057098780358, 2.03163161021332, 1.5165589271751867, 2.391488658266402, 2.640014269888514, 2.153328431655919, 1.191412553838028, 2.64380532657309, 2.6753126014096016, 2.9562366228147448], \"xaxis\": \"x14\", \"y\": [0.0004337243841065953, 1.1108012743093335e-09, 4.717749467497328e-07, 5.597111728168741e-09, 0.019259178689961923, 0.00015865248205273314, 2.6453705505426204e-05, 1.2764023216744377e-11, 3.4450729101456406e-06, 5.167420942207734e-06, 7.344308135535765e-11, 2.470141657816699e-11, 1.8248832708908834e-06, 2.2169028347942e-06, 0.00031550587547210827, 0.0197974863223102, 6.308241593768865e-05], \"yaxis\": \"y14\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.2999999999999998, 2, 4, 8, 16, 16.7], \"xaxis\": \"x3\", \"y\": [2.85168238633715e-07, 5.113164637727514e-07, 3.515249330255268e-06, 5.933575828730239e-06, 8.39487800112118e-06, 9.550187523162385e-06, 1.1044057137355722e-05, 1.6165531171201104e-05, 2.1025362834713015e-05, 2.6781315018468085e-05, 2.7603341256578127e-05, 2.879490739702494e-05, 2.8957120638078643e-05, 3.558787916816092e-05, 4.519860073508858e-05, 0.00010383242911913873, 0.00034161359596541536, 0.06032012701524992, 0.10815606319810658], \"yaxis\": \"y3\", \"z\": [[null, null, null, null, null, null], [null, null, null, 0.7015297451634195, null, null], [null, 0.7391285984592513, null, null, null, null], [null, null, 0.7519901956456392, null, null, null], [null, 0.7487968383534321, null, null, null, null], [null, 0.7613559108152659, null, null, null, null], [null, 0.7503543136600246, null, null, null, null], [null, null, 0.7535510065158645, null, null, null], [null, 0.7501677220885898, null, null, null, null], [null, null, null, null, 0.7573360126551616, null], [null, null, 0.7416582064297801, null, null, null], [null, 0.7456486000949089, null, null, null, null], [null, null, null, 0.7276088222633006, null, null], [null, 0.754907958049808, null, null, null, null], [null, null, null, null, 0.746725892559226, null], [null, null, null, null, 0.7521821788855245, null], [null, null, null, 0.35261194029850745, null, null], [null, null, 0.31287128712871287, null, null, null], [null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [8, 8, 16, 4, 8, 2, 2, 2, 16, 2, 2, 2, 2, 16, 4, 4, 4], \"xaxis\": \"x3\", \"y\": [5.113164637727514e-07, 2.8957120638078643e-05, 0.00010383242911913873, 0.06032012701524992, 0.00034161359596541536, 3.515249330255268e-06, 3.558787916816092e-05, 2.879490739702494e-05, 2.6781315018468085e-05, 2.1025362834713015e-05, 9.550187523162385e-06, 1.1044057137355722e-05, 8.39487800112118e-06, 4.519860073508858e-05, 5.933575828730239e-06, 2.7603341256578127e-05, 1.6165531171201104e-05], \"yaxis\": \"y3\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.2999999999999998, 2, 4, 8, 16, 16.7], \"xaxis\": \"x7\", \"y\": [1.0842145594819437, 1.191412553838028, 1.5165589271751867, 1.7529057098780358, 1.796459751947201, 2.03163161021332, 2.153328431655919, 2.169265655030074, 2.391488658266402, 2.640014269888514, 2.64380532657309, 2.6753126014096016, 2.9562366228147448, 3.150463025055146, 4.03344948971642, 5.336965770180979, 6.90519349381888, 7.852372823525647, 8.628749242986736], \"yaxis\": \"y7\", \"z\": [[null, null, null, null, null, null], [null, null, null, null, 0.746725892559226, null], [null, 0.7501677220885898, null, null, null, null], [null, 0.7456486000949089, null, null, null, null], [null, 0.754907958049808, null, null, null, null], [null, null, null, null, 0.7573360126551616, null], [null, 0.7487968383534321, null, null, null, null], [null, null, null, null, 0.7521821788855245, null], [null, 0.7613559108152659, null, null, null, null], [null, 0.7503543136600246, null, null, null, null], [null, null, 0.7519901956456392, null, null, null], [null, null, 0.7416582064297801, null, null, null], [null, null, 0.7535510065158645, null, null, null], [null, null, 0.31287128712871287, null, null, null], [null, null, null, 0.7276088222633006, null, null], [null, null, null, 0.35261194029850745, null, null], [null, null, null, 0.7015297451634195, null, null], [null, 0.7391285984592513, null, null, null, null], [null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [8, 8, 16, 4, 8, 2, 2, 2, 16, 2, 2, 2, 2, 16, 4, 4, 4], \"xaxis\": \"x7\", \"y\": [6.90519349381888, 4.03344948971642, 2.169265655030074, 3.150463025055146, 5.336965770180979, 7.852372823525647, 1.796459751947201, 1.7529057098780358, 2.03163161021332, 1.5165589271751867, 2.391488658266402, 2.640014269888514, 2.153328431655919, 1.191412553838028, 2.64380532657309, 2.6753126014096016, 2.9562366228147448], \"yaxis\": \"y7\"}, {\"type\": \"scatter\", \"xaxis\": \"x11\", \"yaxis\": \"y11\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.2999999999999998, 2, 4, 8, 16, 16.7], \"xaxis\": \"x15\", \"y\": [4.4305379630856e-12, 1.2764023216744377e-11, 2.470141657816699e-11, 7.344308135535765e-11, 1.1108012743093335e-09, 5.597111728168741e-09, 4.717749467497328e-07, 1.8248832708908834e-06, 2.2169028347942e-06, 3.4450729101456406e-06, 5.167420942207734e-06, 2.6453705505426204e-05, 6.308241593768865e-05, 0.00015865248205273314, 0.00031550587547210827, 0.0004337243841065953, 0.019259178689961923, 0.0197974863223102, 0.057034964412122906], \"yaxis\": \"y15\", \"z\": [[null, null, null, null, null, null], [null, 0.7456486000949089, null, null, null, null], [null, 0.7503543136600246, null, null, null, null], [null, 0.7613559108152659, null, null, null, null], [null, null, null, 0.7276088222633006, null, null], [null, null, 0.31287128712871287, null, null, null], [null, null, null, null, 0.7521821788855245, null], [null, 0.7487968383534321, null, null, null, null], [null, null, null, null, 0.746725892559226, null], [null, null, null, null, 0.7573360126551616, null], [null, 0.7501677220885898, null, null, null, null], [null, 0.754907958049808, null, null, null, null], [null, null, 0.7535510065158645, null, null, null], [null, 0.7391285984592513, null, null, null, null], [null, null, 0.7519901956456392, null, null, null], [null, null, null, 0.7015297451634195, null, null], [null, null, null, 0.35261194029850745, null, null], [null, null, 0.7416582064297801, null, null, null], [null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [8, 8, 16, 4, 8, 2, 2, 2, 16, 2, 2, 2, 2, 16, 4, 4, 4], \"xaxis\": \"x15\", \"y\": [0.0004337243841065953, 1.1108012743093335e-09, 4.717749467497328e-07, 5.597111728168741e-09, 0.019259178689961923, 0.00015865248205273314, 2.6453705505426204e-05, 1.2764023216744377e-11, 3.4450729101456406e-06, 5.167420942207734e-06, 7.344308135535765e-11, 2.470141657816699e-11, 1.8248832708908834e-06, 2.2169028347942e-06, 0.00031550587547210827, 0.0197974863223102, 6.308241593768865e-05], \"yaxis\": \"y15\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [4.4305379630856e-12, 1.2764023216744377e-11, 2.470141657816699e-11, 7.344308135535765e-11, 1.1108012743093335e-09, 5.597111728168741e-09, 4.717749467497328e-07, 1.8248832708908834e-06, 2.2169028347942e-06, 3.4450729101456406e-06, 5.167420942207734e-06, 2.6453705505426204e-05, 6.308241593768865e-05, 0.00015865248205273314, 0.00031550587547210827, 0.0004337243841065953, 0.019259178689961923, 0.0197974863223102, 0.057034964412122906], \"xaxis\": \"x4\", \"y\": [2.85168238633715e-07, 5.113164637727514e-07, 3.515249330255268e-06, 5.933575828730239e-06, 8.39487800112118e-06, 9.550187523162385e-06, 1.1044057137355722e-05, 1.6165531171201104e-05, 2.1025362834713015e-05, 2.6781315018468085e-05, 2.7603341256578127e-05, 2.879490739702494e-05, 2.8957120638078643e-05, 3.558787916816092e-05, 4.519860073508858e-05, 0.00010383242911913873, 0.00034161359596541536, 0.06032012701524992, 0.10815606319810658], \"yaxis\": \"y4\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.7015297451634195, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, 0.7391285984592513, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.7519901956456392, null, null, null, null], [null, null, null, null, null, null, null, 0.7487968383534321, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, 0.7613559108152659, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, 0.7503543136600246, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, 0.7535510065158645, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, 0.7501677220885898, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, 0.7573360126551616, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.7416582064297801, null], [null, 0.7456486000949089, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, 0.7276088222633006, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, 0.754907958049808, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, 0.746725892559226, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, 0.7521821788855245, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.35261194029850745, null, null], [null, null, null, null, null, 0.31287128712871287, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [0.0004337243841065953, 1.1108012743093335e-09, 4.717749467497328e-07, 5.597111728168741e-09, 0.019259178689961923, 0.00015865248205273314, 2.6453705505426204e-05, 1.2764023216744377e-11, 3.4450729101456406e-06, 5.167420942207734e-06, 7.344308135535765e-11, 2.470141657816699e-11, 1.8248832708908834e-06, 2.2169028347942e-06, 0.00031550587547210827, 0.0197974863223102, 6.308241593768865e-05], \"xaxis\": \"x4\", \"y\": [5.113164637727514e-07, 2.8957120638078643e-05, 0.00010383242911913873, 0.06032012701524992, 0.00034161359596541536, 3.515249330255268e-06, 3.558787916816092e-05, 2.879490739702494e-05, 2.6781315018468085e-05, 2.1025362834713015e-05, 9.550187523162385e-06, 1.1044057137355722e-05, 8.39487800112118e-06, 4.519860073508858e-05, 5.933575828730239e-06, 2.7603341256578127e-05, 1.6165531171201104e-05], \"yaxis\": \"y4\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [4.4305379630856e-12, 1.2764023216744377e-11, 2.470141657816699e-11, 7.344308135535765e-11, 1.1108012743093335e-09, 5.597111728168741e-09, 4.717749467497328e-07, 1.8248832708908834e-06, 2.2169028347942e-06, 3.4450729101456406e-06, 5.167420942207734e-06, 2.6453705505426204e-05, 6.308241593768865e-05, 0.00015865248205273314, 0.00031550587547210827, 0.0004337243841065953, 0.019259178689961923, 0.0197974863223102, 0.057034964412122906], \"xaxis\": \"x8\", \"y\": [1.0842145594819437, 1.191412553838028, 1.5165589271751867, 1.7529057098780358, 1.796459751947201, 2.03163161021332, 2.153328431655919, 2.169265655030074, 2.391488658266402, 2.640014269888514, 2.64380532657309, 2.6753126014096016, 2.9562366228147448, 3.150463025055146, 4.03344948971642, 5.336965770180979, 6.90519349381888, 7.852372823525647, 8.628749242986736], \"yaxis\": \"y8\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, 0.746725892559226, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, 0.7501677220885898, null, null, null, null, null, null, null, null], [null, 0.7456486000949089, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, 0.754907958049808, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, 0.7573360126551616, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, 0.7487968383534321, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, 0.7521821788855245, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, 0.7613559108152659, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, 0.7503543136600246, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.7519901956456392, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.7416582064297801, null], [null, null, null, null, null, null, null, null, null, null, null, null, 0.7535510065158645, null, null, null, null, null, null], [null, null, null, null, null, 0.31287128712871287, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, 0.7276088222633006, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.35261194029850745, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.7015297451634195, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, 0.7391285984592513, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [0.0004337243841065953, 1.1108012743093335e-09, 4.717749467497328e-07, 5.597111728168741e-09, 0.019259178689961923, 0.00015865248205273314, 2.6453705505426204e-05, 1.2764023216744377e-11, 3.4450729101456406e-06, 5.167420942207734e-06, 7.344308135535765e-11, 2.470141657816699e-11, 1.8248832708908834e-06, 2.2169028347942e-06, 0.00031550587547210827, 0.0197974863223102, 6.308241593768865e-05], \"xaxis\": \"x8\", \"y\": [6.90519349381888, 4.03344948971642, 2.169265655030074, 3.150463025055146, 5.336965770180979, 7.852372823525647, 1.796459751947201, 1.7529057098780358, 2.03163161021332, 1.5165589271751867, 2.391488658266402, 2.640014269888514, 2.153328431655919, 1.191412553838028, 2.64380532657309, 2.6753126014096016, 2.9562366228147448], \"yaxis\": \"y8\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [4.4305379630856e-12, 1.2764023216744377e-11, 2.470141657816699e-11, 7.344308135535765e-11, 1.1108012743093335e-09, 5.597111728168741e-09, 4.717749467497328e-07, 1.8248832708908834e-06, 2.2169028347942e-06, 3.4450729101456406e-06, 5.167420942207734e-06, 2.6453705505426204e-05, 6.308241593768865e-05, 0.00015865248205273314, 0.00031550587547210827, 0.0004337243841065953, 0.019259178689961923, 0.0197974863223102, 0.057034964412122906], \"xaxis\": \"x12\", \"y\": [1.2999999999999998, 2, 4, 8, 16, 16.7], \"yaxis\": \"y12\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, 0.7456486000949089, 0.7503543136600246, 0.7613559108152659, null, null, null, 0.7487968383534321, null, null, 0.7501677220885898, 0.754907958049808, null, 0.7391285984592513, null, null, null, null, null], [null, null, null, null, null, 0.31287128712871287, null, null, null, null, null, null, 0.7535510065158645, null, 0.7519901956456392, null, null, 0.7416582064297801, null], [null, null, null, null, 0.7276088222633006, null, null, null, null, null, null, null, null, null, null, 0.7015297451634195, 0.35261194029850745, null, null], [null, null, null, null, null, null, 0.7521821788855245, null, 0.746725892559226, 0.7573360126551616, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [0.0004337243841065953, 1.1108012743093335e-09, 4.717749467497328e-07, 5.597111728168741e-09, 0.019259178689961923, 0.00015865248205273314, 2.6453705505426204e-05, 1.2764023216744377e-11, 3.4450729101456406e-06, 5.167420942207734e-06, 7.344308135535765e-11, 2.470141657816699e-11, 1.8248832708908834e-06, 2.2169028347942e-06, 0.00031550587547210827, 0.0197974863223102, 6.308241593768865e-05], \"xaxis\": \"x12\", \"y\": [8, 8, 16, 4, 8, 2, 2, 2, 16, 2, 2, 2, 2, 16, 4, 4, 4], \"yaxis\": \"y12\"}, {\"type\": \"scatter\", \"xaxis\": \"x16\", \"yaxis\": \"y16\"}],\n","                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Contour Plot\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 0.2125], \"matches\": \"x13\", \"range\": [-6.544898846822439, -0.9659491291174427], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis10\": {\"anchor\": \"y10\", \"domain\": [0.2625, 0.475], \"matches\": \"x14\", \"range\": [0.03511523494993622, 0.9359478482950501], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis11\": {\"anchor\": \"y11\", \"domain\": [0.525, 0.7375], \"matches\": \"x15\", \"range\": [1.2999999999999998, 16.7], \"showticklabels\": false}, \"xaxis12\": {\"anchor\": \"y12\", \"domain\": [0.7875, 1.0], \"matches\": \"x16\", \"range\": [-11.353543537836787, -1.2438588251016098], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis13\": {\"anchor\": \"y13\", \"domain\": [0.0, 0.2125], \"range\": [-6.544898846822439, -0.9659491291174427], \"title\": {\"text\": \"learning_rate\"}, \"type\": \"log\"}, \"xaxis14\": {\"anchor\": \"y14\", \"domain\": [0.2625, 0.475], \"range\": [0.03511523494993622, 0.9359478482950501], \"title\": {\"text\": \"num_train_epochs\"}, \"type\": \"log\"}, \"xaxis15\": {\"anchor\": \"y15\", \"domain\": [0.525, 0.7375], \"range\": [1.2999999999999998, 16.7], \"title\": {\"text\": \"per_device_train_batch_size\"}}, \"xaxis16\": {\"anchor\": \"y16\", \"domain\": [0.7875, 1.0], \"range\": [-11.353543537836787, -1.2438588251016098], \"title\": {\"text\": \"weight_decay\"}, \"type\": \"log\"}, \"xaxis2\": {\"anchor\": \"y2\", \"domain\": [0.2625, 0.475], \"matches\": \"x14\", \"range\": [0.03511523494993622, 0.9359478482950501], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis3\": {\"anchor\": \"y3\", \"domain\": [0.525, 0.7375], \"matches\": \"x15\", \"range\": [1.2999999999999998, 16.7], \"showticklabels\": false}, \"xaxis4\": {\"anchor\": \"y4\", \"domain\": [0.7875, 1.0], \"matches\": \"x16\", \"range\": [-11.353543537836787, -1.2438588251016098], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis5\": {\"anchor\": \"y5\", \"domain\": [0.0, 0.2125], \"matches\": \"x13\", \"range\": [-6.544898846822439, -0.9659491291174427], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis6\": {\"anchor\": \"y6\", \"domain\": [0.2625, 0.475], \"matches\": \"x14\", \"range\": [0.03511523494993622, 0.9359478482950501], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis7\": {\"anchor\": \"y7\", \"domain\": [0.525, 0.7375], \"matches\": \"x15\", \"range\": [1.2999999999999998, 16.7], \"showticklabels\": false}, \"xaxis8\": {\"anchor\": \"y8\", \"domain\": [0.7875, 1.0], \"matches\": \"x16\", \"range\": [-11.353543537836787, -1.2438588251016098], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis9\": {\"anchor\": \"y9\", \"domain\": [0.0, 0.2125], \"matches\": \"x13\", \"range\": [-6.544898846822439, -0.9659491291174427], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.80625, 1.0], \"range\": [-6.544898846822439, -0.9659491291174427], \"title\": {\"text\": \"learning_rate\"}, \"type\": \"log\"}, \"yaxis10\": {\"anchor\": \"x10\", \"domain\": [0.26875, 0.4625], \"matches\": \"y9\", \"range\": [1.2999999999999998, 16.7], \"showticklabels\": false}, \"yaxis11\": {\"anchor\": \"x11\", \"domain\": [0.26875, 0.4625], \"matches\": \"y9\", \"range\": [1.2999999999999998, 16.7], \"showticklabels\": false}, \"yaxis12\": {\"anchor\": \"x12\", \"domain\": [0.26875, 0.4625], \"matches\": \"y9\", \"range\": [1.2999999999999998, 16.7], \"showticklabels\": false}, \"yaxis13\": {\"anchor\": \"x13\", \"domain\": [0.0, 0.19375], \"range\": [-11.353543537836787, -1.2438588251016098], \"title\": {\"text\": \"weight_decay\"}, \"type\": \"log\"}, \"yaxis14\": {\"anchor\": \"x14\", \"domain\": [0.0, 0.19375], \"matches\": \"y13\", \"range\": [-11.353543537836787, -1.2438588251016098], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis15\": {\"anchor\": \"x15\", \"domain\": [0.0, 0.19375], \"matches\": \"y13\", \"range\": [-11.353543537836787, -1.2438588251016098], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis16\": {\"anchor\": \"x16\", \"domain\": [0.0, 0.19375], \"matches\": \"y13\", \"range\": [-11.353543537836787, -1.2438588251016098], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis2\": {\"anchor\": \"x2\", \"domain\": [0.80625, 1.0], \"matches\": \"y\", \"range\": [-6.544898846822439, -0.9659491291174427], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis3\": {\"anchor\": \"x3\", \"domain\": [0.80625, 1.0], \"matches\": \"y\", \"range\": [-6.544898846822439, -0.9659491291174427], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis4\": {\"anchor\": \"x4\", \"domain\": [0.80625, 1.0], \"matches\": \"y\", \"range\": [-6.544898846822439, -0.9659491291174427], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis5\": {\"anchor\": \"x5\", \"domain\": [0.5375, 0.73125], \"range\": [0.03511523494993622, 0.9359478482950501], \"title\": {\"text\": \"num_train_epochs\"}, \"type\": \"log\"}, \"yaxis6\": {\"anchor\": \"x6\", \"domain\": [0.5375, 0.73125], \"matches\": \"y5\", \"range\": [0.03511523494993622, 0.9359478482950501], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis7\": {\"anchor\": \"x7\", \"domain\": [0.5375, 0.73125], \"matches\": \"y5\", \"range\": [0.03511523494993622, 0.9359478482950501], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis8\": {\"anchor\": \"x8\", \"domain\": [0.5375, 0.73125], \"matches\": \"y5\", \"range\": [0.03511523494993622, 0.9359478482950501], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis9\": {\"anchor\": \"x9\", \"domain\": [0.26875, 0.4625], \"range\": [1.2999999999999998, 16.7], \"title\": {\"text\": \"per_device_train_batch_size\"}}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('16cf00f4-f609-4604-b401-1bf27f2041a1');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"uklDrIbLDvMG","executionInfo":{"status":"ok","timestamp":1638949306466,"user_tz":-60,"elapsed":21,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"5cdd696c-c2d2-4129-86fe-a312ddf55185"},"source":["optuna.visualization.plot_slice(study)"],"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"b5ccb13e-5a4f-4b2d-be8c-1aaf769a75d4\" class=\"plotly-graph-div\" style=\"height:525px; width:1200px;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"b5ccb13e-5a4f-4b2d-be8c-1aaf769a75d4\")) {\n","                    Plotly.newPlot(\n","                        'b5ccb13e-5a4f-4b2d-be8c-1aaf769a75d4',\n","                        [{\"marker\": {\"color\": [0, 1, 2, 3, 4, 9, 11, 12, 13, 17, 20, 21, 25, 30, 36, 42, 45], \"colorbar\": {\"title\": {\"text\": \"#Trials\"}, \"x\": 1.0, \"xpad\": 40}, \"colorscale\": [[0.0, \"rgb(247,251,255)\"], [0.125, \"rgb(222,235,247)\"], [0.25, \"rgb(198,219,239)\"], [0.375, \"rgb(158,202,225)\"], [0.5, \"rgb(107,174,214)\"], [0.625, \"rgb(66,146,198)\"], [0.75, \"rgb(33,113,181)\"], [0.875, \"rgb(8,81,156)\"], [1.0, \"rgb(8,48,107)\"]], \"line\": {\"color\": \"Grey\", \"width\": 0.5}, \"showscale\": true}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [5.113164637727514e-07, 2.8957120638078643e-05, 0.00010383242911913873, 0.06032012701524992, 0.00034161359596541536, 3.515249330255268e-06, 3.558787916816092e-05, 2.879490739702494e-05, 2.6781315018468085e-05, 2.1025362834713015e-05, 9.550187523162385e-06, 1.1044057137355722e-05, 8.39487800112118e-06, 4.519860073508858e-05, 5.933575828730239e-06, 2.7603341256578127e-05, 1.6165531171201104e-05], \"xaxis\": \"x\", \"y\": [0.7015297451634195, 0.7276088222633006, 0.7521821788855245, 0.31287128712871287, 0.35261194029850745, 0.7391285984592513, 0.754907958049808, 0.7456486000949089, 0.7573360126551616, 0.7501677220885898, 0.7613559108152659, 0.7503543136600246, 0.7487968383534321, 0.746725892559226, 0.7519901956456392, 0.7416582064297801, 0.7535510065158645], \"yaxis\": \"y\"}, {\"marker\": {\"color\": [0, 1, 2, 3, 4, 9, 11, 12, 13, 17, 20, 21, 25, 30, 36, 42, 45], \"colorbar\": {\"title\": {\"text\": \"#Trials\"}, \"x\": 1.0, \"xpad\": 40}, \"colorscale\": [[0.0, \"rgb(247,251,255)\"], [0.125, \"rgb(222,235,247)\"], [0.25, \"rgb(198,219,239)\"], [0.375, \"rgb(158,202,225)\"], [0.5, \"rgb(107,174,214)\"], [0.625, \"rgb(66,146,198)\"], [0.75, \"rgb(33,113,181)\"], [0.875, \"rgb(8,81,156)\"], [1.0, \"rgb(8,48,107)\"]], \"line\": {\"color\": \"Grey\", \"width\": 0.5}, \"showscale\": false}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [6.90519349381888, 4.03344948971642, 2.169265655030074, 3.150463025055146, 5.336965770180979, 7.852372823525647, 1.796459751947201, 1.7529057098780358, 2.03163161021332, 1.5165589271751867, 2.391488658266402, 2.640014269888514, 2.153328431655919, 1.191412553838028, 2.64380532657309, 2.6753126014096016, 2.9562366228147448], \"xaxis\": \"x2\", \"y\": [0.7015297451634195, 0.7276088222633006, 0.7521821788855245, 0.31287128712871287, 0.35261194029850745, 0.7391285984592513, 0.754907958049808, 0.7456486000949089, 0.7573360126551616, 0.7501677220885898, 0.7613559108152659, 0.7503543136600246, 0.7487968383534321, 0.746725892559226, 0.7519901956456392, 0.7416582064297801, 0.7535510065158645], \"yaxis\": \"y2\"}, {\"marker\": {\"color\": [0, 1, 2, 3, 4, 9, 11, 12, 13, 17, 20, 21, 25, 30, 36, 42, 45], \"colorbar\": {\"title\": {\"text\": \"#Trials\"}, \"x\": 1.0, \"xpad\": 40}, \"colorscale\": [[0.0, \"rgb(247,251,255)\"], [0.125, \"rgb(222,235,247)\"], [0.25, \"rgb(198,219,239)\"], [0.375, \"rgb(158,202,225)\"], [0.5, \"rgb(107,174,214)\"], [0.625, \"rgb(66,146,198)\"], [0.75, \"rgb(33,113,181)\"], [0.875, \"rgb(8,81,156)\"], [1.0, \"rgb(8,48,107)\"]], \"line\": {\"color\": \"Grey\", \"width\": 0.5}, \"showscale\": false}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [8, 8, 16, 4, 8, 2, 2, 2, 16, 2, 2, 2, 2, 16, 4, 4, 4], \"xaxis\": \"x3\", \"y\": [0.7015297451634195, 0.7276088222633006, 0.7521821788855245, 0.31287128712871287, 0.35261194029850745, 0.7391285984592513, 0.754907958049808, 0.7456486000949089, 0.7573360126551616, 0.7501677220885898, 0.7613559108152659, 0.7503543136600246, 0.7487968383534321, 0.746725892559226, 0.7519901956456392, 0.7416582064297801, 0.7535510065158645], \"yaxis\": \"y3\"}, {\"marker\": {\"color\": [0, 1, 2, 3, 4, 9, 11, 12, 13, 17, 20, 21, 25, 30, 36, 42, 45], \"colorbar\": {\"title\": {\"text\": \"#Trials\"}, \"x\": 1.0, \"xpad\": 40}, \"colorscale\": [[0.0, \"rgb(247,251,255)\"], [0.125, \"rgb(222,235,247)\"], [0.25, \"rgb(198,219,239)\"], [0.375, \"rgb(158,202,225)\"], [0.5, \"rgb(107,174,214)\"], [0.625, \"rgb(66,146,198)\"], [0.75, \"rgb(33,113,181)\"], [0.875, \"rgb(8,81,156)\"], [1.0, \"rgb(8,48,107)\"]], \"line\": {\"color\": \"Grey\", \"width\": 0.5}, \"showscale\": false}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [0.0004337243841065953, 1.1108012743093335e-09, 4.717749467497328e-07, 5.597111728168741e-09, 0.019259178689961923, 0.00015865248205273314, 2.6453705505426204e-05, 1.2764023216744377e-11, 3.4450729101456406e-06, 5.167420942207734e-06, 7.344308135535765e-11, 2.470141657816699e-11, 1.8248832708908834e-06, 2.2169028347942e-06, 0.00031550587547210827, 0.0197974863223102, 6.308241593768865e-05], \"xaxis\": \"x4\", \"y\": [0.7015297451634195, 0.7276088222633006, 0.7521821788855245, 0.31287128712871287, 0.35261194029850745, 0.7391285984592513, 0.754907958049808, 0.7456486000949089, 0.7573360126551616, 0.7501677220885898, 0.7613559108152659, 0.7503543136600246, 0.7487968383534321, 0.746725892559226, 0.7519901956456392, 0.7416582064297801, 0.7535510065158645], \"yaxis\": \"y4\"}],\n","                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Slice Plot\"}, \"width\": 1200, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 0.2125], \"title\": {\"text\": \"learning_rate\"}, \"type\": \"log\"}, \"xaxis2\": {\"anchor\": \"y2\", \"domain\": [0.2625, 0.475], \"title\": {\"text\": \"num_train_epochs\"}, \"type\": \"log\"}, \"xaxis3\": {\"anchor\": \"y3\", \"domain\": [0.525, 0.7375], \"title\": {\"text\": \"per_device_train_batch_size\"}}, \"xaxis4\": {\"anchor\": \"y4\", \"domain\": [0.7875, 1.0], \"title\": {\"text\": \"weight_decay\"}, \"type\": \"log\"}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Objective Value\"}}, \"yaxis2\": {\"anchor\": \"x2\", \"domain\": [0.0, 1.0], \"matches\": \"y\", \"showticklabels\": false}, \"yaxis3\": {\"anchor\": \"x3\", \"domain\": [0.0, 1.0], \"matches\": \"y\", \"showticklabels\": false}, \"yaxis4\": {\"anchor\": \"x4\", \"domain\": [0.0, 1.0], \"matches\": \"y\", \"showticklabels\": false}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('b5ccb13e-5a4f-4b2d-be8c-1aaf769a75d4');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"PnOWxc8TD52A","executionInfo":{"status":"ok","timestamp":1638949307009,"user_tz":-60,"elapsed":561,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"f292fe3f-9ce8-44ad-b02f-67d6964c78d3"},"source":["optuna.visualization.plot_edf(study)"],"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"8adee5ee-e366-49d5-8ea7-d2c7ec564f40\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"8adee5ee-e366-49d5-8ea7-d2c7ec564f40\")) {\n","                    Plotly.newPlot(\n","                        '8adee5ee-e366-49d5-8ea7-d2c7ec564f40',\n","                        [{\"mode\": \"lines\", \"name\": \"SIMCSE_BERT_MASK_TF\", \"type\": \"scatter\", \"x\": [0.31287128712871287, 0.31740143484271843, 0.32193158255672405, 0.3264617302707296, 0.3309918779847352, 0.3355220256987408, 0.3400521734127464, 0.34458232112675197, 0.34911246884075753, 0.35364261655476315, 0.3581727642687687, 0.3627029119827743, 0.3672330596967799, 0.3717632074107855, 0.37629335512479106, 0.3808235028387966, 0.38535365055280224, 0.38988379826680786, 0.3944139459808134, 0.398944093694819, 0.4034742414088246, 0.40800438912283016, 0.4125345368368357, 0.41706468455084134, 0.42159483226484695, 0.4261249799788525, 0.4306551276928581, 0.4351852754068637, 0.43971542312086925, 0.4442455708348748, 0.44877571854888043, 0.45330586626288605, 0.4578360139768916, 0.46236616169089717, 0.4668963094049028, 0.4714264571189084, 0.47595660483291397, 0.4804867525469195, 0.48501690026092514, 0.4895470479749307, 0.49407719568893627, 0.4986073434029419, 0.5031374911169475, 0.5076676388309531, 0.5121977865449586, 0.5167279342589642, 0.5212580819729697, 0.5257882296869754, 0.530318377400981, 0.5348485251149866, 0.5393786728289922, 0.5439088205429977, 0.5484389682570033, 0.5529691159710088, 0.5574992636850145, 0.5620294113990201, 0.5665595591130257, 0.5710897068270313, 0.5756198545410368, 0.5801500022550424, 0.5846801499690479, 0.5892102976830536, 0.5937404453970592, 0.5982705931110648, 0.6028007408250704, 0.6073308885390759, 0.6118610362530815, 0.616391183967087, 0.6209213316810926, 0.6254514793950983, 0.6299816271091039, 0.6345117748231095, 0.6390419225371151, 0.6435720702511206, 0.6481022179651261, 0.6526323656791317, 0.6571625133931374, 0.661692661107143, 0.6662228088211486, 0.6707529565351542, 0.6752831042491597, 0.6798132519631652, 0.6843433996771708, 0.6888735473911765, 0.6934036951051821, 0.6979338428191877, 0.7024639905331933, 0.7069941382471988, 0.7115242859612043, 0.7160544336752099, 0.7205845813892156, 0.7251147291032212, 0.7296448768172268, 0.7341750245312324, 0.7387051722452379, 0.7432353199592435, 0.747765467673249, 0.7522956153872546, 0.7568257631012603, 0.7613559108152659], \"y\": [0.058823529411764705, 0.058823529411764705, 0.058823529411764705, 0.058823529411764705, 0.058823529411764705, 0.058823529411764705, 0.058823529411764705, 0.058823529411764705, 0.058823529411764705, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.17647058823529413, 0.23529411764705882, 0.23529411764705882, 0.23529411764705882, 0.35294117647058826, 0.47058823529411764, 0.7647058823529411, 0.8823529411764706, 1.0]}],\n","                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Empirical Distribution Function Plot\"}, \"xaxis\": {\"title\": {\"text\": \"Objective Value\"}}, \"yaxis\": {\"range\": [0, 1], \"title\": {\"text\": \"Cumulative Probability\"}}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('8adee5ee-e366-49d5-8ea7-d2c7ec564f40');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]}]}