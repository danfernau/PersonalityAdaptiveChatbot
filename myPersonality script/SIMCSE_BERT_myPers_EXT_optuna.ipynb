{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"SIMCSE_BERT_myPers_EXT_optuna.ipynb","provenance":[{"file_id":"1GViGI-xQToPJ6TlIhvkfgKwhSCBhG4Ht","timestamp":1621319032244}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"88e41be6abfa4a539001de4dc612be30":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d4f376fb61004838a76912e8649c9b3b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3ba20b6178894167946c3b1bc79ecf3c","IPY_MODEL_5549a105780040db92b0d15e78a6b978","IPY_MODEL_62118d4704f6469ea882ec69b9f26793"]}},"d4f376fb61004838a76912e8649c9b3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3ba20b6178894167946c3b1bc79ecf3c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ae870618a28d4c02872f693a970efeda","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a871b9856eb7411aa91a3432cf3fdc81"}},"5549a105780040db92b0d15e78a6b978":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_39f316213743479299da591859cbaab0","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":252,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":252,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8879a94e9f90423dae9c3d3964a84118"}},"62118d4704f6469ea882ec69b9f26793":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3b6fe4347e3640bf9acac9f164e4c528","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 252/252 [00:00&lt;00:00, 6.95kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8630ee3e426b40a09171d60325948440"}},"ae870618a28d4c02872f693a970efeda":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a871b9856eb7411aa91a3432cf3fdc81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"39f316213743479299da591859cbaab0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8879a94e9f90423dae9c3d3964a84118":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3b6fe4347e3640bf9acac9f164e4c528":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8630ee3e426b40a09171d60325948440":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c9ef67cc03ec4678a291291023a3e31b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fdc747ab70be4134a8c3e5ae7ef636de","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9b26e1087ffa4bf69f59279770b19fd3","IPY_MODEL_0bd0e741473f4d9ba6fa1451b328dee8","IPY_MODEL_3c4ef7517da542ee87ab5985610e50d3"]}},"fdc747ab70be4134a8c3e5ae7ef636de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9b26e1087ffa4bf69f59279770b19fd3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_eb770023f970453484c7916f15c94fee","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cace7ec0813f401fb8faf43f541f3d58"}},"0bd0e741473f4d9ba6fa1451b328dee8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_07db7970663a478083dbd4aa4338c6fb","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":689,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":689,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_72f817d0a7e14c238482bfcc5c32e52c"}},"3c4ef7517da542ee87ab5985610e50d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9df92c7fbe594569ad525ce8cf43e0f4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 689/689 [00:00&lt;00:00, 24.1kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_75f2f279ea14438098ec27efaf22d435"}},"eb770023f970453484c7916f15c94fee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cace7ec0813f401fb8faf43f541f3d58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"07db7970663a478083dbd4aa4338c6fb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"72f817d0a7e14c238482bfcc5c32e52c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9df92c7fbe594569ad525ce8cf43e0f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"75f2f279ea14438098ec27efaf22d435":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6144756a1c47491f8b2b5ef415025193":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b3553e14506745eeb1d61c03483367ce","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_63d68356abf140018749c04e47537f87","IPY_MODEL_6165251299a541d6abf842ee0f373425","IPY_MODEL_559ffbb49dcf4013aa7a2bd5ad45c716"]}},"b3553e14506745eeb1d61c03483367ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"63d68356abf140018749c04e47537f87":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_133c52d8205d48fb95ee558806d9b3d3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3699238bc96d45f2a9884468aa90c5c9"}},"6165251299a541d6abf842ee0f373425":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_19ac064ba1204ccba312b5d112cf81df","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_68aef0d23ac64d6c9d1773e094888098"}},"559ffbb49dcf4013aa7a2bd5ad45c716":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_97487e5921ed46f1b29cffd79b347df6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 226k/226k [00:00&lt;00:00, 781kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_340a637af3da47398abde1b6e87c9c3a"}},"133c52d8205d48fb95ee558806d9b3d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3699238bc96d45f2a9884468aa90c5c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"19ac064ba1204ccba312b5d112cf81df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"68aef0d23ac64d6c9d1773e094888098":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"97487e5921ed46f1b29cffd79b347df6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"340a637af3da47398abde1b6e87c9c3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b9e71aa061c44cf6a2592b04326a7278":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_13c83f5f0ef745a5a3ad730cbdfd4cb7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_df985c65523446ada4351bf2ca023e6c","IPY_MODEL_49b0a13f56434637ba26852a68228cea","IPY_MODEL_c3fc6e7d1fdf45cb8b566bf7b3239307"]}},"13c83f5f0ef745a5a3ad730cbdfd4cb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"df985c65523446ada4351bf2ca023e6c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e1e71c173bc9456594d09c5d225df16d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_60e26b6e67784ba2a6acb109b3c9cd74"}},"49b0a13f56434637ba26852a68228cea":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_aa776e7028084e7ebab4b9716d801c1d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":112,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":112,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9f175d5f0ecc457e950e61255cbe48f9"}},"c3fc6e7d1fdf45cb8b566bf7b3239307":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f465a8172ad846aea7aae94839edc436","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 112/112 [00:00&lt;00:00, 3.13kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a3631955d58b43aab7a89d77009508ce"}},"e1e71c173bc9456594d09c5d225df16d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"60e26b6e67784ba2a6acb109b3c9cd74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aa776e7028084e7ebab4b9716d801c1d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9f175d5f0ecc457e950e61255cbe48f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f465a8172ad846aea7aae94839edc436":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a3631955d58b43aab7a89d77009508ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"11ea43f726524319b2127cd9d3ba0e30":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a668eeb2206441c2adc1a9665ceb2084","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4df1d41eea2047dc92cac4b27b6ea432","IPY_MODEL_aeee2096ab024462808469b3ad7da2bc","IPY_MODEL_429392263d3842cda65af8e8c5005ba4"]}},"a668eeb2206441c2adc1a9665ceb2084":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4df1d41eea2047dc92cac4b27b6ea432":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bcb5d82babaa491c98dcfebc8226d820","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8049d69d2d4e40a892946a8d33520697"}},"aeee2096ab024462808469b3ad7da2bc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_67a3f7be363a4e8a97356ff7dce824ec","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":8,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":8,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dbf1524e3f4f40ea83a18585be56ae94"}},"429392263d3842cda65af8e8c5005ba4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0a424b76457c4a71925c52c5d3284dc4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 8/8 [00:01&lt;00:00,  6.47ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_09f19d9ab3924ba9aad61a28fce064c2"}},"bcb5d82babaa491c98dcfebc8226d820":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8049d69d2d4e40a892946a8d33520697":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"67a3f7be363a4e8a97356ff7dce824ec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"dbf1524e3f4f40ea83a18585be56ae94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0a424b76457c4a71925c52c5d3284dc4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"09f19d9ab3924ba9aad61a28fce064c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4fa119df3c804e219bf9fab3c4fd4f46":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3025504d55f34e29bd81ac25860c9d4e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4577b09b8e73454a9806374a083a3855","IPY_MODEL_06646a56de9c40bb9317ae652754a981","IPY_MODEL_152d43674c0949ab963606b7d052563c"]}},"3025504d55f34e29bd81ac25860c9d4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4577b09b8e73454a9806374a083a3855":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_af7eeb5a49014c22946a9ab627166da3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3b49abd558334ba081414e27772916d5"}},"06646a56de9c40bb9317ae652754a981":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_557b142124d74d11bd20d22485f279a5","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ea75f777ddae4976a6626fcf818dd219"}},"152d43674c0949ab963606b7d052563c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3283ab13684342e68cf4a55e81f8904f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:00&lt;00:00,  5.63ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3be4e61837b847f7a5bf0cba5e6bc2b6"}},"af7eeb5a49014c22946a9ab627166da3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3b49abd558334ba081414e27772916d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"557b142124d74d11bd20d22485f279a5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ea75f777ddae4976a6626fcf818dd219":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3283ab13684342e68cf4a55e81f8904f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3be4e61837b847f7a5bf0cba5e6bc2b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d5ef0ea543e947f4b021ed727cc933aa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_67f0f9a691454954bbfc40deafaf5ccb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8f21b4cbe43345d987e639b934230e65","IPY_MODEL_46d8f8e3d194406a80ed76db49b961b3","IPY_MODEL_a472b9654929469783c472884a4ffd8a"]}},"67f0f9a691454954bbfc40deafaf5ccb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8f21b4cbe43345d987e639b934230e65":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1355ecf663664fb092e9a34c21fc7b37","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_38c58d8ce8a34c9ead7bb70bbbb07b38"}},"46d8f8e3d194406a80ed76db49b961b3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c7756961b6374312b425401844f174ff","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":437998343,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":437998343,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_354a407b506f423db0522ac19cc3bb60"}},"a472b9654929469783c472884a4ffd8a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5d716213c847404a8c316a4ddf27f2de","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 418M/418M [00:07&lt;00:00, 55.1MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_01ac03d97c084afca04e214481a99fc4"}},"1355ecf663664fb092e9a34c21fc7b37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"38c58d8ce8a34c9ead7bb70bbbb07b38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c7756961b6374312b425401844f174ff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"354a407b506f423db0522ac19cc3bb60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5d716213c847404a8c316a4ddf27f2de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"01ac03d97c084afca04e214481a99fc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"-9igszFADeud"},"source":["# Initiliation"]},{"cell_type":"code","metadata":{"id":"4EOaUe7B1xDa","executionInfo":{"status":"ok","timestamp":1640447792864,"user_tz":-60,"elapsed":20156,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2dd7fdac-1d48-4f3e-d36f-1e965e59f038"},"source":["!pip install transformers datasets --quiet"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 3.4 MB 8.2 MB/s \n","\u001b[K     |████████████████████████████████| 306 kB 38.6 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 68.6 MB/s \n","\u001b[K     |████████████████████████████████| 3.3 MB 46.6 MB/s \n","\u001b[K     |████████████████████████████████| 895 kB 69.3 MB/s \n","\u001b[K     |████████████████████████████████| 61 kB 640 kB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 68.7 MB/s \n","\u001b[K     |████████████████████████████████| 132 kB 35.8 MB/s \n","\u001b[K     |████████████████████████████████| 243 kB 73.5 MB/s \n","\u001b[K     |████████████████████████████████| 192 kB 49.7 MB/s \n","\u001b[K     |████████████████████████████████| 271 kB 56.9 MB/s \n","\u001b[K     |████████████████████████████████| 160 kB 70.0 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","metadata":{"id":"8KpxUoNWQDGZ","executionInfo":{"status":"ok","timestamp":1640447804699,"user_tz":-60,"elapsed":11845,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}}},"source":["from transformers import TrainingArguments\n","from transformers import Trainer\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoTokenizer\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report, confusion_matrix\n","from datasets import Dataset\n","from datasets import load_metric\n","\n","import numpy as np\n","import math\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from google.colab import drive"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KCQCqALkqtIM"},"source":["# Data Preparation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bv6QdmgkwMsZ","executionInfo":{"status":"ok","timestamp":1640447891629,"user_tz":-60,"elapsed":86941,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"acda090f-4e93-484e-c391-558bfecf7108"},"source":["drive.mount('/content/drive/')"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f9Z1U4B7zvOi","executionInfo":{"status":"ok","timestamp":1640447893075,"user_tz":-60,"elapsed":1452,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"ba378ca7-fbe5-4f8d-af65-1c786ecc5e37"},"source":["%cd 'drive/MyDrive/Masterarbeit/Colab Notebooks/OVERVIEW myPers/00_Datasets/Baseline'"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1aHXlqhpj1STohhfU4gn53D4whaLH__Jz/Masterarbeit/Colab Notebooks/OVERVIEW myPers/00_Datasets/Baseline\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"QYahF3fduAU4","executionInfo":{"status":"ok","timestamp":1640447893784,"user_tz":-60,"elapsed":714,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"4ba7072c-ee29-4deb-c064-1b05f3353764"},"source":["dfPJ = pd.read_csv('myPers_EXT_Baseline.csv', sep=\",\", error_bad_lines=False)\n","dfPJ"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-f8c94393-d75c-49d0-a1c9-9a7a4b7b5afa\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>likes the sound of thunder.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>is so sleepy it's not even funny that's she ca...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>is sore and wants the knot of muscles at the b...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>likes how the day sounds in this new song.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>is home. &lt;3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9912</th>\n","      <td>little things give you away.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9913</th>\n","      <td>is wishing it was Saturday.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9914</th>\n","      <td>is studying hard for the G.R.E.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9915</th>\n","      <td>snipers get more head</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9916</th>\n","      <td>Last night was amazing! Not only did I see *PR...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9917 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8c94393-d75c-49d0-a1c9-9a7a4b7b5afa')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f8c94393-d75c-49d0-a1c9-9a7a4b7b5afa button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f8c94393-d75c-49d0-a1c9-9a7a4b7b5afa');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                   text  label\n","0                           likes the sound of thunder.      0\n","1     is so sleepy it's not even funny that's she ca...      0\n","2     is sore and wants the knot of muscles at the b...      0\n","3            likes how the day sounds in this new song.      0\n","4                                           is home. <3      0\n","...                                                 ...    ...\n","9912                       little things give you away.      0\n","9913                        is wishing it was Saturday.      1\n","9914                    is studying hard for the G.R.E.      1\n","9915                              snipers get more head      0\n","9916  Last night was amazing! Not only did I see *PR...      1\n","\n","[9917 rows x 2 columns]"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["df_clean= []\n","for i,row in dfPJ.iterrows():\n","  df_clean.append({\n","      'text': str(row['text']),\n","      'label': int(row['label']),\n","  })\n","\n","dfPJ = pd.DataFrame(df_clean)\n","dfPJ"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"cwcaCzADt30I","executionInfo":{"status":"ok","timestamp":1640447894893,"user_tz":-60,"elapsed":1114,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"08a18aae-8df8-46bc-ccaa-39592198129f"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-090a3bae-f3b8-42a2-a307-6889df3acac0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>likes the sound of thunder.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>is so sleepy it's not even funny that's she ca...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>is sore and wants the knot of muscles at the b...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>likes how the day sounds in this new song.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>is home. &lt;3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9912</th>\n","      <td>little things give you away.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9913</th>\n","      <td>is wishing it was Saturday.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9914</th>\n","      <td>is studying hard for the G.R.E.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9915</th>\n","      <td>snipers get more head</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9916</th>\n","      <td>Last night was amazing! Not only did I see *PR...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9917 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-090a3bae-f3b8-42a2-a307-6889df3acac0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-090a3bae-f3b8-42a2-a307-6889df3acac0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-090a3bae-f3b8-42a2-a307-6889df3acac0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                   text  label\n","0                           likes the sound of thunder.      0\n","1     is so sleepy it's not even funny that's she ca...      0\n","2     is sore and wants the knot of muscles at the b...      0\n","3            likes how the day sounds in this new song.      0\n","4                                           is home. <3      0\n","...                                                 ...    ...\n","9912                       little things give you away.      0\n","9913                        is wishing it was Saturday.      1\n","9914                    is studying hard for the G.R.E.      1\n","9915                              snipers get more head      0\n","9916  Last night was amazing! Not only did I see *PR...      1\n","\n","[9917 rows x 2 columns]"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"38B9v6_0sP3h"},"source":["# Model Training"]},{"cell_type":"code","metadata":{"id":"2ewqnPtVdCgV","executionInfo":{"status":"ok","timestamp":1640447894895,"user_tz":-60,"elapsed":9,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}}},"source":["modeltype = \"princeton-nlp/sup-simcse-bert-base-uncased\""],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":277,"referenced_widgets":["88e41be6abfa4a539001de4dc612be30","d4f376fb61004838a76912e8649c9b3b","3ba20b6178894167946c3b1bc79ecf3c","5549a105780040db92b0d15e78a6b978","62118d4704f6469ea882ec69b9f26793","ae870618a28d4c02872f693a970efeda","a871b9856eb7411aa91a3432cf3fdc81","39f316213743479299da591859cbaab0","8879a94e9f90423dae9c3d3964a84118","3b6fe4347e3640bf9acac9f164e4c528","8630ee3e426b40a09171d60325948440","c9ef67cc03ec4678a291291023a3e31b","fdc747ab70be4134a8c3e5ae7ef636de","9b26e1087ffa4bf69f59279770b19fd3","0bd0e741473f4d9ba6fa1451b328dee8","3c4ef7517da542ee87ab5985610e50d3","eb770023f970453484c7916f15c94fee","cace7ec0813f401fb8faf43f541f3d58","07db7970663a478083dbd4aa4338c6fb","72f817d0a7e14c238482bfcc5c32e52c","9df92c7fbe594569ad525ce8cf43e0f4","75f2f279ea14438098ec27efaf22d435","6144756a1c47491f8b2b5ef415025193","b3553e14506745eeb1d61c03483367ce","63d68356abf140018749c04e47537f87","6165251299a541d6abf842ee0f373425","559ffbb49dcf4013aa7a2bd5ad45c716","133c52d8205d48fb95ee558806d9b3d3","3699238bc96d45f2a9884468aa90c5c9","19ac064ba1204ccba312b5d112cf81df","68aef0d23ac64d6c9d1773e094888098","97487e5921ed46f1b29cffd79b347df6","340a637af3da47398abde1b6e87c9c3a","b9e71aa061c44cf6a2592b04326a7278","13c83f5f0ef745a5a3ad730cbdfd4cb7","df985c65523446ada4351bf2ca023e6c","49b0a13f56434637ba26852a68228cea","c3fc6e7d1fdf45cb8b566bf7b3239307","e1e71c173bc9456594d09c5d225df16d","60e26b6e67784ba2a6acb109b3c9cd74","aa776e7028084e7ebab4b9716d801c1d","9f175d5f0ecc457e950e61255cbe48f9","f465a8172ad846aea7aae94839edc436","a3631955d58b43aab7a89d77009508ce","11ea43f726524319b2127cd9d3ba0e30","a668eeb2206441c2adc1a9665ceb2084","4df1d41eea2047dc92cac4b27b6ea432","aeee2096ab024462808469b3ad7da2bc","429392263d3842cda65af8e8c5005ba4","bcb5d82babaa491c98dcfebc8226d820","8049d69d2d4e40a892946a8d33520697","67a3f7be363a4e8a97356ff7dce824ec","dbf1524e3f4f40ea83a18585be56ae94","0a424b76457c4a71925c52c5d3284dc4","09f19d9ab3924ba9aad61a28fce064c2","4fa119df3c804e219bf9fab3c4fd4f46","3025504d55f34e29bd81ac25860c9d4e","4577b09b8e73454a9806374a083a3855","06646a56de9c40bb9317ae652754a981","152d43674c0949ab963606b7d052563c","af7eeb5a49014c22946a9ab627166da3","3b49abd558334ba081414e27772916d5","557b142124d74d11bd20d22485f279a5","ea75f777ddae4976a6626fcf818dd219","3283ab13684342e68cf4a55e81f8904f","3be4e61837b847f7a5bf0cba5e6bc2b6","d5ef0ea543e947f4b021ed727cc933aa","67f0f9a691454954bbfc40deafaf5ccb","8f21b4cbe43345d987e639b934230e65","46d8f8e3d194406a80ed76db49b961b3","a472b9654929469783c472884a4ffd8a","1355ecf663664fb092e9a34c21fc7b37","38c58d8ce8a34c9ead7bb70bbbb07b38","c7756961b6374312b425401844f174ff","354a407b506f423db0522ac19cc3bb60","5d716213c847404a8c316a4ddf27f2de","01ac03d97c084afca04e214481a99fc4"]},"id":"kB1ZJyhlp4O8","executionInfo":{"status":"ok","timestamp":1640447946578,"user_tz":-60,"elapsed":51690,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"957caac5-fad9-4783-8fed-81c458e54df8"},"source":["train, test = train_test_split(dfPJ, test_size=0.2, random_state=0, stratify=dfPJ.label)\n","\n","train = Dataset.from_pandas(train)\n","test = Dataset.from_pandas(test)\n","\n","tokenizer = AutoTokenizer.from_pretrained(modeltype)\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n","\n","tokenized_train = train.map(tokenize_function, batched=True)\n","tokenized_test = test.map(tokenize_function, batched=True)\n","\n","full_train_dataset = tokenized_train\n","full_eval_dataset = tokenized_test\n","\n","model = AutoModelForSequenceClassification.from_pretrained(modeltype, num_labels=2)\n","\n","training_args = TrainingArguments(\n","    \"SIMCSE_BERT_CON\", \n","    evaluation_strategy=\"epoch\",\n","    save_strategy = 'no',\n","    save_steps = 100000,\n","    save_total_limit = 1,\n","    metric_for_best_model=\"eval_f1\")\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n","    acc = accuracy_score(labels, preds)\n","    print(classification_report(labels, preds, labels=[0,1]))\n","    print(confusion_matrix(labels,preds))\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }"],"execution_count":8,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88e41be6abfa4a539001de4dc612be30","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/252 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c9ef67cc03ec4678a291291023a3e31b","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/689 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6144756a1c47491f8b2b5ef415025193","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9e71aa061c44cf6a2592b04326a7278","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"11ea43f726524319b2127cd9d3ba0e30","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/8 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4fa119df3c804e219bf9fab3c4fd4f46","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5ef0ea543e947f4b021ed727cc933aa","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/418M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","metadata":{"id":"W_PFsTTOqm4a"},"source":["# Hyperparameter Optimization"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N9iWNo8V7gby","executionInfo":{"status":"ok","timestamp":1640447952671,"user_tz":-60,"elapsed":6110,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"68c1144e-080f-499a-892e-06c724749166"},"source":["! pip install optuna --quiet"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 308 kB 8.8 MB/s \n","\u001b[K     |████████████████████████████████| 209 kB 65.1 MB/s \n","\u001b[K     |████████████████████████████████| 80 kB 10.6 MB/s \n","\u001b[K     |████████████████████████████████| 75 kB 5.2 MB/s \n","\u001b[K     |████████████████████████████████| 112 kB 73.2 MB/s \n","\u001b[K     |████████████████████████████████| 149 kB 37.3 MB/s \n","\u001b[K     |████████████████████████████████| 49 kB 6.5 MB/s \n","\u001b[?25h  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","metadata":{"id":"y4BgLFRH7kVg","executionInfo":{"status":"ok","timestamp":1640447952672,"user_tz":-60,"elapsed":20,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}}},"source":["def model_init():\n","    return AutoModelForSequenceClassification.from_pretrained(modeltype, num_labels=2)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mpa3mq0u7sN3","executionInfo":{"status":"ok","timestamp":1640447966197,"user_tz":-60,"elapsed":13542,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"822f62e9-8464-42e1-b8e3-d8dd8f521452"},"source":["trainer = Trainer(\n","      model_init=model_init,\n","      args=training_args, \n","      train_dataset=full_train_dataset, \n","      eval_dataset=full_eval_dataset,\n","      compute_metrics=compute_metrics \n","  )"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"pjNfFgAH7voa","executionInfo":{"status":"ok","timestamp":1640492856036,"user_tz":-60,"elapsed":44889859,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"01345f75-d7e9-4661-ecdd-97bff6ed2042"},"source":["import sklearn.metrics as metrics\n","import optuna\n","import sys\n","import logging\n","\n","def objective (metrics):\n","  return metrics['eval_f1']\n","\n","def hyperparameter_space(trial):\n","\n","    return {\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 5e-8, 5e-1, log=True),\n","        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [2, 4, 8, 16]),\n","        \"weight_decay\": trial.suggest_float(\"weight_decay\", 5e-12, 5e-1, log=True),\n","        \"num_train_epochs\": trial.suggest_float(\"num_train_epochs\",1,8,log=True),\n","        #\"adam_epsilon\": trial.suggest_float(\"adam_epsilon\", 1e-10, 1e-6, log=True),\n","        #\"seed\" : trial.suggest_float(\"seed\",10,60,log=True)\n","        }\n","\n","optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n","study_name = \"SIMCSE_BERT_myPers_CON\"  # Unique identifier of the study.\n","storage_name = \"sqlite:///{}.db\".format(study_name)\n","\n","best_run = trainer.hyperparameter_search(hp_space=hyperparameter_space,compute_objective=objective, n_trials=50, direction=\"maximize\",study_name=study_name, storage=storage_name )\n","\n","study = optuna.create_study()"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-25 15:59:26,570]\u001b[0m A new study created in RDB with name: SIMCSE_BERT_myPers_CON\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["A new study created in RDB with name: SIMCSE_BERT_myPers_CON\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 16025\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='16025' max='16025' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [16025/16025 44:12, Epoch 4/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.682800</td>\n","      <td>0.684055</td>\n","      <td>0.602823</td>\n","      <td>0.493420</td>\n","      <td>0.611534</td>\n","      <td>0.545014</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.679200</td>\n","      <td>0.830209</td>\n","      <td>0.613407</td>\n","      <td>0.598443</td>\n","      <td>0.601234</td>\n","      <td>0.597887</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.769100</td>\n","      <td>1.290149</td>\n","      <td>0.613407</td>\n","      <td>0.608094</td>\n","      <td>0.607954</td>\n","      <td>0.609743</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.686100</td>\n","      <td>1.567699</td>\n","      <td>0.617944</td>\n","      <td>0.609603</td>\n","      <td>0.609454</td>\n","      <td>0.609784</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.647700</td>\n","      <td>1.568755</td>\n","      <td>0.617944</td>\n","      <td>0.609720</td>\n","      <td>0.609546</td>\n","      <td>0.609940</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.60      0.93      0.73      1142\n","           1       0.62      0.16      0.26       842\n","\n","    accuracy                           0.60      1984\n","   macro avg       0.61      0.55      0.49      1984\n","weighted avg       0.61      0.60      0.53      1984\n","\n","[[1059   83]\n"," [ 705  137]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.65      0.70      0.68      1142\n","           1       0.55      0.50      0.52       842\n","\n","    accuracy                           0.61      1984\n","   macro avg       0.60      0.60      0.60      1984\n","weighted avg       0.61      0.61      0.61      1984\n","\n","[[800 342]\n"," [425 417]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.67      0.63      0.65      1142\n","           1       0.54      0.59      0.56       842\n","\n","    accuracy                           0.61      1984\n","   macro avg       0.61      0.61      0.61      1984\n","weighted avg       0.62      0.61      0.61      1984\n","\n","[[724 418]\n"," [349 493]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.67      0.66      0.67      1142\n","           1       0.55      0.56      0.55       842\n","\n","    accuracy                           0.62      1984\n","   macro avg       0.61      0.61      0.61      1984\n","weighted avg       0.62      0.62      0.62      1984\n","\n","[[758 384]\n"," [374 468]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.67      0.66      0.67      1142\n","           1       0.55      0.56      0.55       842\n","\n","    accuracy                           0.62      1984\n","   macro avg       0.61      0.61      0.61      1984\n","weighted avg       0.62      0.62      0.62      1984\n","\n","[[757 385]\n"," [373 469]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-25 16:43:42,123]\u001b[0m Trial 0 finished with value: 0.6097196385862345 and parameters: {'learning_rate': 4.476801730909803e-06, 'per_device_train_batch_size': 2, 'weight_decay': 2.5850386437508592e-11, 'num_train_epochs': 4.039553163999802}. Best is trial 0 with value: 0.6097196385862345.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 0 finished with value: 0.6097196385862345 and parameters: {'learning_rate': 4.476801730909803e-06, 'per_device_train_batch_size': 2, 'weight_decay': 2.5850386437508592e-11, 'num_train_epochs': 4.039553163999802}. Best is trial 0 with value: 0.6097196385862345.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2272\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2272' max='2272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2272/2272 36:34, Epoch 4/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.676743</td>\n","      <td>0.577621</td>\n","      <td>0.375874</td>\n","      <td>0.610007</td>\n","      <td>0.503155</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.678900</td>\n","      <td>0.670517</td>\n","      <td>0.589214</td>\n","      <td>0.475121</td>\n","      <td>0.577371</td>\n","      <td>0.530853</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.664900</td>\n","      <td>0.668263</td>\n","      <td>0.582157</td>\n","      <td>0.535135</td>\n","      <td>0.559483</td>\n","      <td>0.546563</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.651400</td>\n","      <td>0.666888</td>\n","      <td>0.586694</td>\n","      <td>0.540997</td>\n","      <td>0.565495</td>\n","      <td>0.551595</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.639100</td>\n","      <td>0.667016</td>\n","      <td>0.584677</td>\n","      <td>0.540687</td>\n","      <td>0.563124</td>\n","      <td>0.550468</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.64      0.01      0.02       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.61      0.50      0.38      1984\n","weighted avg       0.61      0.58      0.43      1984\n","\n","[[1137    5]\n"," [ 833    9]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.59      0.92      0.72      1142\n","           1       0.56      0.14      0.23       842\n","\n","    accuracy                           0.59      1984\n","   macro avg       0.58      0.53      0.48      1984\n","weighted avg       0.58      0.59      0.51      1984\n","\n","[[1047   95]\n"," [ 720  122]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.61      0.78      0.68      1142\n","           1       0.51      0.31      0.39       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.56      0.55      0.54      1984\n","weighted avg       0.57      0.58      0.56      1984\n","\n","[[893 249]\n"," [580 262]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.61      0.78      0.69      1142\n","           1       0.52      0.32      0.40       842\n","\n","    accuracy                           0.59      1984\n","   macro avg       0.57      0.55      0.54      1984\n","weighted avg       0.57      0.59      0.56      1984\n","\n","[[895 247]\n"," [573 269]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.61      0.78      0.68      1142\n","           1       0.52      0.32      0.40       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.56      0.55      0.54      1984\n","weighted avg       0.57      0.58      0.56      1984\n","\n","[[887 255]\n"," [569 273]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-25 17:21:13,363]\u001b[0m Trial 1 finished with value: 0.5406865472036322 and parameters: {'learning_rate': 2.513839845748559e-06, 'per_device_train_batch_size': 16, 'weight_decay': 0.0007864277011823985, 'num_train_epochs': 4.580298688440557}. Best is trial 0 with value: 0.6097196385862345.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 1 finished with value: 0.5406865472036322 and parameters: {'learning_rate': 2.513839845748559e-06, 'per_device_train_batch_size': 16, 'weight_decay': 0.0007864277011823985, 'num_train_epochs': 4.580298688440557}. Best is trial 0 with value: 0.6097196385862345.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 7495\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='7495' max='7495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7495/7495 20:20, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.688000</td>\n","      <td>0.682054</td>\n","      <td>0.575605</td>\n","      <td>0.365323</td>\n","      <td>0.287802</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.684400</td>\n","      <td>0.685335</td>\n","      <td>0.575605</td>\n","      <td>0.365323</td>\n","      <td>0.287802</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.00      0.00      0.00       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.29      0.50      0.37      1984\n","weighted avg       0.33      0.58      0.42      1984\n","\n","[[1142    0]\n"," [ 842    0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.00      0.00      0.00       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.29      0.50      0.37      1984\n","weighted avg       0.33      0.58      0.42      1984\n","\n","[[1142    0]\n"," [ 842    0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-25 17:41:35,909]\u001b[0m Trial 2 finished with value: 0.3653230966090851 and parameters: {'learning_rate': 7.8526174359856e-05, 'per_device_train_batch_size': 2, 'weight_decay': 0.002627620322773597, 'num_train_epochs': 1.8892289717413373}. Best is trial 0 with value: 0.6097196385862345.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 2 finished with value: 0.3653230966090851 and parameters: {'learning_rate': 7.8526174359856e-05, 'per_device_train_batch_size': 2, 'weight_decay': 0.002627620322773597, 'num_train_epochs': 1.8892289717413373}. Best is trial 0 with value: 0.6097196385862345.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 8\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3760\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3760' max='3760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3760/3760 1:00:23, Epoch 7/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.676559</td>\n","      <td>0.577117</td>\n","      <td>0.375658</td>\n","      <td>0.588471</td>\n","      <td>0.502717</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.678700</td>\n","      <td>0.669727</td>\n","      <td>0.584677</td>\n","      <td>0.482380</td>\n","      <td>0.563421</td>\n","      <td>0.529564</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.663200</td>\n","      <td>0.668294</td>\n","      <td>0.575101</td>\n","      <td>0.551731</td>\n","      <td>0.557359</td>\n","      <td>0.553225</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.645200</td>\n","      <td>0.662218</td>\n","      <td>0.587198</td>\n","      <td>0.551601</td>\n","      <td>0.567500</td>\n","      <td>0.557337</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.625300</td>\n","      <td>0.661797</td>\n","      <td>0.598790</td>\n","      <td>0.572273</td>\n","      <td>0.582492</td>\n","      <td>0.574271</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.608000</td>\n","      <td>0.662289</td>\n","      <td>0.599294</td>\n","      <td>0.571062</td>\n","      <td>0.582774</td>\n","      <td>0.573617</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.595800</td>\n","      <td>0.662946</td>\n","      <td>0.602319</td>\n","      <td>0.577703</td>\n","      <td>0.586790</td>\n","      <td>0.579051</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.583700</td>\n","      <td>0.663186</td>\n","      <td>0.600806</td>\n","      <td>0.578953</td>\n","      <td>0.585736</td>\n","      <td>0.579610</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      0.99      0.73      1142\n","           1       0.60      0.01      0.02       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.59      0.50      0.38      1984\n","weighted avg       0.59      0.58      0.43      1984\n","\n","[[1136    6]\n"," [ 833    9]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.59      0.89      0.71      1142\n","           1       0.53      0.17      0.25       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.56      0.53      0.48      1984\n","weighted avg       0.57      0.58      0.52      1984\n","\n","[[1021  121]\n"," [ 703  139]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.70      0.65      1142\n","           1       0.50      0.41      0.45       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.56      0.55      0.55      1984\n","weighted avg       0.57      0.58      0.57      1984\n","\n","[[797 345]\n"," [498 344]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.75      0.68      1142\n","           1       0.52      0.36      0.43       842\n","\n","    accuracy                           0.59      1984\n","   macro avg       0.57      0.56      0.55      1984\n","weighted avg       0.57      0.59      0.57      1984\n","\n","[[862 280]\n"," [539 303]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.63      0.74      0.68      1142\n","           1       0.54      0.41      0.47       842\n","\n","    accuracy                           0.60      1984\n","   macro avg       0.58      0.57      0.57      1984\n","weighted avg       0.59      0.60      0.59      1984\n","\n","[[841 301]\n"," [495 347]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.63      0.74      0.68      1142\n","           1       0.54      0.40      0.46       842\n","\n","    accuracy                           0.60      1984\n","   macro avg       0.58      0.57      0.57      1984\n","weighted avg       0.59      0.60      0.59      1984\n","\n","[[849 293]\n"," [502 340]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.63      0.73      0.68      1142\n","           1       0.54      0.43      0.48       842\n","\n","    accuracy                           0.60      1984\n","   macro avg       0.59      0.58      0.58      1984\n","weighted avg       0.59      0.60      0.59      1984\n","\n","[[837 305]\n"," [484 358]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.64      0.72      0.67      1142\n","           1       0.54      0.44      0.48       842\n","\n","    accuracy                           0.60      1984\n","   macro avg       0.59      0.58      0.58      1984\n","weighted avg       0.59      0.60      0.59      1984\n","\n","[[822 320]\n"," [472 370]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-25 18:42:03,355]\u001b[0m Trial 3 finished with value: 0.5789527839586361 and parameters: {'learning_rate': 2.551381220577768e-06, 'per_device_train_batch_size': 16, 'weight_decay': 0.005755809641012473, 'num_train_epochs': 7.578759623990395}. Best is trial 0 with value: 0.6097196385862345.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 3 finished with value: 0.5789527839586361 and parameters: {'learning_rate': 2.551381220577768e-06, 'per_device_train_batch_size': 16, 'weight_decay': 0.005755809641012473, 'num_train_epochs': 7.578759623990395}. Best is trial 0 with value: 0.6097196385862345.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5428\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5428' max='5428' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5428/5428 15:05, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.672500</td>\n","      <td>0.682023</td>\n","      <td>0.577117</td>\n","      <td>0.370311</td>\n","      <td>0.688277</td>\n","      <td>0.501937</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.678700</td>\n","      <td>0.681344</td>\n","      <td>0.577117</td>\n","      <td>0.370311</td>\n","      <td>0.688277</td>\n","      <td>0.501937</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.80      0.00      0.01       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.69      0.50      0.37      1984\n","weighted avg       0.67      0.58      0.42      1984\n","\n","[[1141    1]\n"," [ 838    4]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.80      0.00      0.01       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.69      0.50      0.37      1984\n","weighted avg       0.67      0.58      0.42      1984\n","\n","[[1141    1]\n"," [ 838    4]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-25 18:57:11,403]\u001b[0m Trial 4 finished with value: 0.3703105027563971 and parameters: {'learning_rate': 8.147063369349608e-07, 'per_device_train_batch_size': 2, 'weight_decay': 8.968596925101844e-07, 'num_train_epochs': 1.3682781516434461}. Best is trial 0 with value: 0.6097196385862345.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 4 finished with value: 0.3703105027563971 and parameters: {'learning_rate': 8.147063369349608e-07, 'per_device_train_batch_size': 2, 'weight_decay': 8.968596925101844e-07, 'num_train_epochs': 1.3682781516434461}. Best is trial 0 with value: 0.6097196385862345.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3827\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3827' max='3827' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3827/3827 32:28, Epoch 3/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.678600</td>\n","      <td>0.645220</td>\n","      <td>0.615423</td>\n","      <td>0.555777</td>\n","      <td>0.607979</td>\n","      <td>0.573119</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.515700</td>\n","      <td>0.689808</td>\n","      <td>0.649194</td>\n","      <td>0.632071</td>\n","      <td>0.638916</td>\n","      <td>0.631001</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.231100</td>\n","      <td>1.461254</td>\n","      <td>0.634073</td>\n","      <td>0.628638</td>\n","      <td>0.628269</td>\n","      <td>0.630190</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.097300</td>\n","      <td>1.952022</td>\n","      <td>0.640121</td>\n","      <td>0.629494</td>\n","      <td>0.630534</td>\n","      <td>0.628892</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.85      0.72      1142\n","           1       0.60      0.29      0.39       842\n","\n","    accuracy                           0.62      1984\n","   macro avg       0.61      0.57      0.56      1984\n","weighted avg       0.61      0.62      0.58      1984\n","\n","[[974 168]\n"," [595 247]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.68      0.75      0.71      1142\n","           1       0.60      0.51      0.55       842\n","\n","    accuracy                           0.65      1984\n","   macro avg       0.64      0.63      0.63      1984\n","weighted avg       0.64      0.65      0.64      1984\n","\n","[[858 284]\n"," [412 430]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.69      0.66      0.67      1142\n","           1       0.56      0.60      0.58       842\n","\n","    accuracy                           0.63      1984\n","   macro avg       0.63      0.63      0.63      1984\n","weighted avg       0.64      0.63      0.64      1984\n","\n","[[749 393]\n"," [333 509]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.68      0.70      0.69      1142\n","           1       0.58      0.55      0.57       842\n","\n","    accuracy                           0.64      1984\n","   macro avg       0.63      0.63      0.63      1984\n","weighted avg       0.64      0.64      0.64      1984\n","\n","[[803 339]\n"," [375 467]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-25 19:29:43,119]\u001b[0m Trial 5 finished with value: 0.6294944760629394 and parameters: {'learning_rate': 2.6055036956959964e-05, 'per_device_train_batch_size': 8, 'weight_decay': 0.0008487645275541602, 'num_train_epochs': 3.857300200725326}. Best is trial 5 with value: 0.6294944760629394.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 5 finished with value: 0.6294944760629394 and parameters: {'learning_rate': 2.6055036956959964e-05, 'per_device_train_batch_size': 8, 'weight_decay': 0.0008487645275541602, 'num_train_epochs': 3.857300200725326}. Best is trial 5 with value: 0.6294944760629394.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1341\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1342' max='1341' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1341/1341 20:59, Epoch 2.70/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.671329</td>\n","      <td>0.585685</td>\n","      <td>0.424492</td>\n","      <td>0.596631</td>\n","      <td>0.517492</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.674600</td>\n","      <td>0.662618</td>\n","      <td>0.598790</td>\n","      <td>0.555058</td>\n","      <td>0.581200</td>\n","      <td>0.564287</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.636600</td>\n","      <td>0.663601</td>\n","      <td>0.585181</td>\n","      <td>0.559048</td>\n","      <td>0.567397</td>\n","      <td>0.561201</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      0.97      0.73      1142\n","           1       0.61      0.07      0.12       842\n","\n","    accuracy                           0.59      1984\n","   macro avg       0.60      0.52      0.42      1984\n","weighted avg       0.59      0.59      0.47      1984\n","\n","[[1106   36]\n"," [ 786   56]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.79      0.69      1142\n","           1       0.54      0.34      0.42       842\n","\n","    accuracy                           0.60      1984\n","   macro avg       0.58      0.56      0.56      1984\n","weighted avg       0.59      0.60      0.58      1984\n","\n","[[905 237]\n"," [559 283]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.72      0.67      1142\n","           1       0.51      0.40      0.45       842\n","\n","    accuracy                           0.59      1984\n","   macro avg       0.57      0.56      0.56      1984\n","weighted avg       0.58      0.59      0.58      1984\n","\n","[[822 320]\n"," [503 339]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-25 19:51:25,819]\u001b[0m Trial 6 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 6 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2896\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='993' max='2896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 993/2896 15:10 < 29:09, 1.09 it/s, Epoch 2/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.684048</td>\n","      <td>0.564012</td>\n","      <td>0.453244</td>\n","      <td>0.516821</td>\n","      <td>0.507557</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.685800</td>\n","      <td>0.682471</td>\n","      <td>0.570565</td>\n","      <td>0.421145</td>\n","      <td>0.520532</td>\n","      <td>0.505293</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      0.88      0.70      1142\n","           1       0.45      0.13      0.21       842\n","\n","    accuracy                           0.56      1984\n","   macro avg       0.52      0.51      0.45      1984\n","weighted avg       0.53      0.56      0.49      1984\n","\n","[[1006  136]\n"," [ 729  113]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      0.94      0.72      1142\n","           1       0.46      0.07      0.13       842\n","\n","    accuracy                           0.57      1984\n","   macro avg       0.52      0.51      0.42      1984\n","weighted avg       0.53      0.57      0.47      1984\n","\n","[[1070   72]\n"," [ 780   62]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-25 20:07:20,188]\u001b[0m Trial 7 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 7 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1559\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='993' max='1559' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 993/1559 07:42 < 04:24, 2.14 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>8.186200</td>\n","      <td>2.073207</td>\n","      <td>0.575605</td>\n","      <td>0.365323</td>\n","      <td>0.287802</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.00      0.00      0.00       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.29      0.50      0.37      1984\n","weighted avg       0.33      0.58      0.42      1984\n","\n","[[1142    0]\n"," [ 842    0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-25 20:15:46,023]\u001b[0m Trial 8 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 8 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4932\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1985' max='4932' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1985/4932 08:27 < 12:34, 3.91 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>31.644000</td>\n","      <td>131.424866</td>\n","      <td>0.575605</td>\n","      <td>0.365323</td>\n","      <td>0.287802</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.00      0.00      0.00       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.29      0.50      0.37      1984\n","weighted avg       0.33      0.58      0.42      1984\n","\n","[[1142    0]\n"," [ 842    0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-25 20:24:56,638]\u001b[0m Trial 9 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 9 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3803\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='993' max='3803' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 993/3803 07:42 < 21:52, 2.14 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.053800</td>\n","      <td>0.696259</td>\n","      <td>0.424395</td>\n","      <td>0.297948</td>\n","      <td>0.212198</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00      1142\n","           1       0.42      1.00      0.60       842\n","\n","    accuracy                           0.42      1984\n","   macro avg       0.21      0.50      0.30      1984\n","weighted avg       0.18      0.42      0.25      1984\n","\n","[[   0 1142]\n"," [   0  842]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-25 20:33:22,441]\u001b[0m Trial 10 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 10 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3753\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='993' max='3753' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 993/3753 07:42 < 21:28, 2.14 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.696300</td>\n","      <td>0.682878</td>\n","      <td>0.575605</td>\n","      <td>0.365323</td>\n","      <td>0.287802</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.00      0.00      0.00       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.29      0.50      0.37      1984\n","weighted avg       0.33      0.58      0.42      1984\n","\n","[[1142    0]\n"," [ 842    0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-25 20:41:48,137]\u001b[0m Trial 11 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 11 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 19565\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3968' max='19565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 3968/19565 10:00 < 39:22, 6.60 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.789900</td>\n","      <td>0.683108</td>\n","      <td>0.575605</td>\n","      <td>0.365323</td>\n","      <td>0.287802</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.00      0.00      0.00       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.29      0.50      0.37      1984\n","weighted avg       0.33      0.58      0.42      1984\n","\n","[[1142    0]\n"," [ 842    0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-25 20:52:31,706]\u001b[0m Trial 12 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 12 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6515\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6515' max='6515' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6515/6515 30:30, Epoch 3/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.674200</td>\n","      <td>0.645986</td>\n","      <td>0.621472</td>\n","      <td>0.581234</td>\n","      <td>0.610259</td>\n","      <td>0.588045</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.519400</td>\n","      <td>0.793892</td>\n","      <td>0.647177</td>\n","      <td>0.633594</td>\n","      <td>0.636988</td>\n","      <td>0.632526</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.347000</td>\n","      <td>1.706247</td>\n","      <td>0.637097</td>\n","      <td>0.630145</td>\n","      <td>0.629761</td>\n","      <td>0.630789</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.150400</td>\n","      <td>1.810802</td>\n","      <td>0.638609</td>\n","      <td>0.626516</td>\n","      <td>0.628424</td>\n","      <td>0.625707</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.63      0.81      0.71      1142\n","           1       0.59      0.37      0.45       842\n","\n","    accuracy                           0.62      1984\n","   macro avg       0.61      0.59      0.58      1984\n","weighted avg       0.61      0.62      0.60      1984\n","\n","[[924 218]\n"," [533 309]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.68      0.73      0.70      1142\n","           1       0.59      0.54      0.56       842\n","\n","    accuracy                           0.65      1984\n","   macro avg       0.64      0.63      0.63      1984\n","weighted avg       0.64      0.65      0.64      1984\n","\n","[[833 309]\n"," [391 451]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.69      0.67      0.68      1142\n","           1       0.57      0.59      0.58       842\n","\n","    accuracy                           0.64      1984\n","   macro avg       0.63      0.63      0.63      1984\n","weighted avg       0.64      0.64      0.64      1984\n","\n","[[768 374]\n"," [346 496]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.68      0.71      0.69      1142\n","           1       0.58      0.54      0.56       842\n","\n","    accuracy                           0.64      1984\n","   macro avg       0.63      0.63      0.63      1984\n","weighted avg       0.64      0.64      0.64      1984\n","\n","[[812 330]\n"," [387 455]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-25 21:23:05,175]\u001b[0m Trial 13 finished with value: 0.6265161243402462 and parameters: {'learning_rate': 1.8625421064374152e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.44413277348876284, 'num_train_epochs': 3.2835956183603625}. Best is trial 5 with value: 0.6294944760629394.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 13 finished with value: 0.6265161243402462 and parameters: {'learning_rate': 1.8625421064374152e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.44413277348876284, 'num_train_epochs': 3.2835956183603625}. Best is trial 5 with value: 0.6294944760629394.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4330\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4330' max='4330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4330/4330 20:31, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.682800</td>\n","      <td>0.658393</td>\n","      <td>0.617944</td>\n","      <td>0.553847</td>\n","      <td>0.614072</td>\n","      <td>0.573749</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.525600</td>\n","      <td>0.880254</td>\n","      <td>0.626008</td>\n","      <td>0.616895</td>\n","      <td>0.617012</td>\n","      <td>0.616789</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.488400</td>\n","      <td>1.099419</td>\n","      <td>0.623992</td>\n","      <td>0.611043</td>\n","      <td>0.612995</td>\n","      <td>0.610358</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.87      0.72      1142\n","           1       0.61      0.28      0.38       842\n","\n","    accuracy                           0.62      1984\n","   macro avg       0.61      0.57      0.55      1984\n","weighted avg       0.62      0.62      0.58      1984\n","\n","[[989 153]\n"," [605 237]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.67      0.68      0.68      1142\n","           1       0.56      0.56      0.56       842\n","\n","    accuracy                           0.63      1984\n","   macro avg       0.62      0.62      0.62      1984\n","weighted avg       0.63      0.63      0.63      1984\n","\n","[[774 368]\n"," [374 468]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.66      0.70      0.68      1142\n","           1       0.56      0.52      0.54       842\n","\n","    accuracy                           0.62      1984\n","   macro avg       0.61      0.61      0.61      1984\n","weighted avg       0.62      0.62      0.62      1984\n","\n","[[800 342]\n"," [404 438]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-25 21:43:38,762]\u001b[0m Trial 14 finished with value: 0.6110429589731137 and parameters: {'learning_rate': 3.1267957411782596e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.31936477624863213, 'num_train_epochs': 2.1822213333826617}. Best is trial 5 with value: 0.6294944760629394.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 14 finished with value: 0.6110429589731137 and parameters: {'learning_rate': 3.1267957411782596e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.31936477624863213, 'num_train_epochs': 2.1822213333826617}. Best is trial 5 with value: 0.6294944760629394.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6049\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1985' max='6049' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1985/6049 08:27 < 17:21, 3.90 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.861300</td>\n","      <td>0.719313</td>\n","      <td>0.575605</td>\n","      <td>0.365323</td>\n","      <td>0.287802</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.00      0.00      0.00       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.29      0.50      0.37      1984\n","weighted avg       0.33      0.58      0.42      1984\n","\n","[[1142    0]\n"," [ 842    0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-25 21:52:49,667]\u001b[0m Trial 15 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 15 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6400\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1985' max='6400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1985/6400 08:28 < 18:51, 3.90 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.686500</td>\n","      <td>0.681761</td>\n","      <td>0.570565</td>\n","      <td>0.397193</td>\n","      <td>0.505986</td>\n","      <td>0.500926</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      0.96      0.72      1142\n","           1       0.44      0.04      0.07       842\n","\n","    accuracy                           0.57      1984\n","   macro avg       0.51      0.50      0.40      1984\n","weighted avg       0.52      0.57      0.45      1984\n","\n","[[1098   44]\n"," [ 808   34]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-25 22:02:00,688]\u001b[0m Trial 16 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 16 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1026\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1027' max='1026' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1026/1026 08:40, Epoch 1.03/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.674600</td>\n","      <td>0.656796</td>\n","      <td>0.610383</td>\n","      <td>0.539799</td>\n","      <td>0.604240</td>\n","      <td>0.564061</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.663900</td>\n","      <td>0.656877</td>\n","      <td>0.609375</td>\n","      <td>0.537309</td>\n","      <td>0.603119</td>\n","      <td>0.562562</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.61      0.87      0.72      1142\n","           1       0.59      0.26      0.36       842\n","\n","    accuracy                           0.61      1984\n","   macro avg       0.60      0.56      0.54      1984\n","weighted avg       0.61      0.61      0.57      1984\n","\n","[[994 148]\n"," [625 217]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.61      0.87      0.72      1142\n","           1       0.59      0.25      0.35       842\n","\n","    accuracy                           0.61      1984\n","   macro avg       0.60      0.56      0.54      1984\n","weighted avg       0.60      0.61      0.56      1984\n","\n","[[996 146]\n"," [629 213]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-25 22:11:24,041]\u001b[0m Trial 17 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 17 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 12543\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1985' max='12543' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 1985/12543 08:28 < 45:07, 3.90 it/s, Epoch 1/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.719400</td>\n","      <td>0.691281</td>\n","      <td>0.575605</td>\n","      <td>0.365323</td>\n","      <td>0.287802</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.00      0.00      0.00       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.29      0.50      0.37      1984\n","weighted avg       0.33      0.58      0.42      1984\n","\n","[[1142    0]\n"," [ 842    0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-25 22:20:35,698]\u001b[0m Trial 18 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 18 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3216\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='993' max='3216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 993/3216 07:43 < 17:20, 2.14 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.022500</td>\n","      <td>1.474794</td>\n","      <td>0.575605</td>\n","      <td>0.365323</td>\n","      <td>0.287802</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.00      0.00      0.00       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.29      0.50      0.37      1984\n","weighted avg       0.33      0.58      0.42      1984\n","\n","[[1142    0]\n"," [ 842    0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-25 22:29:02,747]\u001b[0m Trial 19 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 19 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 10558\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1985' max='10558' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 1985/10558 08:28 < 36:39, 3.90 it/s, Epoch 1/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.686800</td>\n","      <td>0.680456</td>\n","      <td>0.576109</td>\n","      <td>0.372042</td>\n","      <td>0.560867</td>\n","      <td>0.501374</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.55      0.01      0.01       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.56      0.50      0.37      1984\n","weighted avg       0.56      0.58      0.43      1984\n","\n","[[1137    5]\n"," [ 836    6]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-25 22:38:14,421]\u001b[0m Trial 20 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 20 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4689\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4689' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4689/4689 22:04, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.699200</td>\n","      <td>0.668533</td>\n","      <td>0.595262</td>\n","      <td>0.508645</td>\n","      <td>0.582150</td>\n","      <td>0.544219</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.573600</td>\n","      <td>0.740422</td>\n","      <td>0.613911</td>\n","      <td>0.606292</td>\n","      <td>0.606019</td>\n","      <td>0.606749</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.361300</td>\n","      <td>1.345192</td>\n","      <td>0.619960</td>\n","      <td>0.610699</td>\n","      <td>0.610811</td>\n","      <td>0.610599</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.60      0.88      0.71      1142\n","           1       0.56      0.21      0.30       842\n","\n","    accuracy                           0.60      1984\n","   macro avg       0.58      0.54      0.51      1984\n","weighted avg       0.59      0.60      0.54      1984\n","\n","[[1007  135]\n"," [ 668  174]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.67      0.65      0.66      1142\n","           1       0.54      0.56      0.55       842\n","\n","    accuracy                           0.61      1984\n","   macro avg       0.61      0.61      0.61      1984\n","weighted avg       0.62      0.61      0.61      1984\n","\n","[[747 395]\n"," [371 471]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.67      0.67      0.67      1142\n","           1       0.55      0.55      0.55       842\n","\n","    accuracy                           0.62      1984\n","   macro avg       0.61      0.61      0.61      1984\n","weighted avg       0.62      0.62      0.62      1984\n","\n","[[768 374]\n"," [380 462]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-25 23:00:21,636]\u001b[0m Trial 21 finished with value: 0.6106989501954396 and parameters: {'learning_rate': 3.8766647519613884e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.3951941333092171, 'num_train_epochs': 2.3629865645428607}. Best is trial 5 with value: 0.6294944760629394.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 21 finished with value: 0.6106989501954396 and parameters: {'learning_rate': 3.8766647519613884e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.3951941333092171, 'num_train_epochs': 2.3629865645428607}. Best is trial 5 with value: 0.6294944760629394.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3948\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3948' max='3948' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3948/3948 18:14, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.674600</td>\n","      <td>0.648621</td>\n","      <td>0.614415</td>\n","      <td>0.562576</td>\n","      <td>0.603742</td>\n","      <td>0.575520</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.506200</td>\n","      <td>0.823552</td>\n","      <td>0.632560</td>\n","      <td>0.613825</td>\n","      <td>0.620798</td>\n","      <td>0.613277</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.83      0.71      1142\n","           1       0.58      0.32      0.41       842\n","\n","    accuracy                           0.61      1984\n","   macro avg       0.60      0.58      0.56      1984\n","weighted avg       0.61      0.61      0.59      1984\n","\n","[[951 191]\n"," [574 268]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.66      0.74      0.70      1142\n","           1       0.58      0.49      0.53       842\n","\n","    accuracy                           0.63      1984\n","   macro avg       0.62      0.61      0.61      1984\n","weighted avg       0.63      0.63      0.63      1984\n","\n","[[846 296]\n"," [433 409]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-25 23:18:38,591]\u001b[0m Trial 22 finished with value: 0.6138250553295381 and parameters: {'learning_rate': 2.2143500784731613e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.20733035651443532, 'num_train_epochs': 1.9895336756655286}. Best is trial 5 with value: 0.6294944760629394.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 22 finished with value: 0.6138250553295381 and parameters: {'learning_rate': 2.2143500784731613e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.20733035651443532, 'num_train_epochs': 1.9895336756655286}. Best is trial 5 with value: 0.6294944760629394.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3926\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1985' max='3926' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1985/3926 08:29 < 08:18, 3.89 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.698800</td>\n","      <td>0.691770</td>\n","      <td>0.575605</td>\n","      <td>0.365323</td>\n","      <td>0.287802</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.00      0.00      0.00       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.29      0.50      0.37      1984\n","weighted avg       0.33      0.58      0.42      1984\n","\n","[[1142    0]\n"," [ 842    0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-25 23:27:50,834]\u001b[0m Trial 23 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 23 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1470\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1470' max='1470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1470/1470 12:49, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.677500</td>\n","      <td>0.648073</td>\n","      <td>0.612399</td>\n","      <td>0.522978</td>\n","      <td>0.619308</td>\n","      <td>0.559729</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.658900</td>\n","      <td>0.700977</td>\n","      <td>0.630544</td>\n","      <td>0.619967</td>\n","      <td>0.620770</td>\n","      <td>0.619481</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.61      0.91      0.73      1142\n","           1       0.63      0.21      0.32       842\n","\n","    accuracy                           0.61      1984\n","   macro avg       0.62      0.56      0.52      1984\n","weighted avg       0.62      0.61      0.55      1984\n","\n","[[1037  105]\n"," [ 664  178]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.67      0.69      0.68      1142\n","           1       0.57      0.55      0.56       842\n","\n","    accuracy                           0.63      1984\n","   macro avg       0.62      0.62      0.62      1984\n","weighted avg       0.63      0.63      0.63      1984\n","\n","[[791 351]\n"," [382 460]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-25 23:40:43,385]\u001b[0m Trial 24 finished with value: 0.619966576902523 and parameters: {'learning_rate': 2.732009086725888e-05, 'per_device_train_batch_size': 8, 'weight_decay': 0.007863758418677648, 'num_train_epochs': 1.4816277342397546}. Best is trial 5 with value: 0.6294944760629394.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 24 finished with value: 0.619966576902523 and parameters: {'learning_rate': 2.732009086725888e-05, 'per_device_train_batch_size': 8, 'weight_decay': 0.007863758418677648, 'num_train_epochs': 1.4816277342397546}. Best is trial 5 with value: 0.6294944760629394.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 995\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='996' max='995' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [995/995 08:26, Epoch 1.00/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.675400</td>\n","      <td>0.668650</td>\n","      <td>0.590222</td>\n","      <td>0.521687</td>\n","      <td>0.570174</td>\n","      <td>0.545456</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.675400</td>\n","      <td>0.668649</td>\n","      <td>0.590222</td>\n","      <td>0.521687</td>\n","      <td>0.570174</td>\n","      <td>0.545456</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.60      0.84      0.70      1142\n","           1       0.54      0.25      0.34       842\n","\n","    accuracy                           0.59      1984\n","   macro avg       0.57      0.55      0.52      1984\n","weighted avg       0.58      0.59      0.55      1984\n","\n","[[961 181]\n"," [632 210]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.60      0.84      0.70      1142\n","           1       0.54      0.25      0.34       842\n","\n","    accuracy                           0.59      1984\n","   macro avg       0.57      0.55      0.52      1984\n","weighted avg       0.58      0.59      0.55      1984\n","\n","[[961 181]\n"," [632 210]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-25 23:49:53,204]\u001b[0m Trial 25 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 25 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1364\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='993' max='1364' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 993/1364 07:44 < 02:53, 2.13 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.681800</td>\n","      <td>0.680121</td>\n","      <td>0.574093</td>\n","      <td>0.379526</td>\n","      <td>0.513825</td>\n","      <td>0.500870</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      0.99      0.73      1142\n","           1       0.45      0.02      0.03       842\n","\n","    accuracy                           0.57      1984\n","   macro avg       0.51      0.50      0.38      1984\n","weighted avg       0.52      0.57      0.43      1984\n","\n","[[1125   17]\n"," [ 828   14]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-25 23:58:24,861]\u001b[0m Trial 26 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 26 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4227\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='993' max='4227' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 993/4227 07:45 < 25:17, 2.13 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.695400</td>\n","      <td>0.681793</td>\n","      <td>0.575605</td>\n","      <td>0.365323</td>\n","      <td>0.287802</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.00      0.00      0.00       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.29      0.50      0.37      1984\n","weighted avg       0.33      0.58      0.42      1984\n","\n","[[1142    0]\n"," [ 842    0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-26 00:06:53,468]\u001b[0m Trial 27 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 27 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3455\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='993' max='3455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 993/3455 07:44 < 19:15, 2.13 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.680300</td>\n","      <td>0.677610</td>\n","      <td>0.578125</td>\n","      <td>0.376090</td>\n","      <td>0.634840</td>\n","      <td>0.503593</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.69      0.01      0.02       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.63      0.50      0.38      1984\n","weighted avg       0.63      0.58      0.43      1984\n","\n","[[1138    4]\n"," [ 833    9]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-26 00:15:21,734]\u001b[0m Trial 28 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 28 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2778\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='993' max='2778' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 993/2778 07:45 < 13:58, 2.13 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.676400</td>\n","      <td>0.668116</td>\n","      <td>0.595766</td>\n","      <td>0.464601</td>\n","      <td>0.609101</td>\n","      <td>0.533113</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.59      0.95      0.73      1142\n","           1       0.62      0.12      0.20       842\n","\n","    accuracy                           0.60      1984\n","   macro avg       0.61      0.53      0.46      1984\n","weighted avg       0.61      0.60      0.50      1984\n","\n","[[1082   60]\n"," [ 742  100]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-26 00:23:50,639]\u001b[0m Trial 29 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 29 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6619\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3968' max='6619' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3968/6619 10:02 < 06:42, 6.58 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.687700</td>\n","      <td>0.681690</td>\n","      <td>0.575605</td>\n","      <td>0.365323</td>\n","      <td>0.287802</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.00      0.00      0.00       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.29      0.50      0.37      1984\n","weighted avg       0.33      0.58      0.42      1984\n","\n","[[1142    0]\n"," [ 842    0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-26 00:34:36,631]\u001b[0m Trial 30 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 30 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2647\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2648' max='2647' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2647/2647 11:59, Epoch 1.33/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.669600</td>\n","      <td>0.648158</td>\n","      <td>0.619456</td>\n","      <td>0.553810</td>\n","      <td>0.617442</td>\n","      <td>0.574594</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.531900</td>\n","      <td>0.713848</td>\n","      <td>0.614415</td>\n","      <td>0.590970</td>\n","      <td>0.600425</td>\n","      <td>0.591743</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.87      0.72      1142\n","           1       0.61      0.28      0.38       842\n","\n","    accuracy                           0.62      1984\n","   macro avg       0.62      0.57      0.55      1984\n","weighted avg       0.62      0.62      0.58      1984\n","\n","[[995 147]\n"," [608 234]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.64      0.74      0.69      1142\n","           1       0.56      0.44      0.49       842\n","\n","    accuracy                           0.61      1984\n","   macro avg       0.60      0.59      0.59      1984\n","weighted avg       0.61      0.61      0.61      1984\n","\n","[[847 295]\n"," [470 372]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-26 00:47:19,616]\u001b[0m Trial 31 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 31 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3486\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3487' max='3486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3486/3486 15:36, Epoch 1.76/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.672600</td>\n","      <td>0.648790</td>\n","      <td>0.620968</td>\n","      <td>0.560483</td>\n","      <td>0.617206</td>\n","      <td>0.578092</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.527300</td>\n","      <td>0.744273</td>\n","      <td>0.617944</td>\n","      <td>0.602751</td>\n","      <td>0.605900</td>\n","      <td>0.602140</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.86      0.72      1142\n","           1       0.61      0.29      0.40       842\n","\n","    accuracy                           0.62      1984\n","   macro avg       0.62      0.58      0.56      1984\n","weighted avg       0.62      0.62      0.59      1984\n","\n","[[984 158]\n"," [594 248]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.66      0.71      0.68      1142\n","           1       0.56      0.50      0.53       842\n","\n","    accuracy                           0.62      1984\n","   macro avg       0.61      0.60      0.60      1984\n","weighted avg       0.61      0.62      0.61      1984\n","\n","[[807 335]\n"," [423 419]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-26 01:03:39,052]\u001b[0m Trial 32 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 32 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4207\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4207' max='4207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4207/4207 20:02, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.683700</td>\n","      <td>0.655274</td>\n","      <td>0.615927</td>\n","      <td>0.564119</td>\n","      <td>0.605922</td>\n","      <td>0.576989</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.525500</td>\n","      <td>0.842478</td>\n","      <td>0.622984</td>\n","      <td>0.610586</td>\n","      <td>0.612180</td>\n","      <td>0.609950</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.508600</td>\n","      <td>0.940546</td>\n","      <td>0.627016</td>\n","      <td>0.614894</td>\n","      <td>0.616449</td>\n","      <td>0.614233</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.83      0.71      1142\n","           1       0.59      0.32      0.41       842\n","\n","    accuracy                           0.62      1984\n","   macro avg       0.61      0.58      0.56      1984\n","weighted avg       0.61      0.62      0.59      1984\n","\n","[[953 189]\n"," [573 269]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.66      0.70      0.68      1142\n","           1       0.56      0.52      0.54       842\n","\n","    accuracy                           0.62      1984\n","   macro avg       0.61      0.61      0.61      1984\n","weighted avg       0.62      0.62      0.62      1984\n","\n","[[795 347]\n"," [401 441]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.67      0.70      0.68      1142\n","           1       0.56      0.53      0.55       842\n","\n","    accuracy                           0.63      1984\n","   macro avg       0.62      0.61      0.61      1984\n","weighted avg       0.62      0.63      0.63      1984\n","\n","[[798 344]\n"," [396 446]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-26 01:23:44,008]\u001b[0m Trial 33 finished with value: 0.6148939027665861 and parameters: {'learning_rate': 3.6251056916748545e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.0015849419549483902, 'num_train_epochs': 2.120146567349586}. Best is trial 5 with value: 0.6294944760629394.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 33 finished with value: 0.6148939027665861 and parameters: {'learning_rate': 3.6251056916748545e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.0015849419549483902, 'num_train_epochs': 2.120146567349586}. Best is trial 5 with value: 0.6294944760629394.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 589\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='497' max='589' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [497/589 07:16 < 01:21, 1.13 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.675891</td>\n","      <td>0.578125</td>\n","      <td>0.392352</td>\n","      <td>0.567102</td>\n","      <td>0.506089</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      0.98      0.73      1142\n","           1       0.56      0.03      0.06       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.57      0.51      0.39      1984\n","weighted avg       0.57      0.58      0.44      1984\n","\n","[[1122   20]\n"," [ 817   25]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-26 01:31:44,305]\u001b[0m Trial 34 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 34 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4345\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='993' max='4345' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 993/4345 07:44 < 26:12, 2.13 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.693300</td>\n","      <td>0.660139</td>\n","      <td>0.604839</td>\n","      <td>0.485344</td>\n","      <td>0.627735</td>\n","      <td>0.544426</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.60      0.94      0.73      1142\n","           1       0.66      0.14      0.24       842\n","\n","    accuracy                           0.60      1984\n","   macro avg       0.63      0.54      0.49      1984\n","weighted avg       0.62      0.60      0.52      1984\n","\n","[[1078   64]\n"," [ 720  122]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-26 01:40:12,683]\u001b[0m Trial 35 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 35 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 8754\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3968' max='8754' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3968/8754 10:03 < 12:08, 6.57 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.670400</td>\n","      <td>0.688489</td>\n","      <td>0.578629</td>\n","      <td>0.388643</td>\n","      <td>0.580938</td>\n","      <td>0.505903</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      0.99      0.73      1142\n","           1       0.58      0.02      0.05       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.58      0.51      0.39      1984\n","weighted avg       0.58      0.58      0.44      1984\n","\n","[[1127   15]\n"," [ 821   21]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-26 01:50:59,258]\u001b[0m Trial 36 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 36 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1399\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='497' max='1399' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 497/1399 07:16 < 13:15, 1.13 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.681866</td>\n","      <td>0.575605</td>\n","      <td>0.365323</td>\n","      <td>0.287802</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.00      0.00      0.00       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.29      0.50      0.37      1984\n","weighted avg       0.33      0.58      0.42      1984\n","\n","[[1142    0]\n"," [ 842    0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-26 01:58:59,343]\u001b[0m Trial 37 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 37 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1496\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='993' max='1496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 993/1496 07:44 < 03:55, 2.13 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.711200</td>\n","      <td>0.687654</td>\n","      <td>0.575605</td>\n","      <td>0.365323</td>\n","      <td>0.287802</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.00      0.00      0.00       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.29      0.50      0.37      1984\n","weighted avg       0.33      0.58      0.42      1984\n","\n","[[1142    0]\n"," [ 842    0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-26 02:07:27,414]\u001b[0m Trial 38 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 38 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5161\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1985' max='5161' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1985/5161 08:28 < 13:34, 3.90 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.686800</td>\n","      <td>0.680998</td>\n","      <td>0.576109</td>\n","      <td>0.379393</td>\n","      <td>0.548412</td>\n","      <td>0.502466</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      0.99      0.73      1142\n","           1       0.52      0.02      0.03       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.55      0.50      0.38      1984\n","weighted avg       0.55      0.58      0.43      1984\n","\n","[[1130   12]\n"," [ 829   13]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-26 02:16:38,770]\u001b[0m Trial 39 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 39 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1978\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='497' max='1978' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 497/1978 07:16 < 21:44, 1.14 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.681935</td>\n","      <td>0.575605</td>\n","      <td>0.365323</td>\n","      <td>0.287802</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.00      0.00      0.00       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.29      0.50      0.37      1984\n","weighted avg       0.33      0.58      0.42      1984\n","\n","[[1142    0]\n"," [ 842    0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-26 02:24:38,579]\u001b[0m Trial 40 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 40 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3774\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3775' max='3774' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3774/3774 16:47, Epoch 1.90/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.673100</td>\n","      <td>0.653618</td>\n","      <td>0.610383</td>\n","      <td>0.539369</td>\n","      <td>0.604430</td>\n","      <td>0.563905</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.555300</td>\n","      <td>0.696450</td>\n","      <td>0.602319</td>\n","      <td>0.590648</td>\n","      <td>0.591398</td>\n","      <td>0.590283</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.61      0.87      0.72      1142\n","           1       0.60      0.26      0.36       842\n","\n","    accuracy                           0.61      1984\n","   macro avg       0.60      0.56      0.54      1984\n","weighted avg       0.61      0.61      0.57      1984\n","\n","[[995 147]\n"," [626 216]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.65      0.67      0.66      1142\n","           1       0.53      0.51      0.52       842\n","\n","    accuracy                           0.60      1984\n","   macro avg       0.59      0.59      0.59      1984\n","weighted avg       0.60      0.60      0.60      1984\n","\n","[[765 377]\n"," [412 430]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-26 02:42:09,537]\u001b[0m Trial 41 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 41 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4013\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1985' max='4013' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1985/4013 08:28 < 08:40, 3.90 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.677300</td>\n","      <td>0.671611</td>\n","      <td>0.594758</td>\n","      <td>0.465380</td>\n","      <td>0.603709</td>\n","      <td>0.532549</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.59      0.94      0.73      1142\n","           1       0.61      0.12      0.20       842\n","\n","    accuracy                           0.59      1984\n","   macro avg       0.60      0.53      0.47      1984\n","weighted avg       0.60      0.59      0.51      1984\n","\n","[[1078   64]\n"," [ 740  102]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-26 02:51:21,252]\u001b[0m Trial 42 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 42 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4586\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4586' max='4586' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4586/4586 21:37, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.681500</td>\n","      <td>0.653099</td>\n","      <td>0.620968</td>\n","      <td>0.560863</td>\n","      <td>0.617013</td>\n","      <td>0.578248</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.518000</td>\n","      <td>0.854999</td>\n","      <td>0.634577</td>\n","      <td>0.626315</td>\n","      <td>0.626222</td>\n","      <td>0.626416</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.256400</td>\n","      <td>1.336550</td>\n","      <td>0.632056</td>\n","      <td>0.625218</td>\n","      <td>0.624829</td>\n","      <td>0.625943</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.86      0.72      1142\n","           1       0.61      0.30      0.40       842\n","\n","    accuracy                           0.62      1984\n","   macro avg       0.62      0.58      0.56      1984\n","weighted avg       0.62      0.62      0.59      1984\n","\n","[[983 159]\n"," [593 249]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.68      0.68      0.68      1142\n","           1       0.57      0.57      0.57       842\n","\n","    accuracy                           0.63      1984\n","   macro avg       0.63      0.63      0.63      1984\n","weighted avg       0.63      0.63      0.63      1984\n","\n","[[777 365]\n"," [360 482]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.69      0.67      0.68      1142\n","           1       0.56      0.59      0.57       842\n","\n","    accuracy                           0.63      1984\n","   macro avg       0.62      0.63      0.63      1984\n","weighted avg       0.63      0.63      0.63      1984\n","\n","[[761 381]\n"," [349 493]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-26 03:13:01,032]\u001b[0m Trial 43 finished with value: 0.6252178845429289 and parameters: {'learning_rate': 3.11431339958987e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.10656525639450408, 'num_train_epochs': 2.3114297349756527}. Best is trial 5 with value: 0.6294944760629394.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 43 finished with value: 0.6252178845429289 and parameters: {'learning_rate': 3.11431339958987e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.10656525639450408, 'num_train_epochs': 2.3114297349756527}. Best is trial 5 with value: 0.6294944760629394.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4507\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1985' max='4507' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1985/4507 08:28 < 10:47, 3.90 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.690500</td>\n","      <td>0.670067</td>\n","      <td>0.594254</td>\n","      <td>0.457911</td>\n","      <td>0.608788</td>\n","      <td>0.530551</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.59      0.95      0.73      1142\n","           1       0.63      0.11      0.19       842\n","\n","    accuracy                           0.59      1984\n","   macro avg       0.61      0.53      0.46      1984\n","weighted avg       0.61      0.59      0.50      1984\n","\n","[[1087   55]\n"," [ 750   92]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-26 03:22:13,150]\u001b[0m Trial 44 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 44 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 14118\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3968' max='14118' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 3968/14118 10:00 < 25:37, 6.60 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.687100</td>\n","      <td>0.682452</td>\n","      <td>0.575605</td>\n","      <td>0.365323</td>\n","      <td>0.287802</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.00      0.00      0.00       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.29      0.50      0.37      1984\n","weighted avg       0.33      0.58      0.42      1984\n","\n","[[1142    0]\n"," [ 842    0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-26 03:32:56,957]\u001b[0m Trial 45 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 45 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6008\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6008' max='6008' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6008/6008 28:24, Epoch 3/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.673500</td>\n","      <td>0.650873</td>\n","      <td>0.614919</td>\n","      <td>0.550716</td>\n","      <td>0.609104</td>\n","      <td>0.570810</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.545100</td>\n","      <td>0.714862</td>\n","      <td>0.628528</td>\n","      <td>0.621676</td>\n","      <td>0.621298</td>\n","      <td>0.622410</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.407800</td>\n","      <td>1.041902</td>\n","      <td>0.630544</td>\n","      <td>0.622421</td>\n","      <td>0.622268</td>\n","      <td>0.622601</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.417000</td>\n","      <td>1.042553</td>\n","      <td>0.631048</td>\n","      <td>0.623219</td>\n","      <td>0.622995</td>\n","      <td>0.623507</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.62      0.86      0.72      1142\n","           1       0.60      0.28      0.38       842\n","\n","    accuracy                           0.61      1984\n","   macro avg       0.61      0.57      0.55      1984\n","weighted avg       0.61      0.61      0.58      1984\n","\n","[[985 157]\n"," [607 235]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.68      0.66      0.67      1142\n","           1       0.56      0.58      0.57       842\n","\n","    accuracy                           0.63      1984\n","   macro avg       0.62      0.62      0.62      1984\n","weighted avg       0.63      0.63      0.63      1984\n","\n","[[757 385]\n"," [352 490]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.68      0.68      0.68      1142\n","           1       0.56      0.57      0.57       842\n","\n","    accuracy                           0.63      1984\n","   macro avg       0.62      0.62      0.62      1984\n","weighted avg       0.63      0.63      0.63      1984\n","\n","[[771 371]\n"," [362 480]]\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.68      0.67      0.68      1142\n","           1       0.56      0.57      0.57       842\n","\n","    accuracy                           0.63      1984\n","   macro avg       0.62      0.62      0.62      1984\n","weighted avg       0.63      0.63      0.63      1984\n","\n","[[769 373]\n"," [359 483]]\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2021-12-26 04:01:24,500]\u001b[0m Trial 46 finished with value: 0.6232188166435766 and parameters: {'learning_rate': 1.0901366929821377e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.002238456984312462, 'num_train_epochs': 3.0279507513620847}. Best is trial 5 with value: 0.6294944760629394.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 46 finished with value: 0.6232188166435766 and parameters: {'learning_rate': 1.0901366929821377e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.002238456984312462, 'num_train_epochs': 3.0279507513620847}. Best is trial 5 with value: 0.6294944760629394.\n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3040\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='993' max='3040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 993/3040 07:45 < 16:01, 2.13 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.680900</td>\n","      <td>0.678447</td>\n","      <td>0.578125</td>\n","      <td>0.376090</td>\n","      <td>0.634840</td>\n","      <td>0.503593</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.69      0.01      0.02       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.63      0.50      0.38      1984\n","weighted avg       0.63      0.58      0.43      1984\n","\n","[[1138    4]\n"," [ 833    9]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-26 04:09:53,625]\u001b[0m Trial 47 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 47 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 9498\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1985' max='9498' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1985/9498 08:29 < 32:09, 3.89 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.675300</td>\n","      <td>0.659259</td>\n","      <td>0.602823</td>\n","      <td>0.518071</td>\n","      <td>0.596416</td>\n","      <td>0.552034</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.61      0.89      0.72      1142\n","           1       0.59      0.22      0.32       842\n","\n","    accuracy                           0.60      1984\n","   macro avg       0.60      0.55      0.52      1984\n","weighted avg       0.60      0.60      0.55      1984\n","\n","[[1014  128]\n"," [ 660  182]]\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-26 04:19:06,204]\u001b[0m Trial 48 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 48 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["Trial:\n","loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n","Model config BertConfig {\n","  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running training *****\n","  Num examples = 7933\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3436\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='993' max='3436' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 993/3436 07:45 < 19:08, 2.13 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.784800</td>\n","      <td>0.683060</td>\n","      <td>0.575605</td>\n","      <td>0.365323</td>\n","      <td>0.287802</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n","***** Running Evaluation *****\n","  Num examples = 1984\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.58      1.00      0.73      1142\n","           1       0.00      0.00      0.00       842\n","\n","    accuracy                           0.58      1984\n","   macro avg       0.29      0.50      0.37      1984\n","weighted avg       0.33      0.58      0.42      1984\n","\n","[[1142    0]\n"," [ 842    0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","\u001b[32m[I 2021-12-26 04:27:35,468]\u001b[0m Trial 49 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Trial 49 pruned. \n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-26 04:27:35,513]\u001b[0m A new study created in memory with name: no-name-aa29dc1e-b182-435c-a620-46e1df593737\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["A new study created in memory with name: no-name-aa29dc1e-b182-435c-a620-46e1df593737\n"]}]},{"cell_type":"code","metadata":{"id":"rHREpYlgsNkL","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1640492856038,"user_tz":-60,"elapsed":27,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"14462daa-728d-45ed-fff4-7341b2463a1a"},"source":["storage_name"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'sqlite:///SIMCSE_BERT_myPers_CON.db'"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"vlqXIjF6sPqH","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1640492856039,"user_tz":-60,"elapsed":23,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"0d829707-93b7-49ec-97ff-c4fe01e34cc8"},"source":["study_name"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'SIMCSE_BERT_myPers_CON'"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"mXXqQglwpuZy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640492856041,"user_tz":-60,"elapsed":24,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"e148a227-c40f-486f-a520-61aa45e06f57"},"source":["study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True, direction=\"maximize\")\n","df = study.trials_dataframe(attrs=(\"number\", \"value\", \"params\", \"state\"))"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-12-26 04:27:35,627]\u001b[0m Using an existing study with name 'SIMCSE_BERT_myPers_CON' instead of creating a new one.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Using an existing study with name 'SIMCSE_BERT_myPers_CON' instead of creating a new one.\n"]}]},{"cell_type":"code","metadata":{"id":"JzU25SO_tKCP","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1640492856043,"user_tz":-60,"elapsed":22,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"bd21b6dd-1805-4f82-b9af-b225c4003f39"},"source":["df"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-35e80982-3f88-4e3a-8d0d-020733841125\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>number</th>\n","      <th>value</th>\n","      <th>params_learning_rate</th>\n","      <th>params_num_train_epochs</th>\n","      <th>params_per_device_train_batch_size</th>\n","      <th>params_weight_decay</th>\n","      <th>state</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.609720</td>\n","      <td>4.476802e-06</td>\n","      <td>4.039553</td>\n","      <td>2</td>\n","      <td>2.585039e-11</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.540687</td>\n","      <td>2.513840e-06</td>\n","      <td>4.580299</td>\n","      <td>16</td>\n","      <td>7.864277e-04</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.365323</td>\n","      <td>7.852617e-05</td>\n","      <td>1.889229</td>\n","      <td>2</td>\n","      <td>2.627620e-03</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.578953</td>\n","      <td>2.551381e-06</td>\n","      <td>7.578760</td>\n","      <td>16</td>\n","      <td>5.755810e-03</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.370311</td>\n","      <td>8.147063e-07</td>\n","      <td>1.368278</td>\n","      <td>2</td>\n","      <td>8.968597e-07</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>0.629494</td>\n","      <td>2.605504e-05</td>\n","      <td>3.857300</td>\n","      <td>8</td>\n","      <td>8.487645e-04</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>0.559048</td>\n","      <td>6.815180e-06</td>\n","      <td>2.702363</td>\n","      <td>16</td>\n","      <td>4.113021e-07</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>0.421145</td>\n","      <td>9.382343e-08</td>\n","      <td>5.836803</td>\n","      <td>16</td>\n","      <td>9.150592e-04</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>0.365323</td>\n","      <td>6.998312e-02</td>\n","      <td>1.570599</td>\n","      <td>8</td>\n","      <td>3.901923e-10</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>0.365323</td>\n","      <td>3.838724e-01</td>\n","      <td>2.485405</td>\n","      <td>4</td>\n","      <td>2.542253e-02</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>0.297948</td>\n","      <td>4.417557e-03</td>\n","      <td>3.832715</td>\n","      <td>8</td>\n","      <td>1.253859e-05</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>0.365323</td>\n","      <td>1.500857e-04</td>\n","      <td>3.783069</td>\n","      <td>8</td>\n","      <td>2.223496e-11</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>0.365323</td>\n","      <td>8.245395e-04</td>\n","      <td>4.931844</td>\n","      <td>2</td>\n","      <td>1.475683e-08</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>0.626516</td>\n","      <td>1.862542e-05</td>\n","      <td>3.283596</td>\n","      <td>4</td>\n","      <td>4.441328e-01</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>0.611043</td>\n","      <td>3.126796e-05</td>\n","      <td>2.182221</td>\n","      <td>4</td>\n","      <td>3.193648e-01</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>15</td>\n","      <td>0.365323</td>\n","      <td>1.750193e-03</td>\n","      <td>3.048426</td>\n","      <td>4</td>\n","      <td>4.493768e-01</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>0.397193</td>\n","      <td>1.172474e-07</td>\n","      <td>3.225785</td>\n","      <td>4</td>\n","      <td>4.123949e-05</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>0.537309</td>\n","      <td>1.606039e-05</td>\n","      <td>1.034013</td>\n","      <td>8</td>\n","      <td>1.109200e-04</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>18</td>\n","      <td>0.365323</td>\n","      <td>4.101179e-04</td>\n","      <td>6.321625</td>\n","      <td>4</td>\n","      <td>4.723429e-02</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>19</td>\n","      <td>0.365323</td>\n","      <td>5.175077e-03</td>\n","      <td>3.241376</td>\n","      <td>8</td>\n","      <td>4.765689e-02</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>20</td>\n","      <td>0.372042</td>\n","      <td>4.222658e-07</td>\n","      <td>5.321166</td>\n","      <td>4</td>\n","      <td>1.456055e-04</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>21</td>\n","      <td>0.610699</td>\n","      <td>3.876665e-05</td>\n","      <td>2.362987</td>\n","      <td>4</td>\n","      <td>3.951941e-01</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>22</td>\n","      <td>0.613825</td>\n","      <td>2.214350e-05</td>\n","      <td>1.989534</td>\n","      <td>4</td>\n","      <td>2.073304e-01</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>23</td>\n","      <td>0.365323</td>\n","      <td>2.621395e-04</td>\n","      <td>1.978694</td>\n","      <td>4</td>\n","      <td>1.980987e-02</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>24</td>\n","      <td>0.619967</td>\n","      <td>2.732009e-05</td>\n","      <td>1.481628</td>\n","      <td>8</td>\n","      <td>7.863758e-03</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>25</td>\n","      <td>0.521687</td>\n","      <td>8.875073e-06</td>\n","      <td>1.002638</td>\n","      <td>8</td>\n","      <td>8.951680e-06</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>26</td>\n","      <td>0.379526</td>\n","      <td>7.045918e-07</td>\n","      <td>1.374747</td>\n","      <td>8</td>\n","      <td>4.526986e-04</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>27</td>\n","      <td>0.365323</td>\n","      <td>8.813590e-05</td>\n","      <td>4.260813</td>\n","      <td>8</td>\n","      <td>6.220691e-03</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>28</td>\n","      <td>0.376090</td>\n","      <td>1.747884e-06</td>\n","      <td>3.482356</td>\n","      <td>8</td>\n","      <td>2.055461e-07</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>29</td>\n","      <td>0.464601</td>\n","      <td>6.485265e-06</td>\n","      <td>2.800020</td>\n","      <td>8</td>\n","      <td>4.922328e-03</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>30</td>\n","      <td>0.365323</td>\n","      <td>7.397385e-05</td>\n","      <td>1.668370</td>\n","      <td>2</td>\n","      <td>3.128401e-08</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>31</td>\n","      <td>0.590970</td>\n","      <td>2.034561e-05</td>\n","      <td>1.334165</td>\n","      <td>4</td>\n","      <td>7.332020e-02</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>32</td>\n","      <td>0.602751</td>\n","      <td>1.539934e-05</td>\n","      <td>1.756645</td>\n","      <td>4</td>\n","      <td>9.712358e-02</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>33</td>\n","      <td>0.614894</td>\n","      <td>3.625106e-05</td>\n","      <td>2.120147</td>\n","      <td>4</td>\n","      <td>1.584942e-03</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>34</td>\n","      <td>0.392352</td>\n","      <td>3.591918e-06</td>\n","      <td>1.185643</td>\n","      <td>16</td>\n","      <td>1.181105e-03</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>35</td>\n","      <td>0.485344</td>\n","      <td>5.237964e-05</td>\n","      <td>4.379622</td>\n","      <td>8</td>\n","      <td>9.634832e-03</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>36</td>\n","      <td>0.388643</td>\n","      <td>1.714005e-06</td>\n","      <td>2.206612</td>\n","      <td>2</td>\n","      <td>2.443782e-04</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>37</td>\n","      <td>0.365323</td>\n","      <td>2.300991e-04</td>\n","      <td>2.818909</td>\n","      <td>16</td>\n","      <td>4.990691e-06</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>38</td>\n","      <td>0.365323</td>\n","      <td>6.497007e-04</td>\n","      <td>1.507150</td>\n","      <td>8</td>\n","      <td>2.076203e-03</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>39</td>\n","      <td>0.379393</td>\n","      <td>2.554232e-07</td>\n","      <td>2.601305</td>\n","      <td>4</td>\n","      <td>2.111448e-03</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>40</td>\n","      <td>0.365323</td>\n","      <td>1.237027e-04</td>\n","      <td>3.986470</td>\n","      <td>16</td>\n","      <td>3.721801e-05</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>41</td>\n","      <td>0.590648</td>\n","      <td>9.838876e-06</td>\n","      <td>1.901959</td>\n","      <td>4</td>\n","      <td>1.435857e-01</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>42</td>\n","      <td>0.465380</td>\n","      <td>3.979345e-06</td>\n","      <td>2.022368</td>\n","      <td>4</td>\n","      <td>1.535045e-02</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>43</td>\n","      <td>0.625218</td>\n","      <td>3.114313e-05</td>\n","      <td>2.311430</td>\n","      <td>4</td>\n","      <td>1.065653e-01</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>44</td>\n","      <td>0.457911</td>\n","      <td>4.360371e-05</td>\n","      <td>2.271505</td>\n","      <td>4</td>\n","      <td>1.762699e-02</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>45</td>\n","      <td>0.365323</td>\n","      <td>9.171147e-05</td>\n","      <td>3.558699</td>\n","      <td>2</td>\n","      <td>5.208456e-04</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>46</td>\n","      <td>0.623219</td>\n","      <td>1.090137e-05</td>\n","      <td>3.027951</td>\n","      <td>4</td>\n","      <td>2.238457e-03</td>\n","      <td>COMPLETE</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>47</td>\n","      <td>0.376090</td>\n","      <td>1.247230e-06</td>\n","      <td>3.064415</td>\n","      <td>8</td>\n","      <td>3.569614e-10</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>48</td>\n","      <td>0.518071</td>\n","      <td>6.138048e-06</td>\n","      <td>4.787164</td>\n","      <td>4</td>\n","      <td>4.971160e-03</td>\n","      <td>PRUNED</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>49</td>\n","      <td>0.365323</td>\n","      <td>1.260668e-03</td>\n","      <td>3.463462</td>\n","      <td>8</td>\n","      <td>5.443707e-02</td>\n","      <td>PRUNED</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35e80982-3f88-4e3a-8d0d-020733841125')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-35e80982-3f88-4e3a-8d0d-020733841125 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-35e80982-3f88-4e3a-8d0d-020733841125');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["    number     value  ...  params_weight_decay     state\n","0        0  0.609720  ...         2.585039e-11  COMPLETE\n","1        1  0.540687  ...         7.864277e-04  COMPLETE\n","2        2  0.365323  ...         2.627620e-03  COMPLETE\n","3        3  0.578953  ...         5.755810e-03  COMPLETE\n","4        4  0.370311  ...         8.968597e-07  COMPLETE\n","5        5  0.629494  ...         8.487645e-04  COMPLETE\n","6        6  0.559048  ...         4.113021e-07    PRUNED\n","7        7  0.421145  ...         9.150592e-04    PRUNED\n","8        8  0.365323  ...         3.901923e-10    PRUNED\n","9        9  0.365323  ...         2.542253e-02    PRUNED\n","10      10  0.297948  ...         1.253859e-05    PRUNED\n","11      11  0.365323  ...         2.223496e-11    PRUNED\n","12      12  0.365323  ...         1.475683e-08    PRUNED\n","13      13  0.626516  ...         4.441328e-01  COMPLETE\n","14      14  0.611043  ...         3.193648e-01  COMPLETE\n","15      15  0.365323  ...         4.493768e-01    PRUNED\n","16      16  0.397193  ...         4.123949e-05    PRUNED\n","17      17  0.537309  ...         1.109200e-04    PRUNED\n","18      18  0.365323  ...         4.723429e-02    PRUNED\n","19      19  0.365323  ...         4.765689e-02    PRUNED\n","20      20  0.372042  ...         1.456055e-04    PRUNED\n","21      21  0.610699  ...         3.951941e-01  COMPLETE\n","22      22  0.613825  ...         2.073304e-01  COMPLETE\n","23      23  0.365323  ...         1.980987e-02    PRUNED\n","24      24  0.619967  ...         7.863758e-03  COMPLETE\n","25      25  0.521687  ...         8.951680e-06    PRUNED\n","26      26  0.379526  ...         4.526986e-04    PRUNED\n","27      27  0.365323  ...         6.220691e-03    PRUNED\n","28      28  0.376090  ...         2.055461e-07    PRUNED\n","29      29  0.464601  ...         4.922328e-03    PRUNED\n","30      30  0.365323  ...         3.128401e-08    PRUNED\n","31      31  0.590970  ...         7.332020e-02    PRUNED\n","32      32  0.602751  ...         9.712358e-02    PRUNED\n","33      33  0.614894  ...         1.584942e-03  COMPLETE\n","34      34  0.392352  ...         1.181105e-03    PRUNED\n","35      35  0.485344  ...         9.634832e-03    PRUNED\n","36      36  0.388643  ...         2.443782e-04    PRUNED\n","37      37  0.365323  ...         4.990691e-06    PRUNED\n","38      38  0.365323  ...         2.076203e-03    PRUNED\n","39      39  0.379393  ...         2.111448e-03    PRUNED\n","40      40  0.365323  ...         3.721801e-05    PRUNED\n","41      41  0.590648  ...         1.435857e-01    PRUNED\n","42      42  0.465380  ...         1.535045e-02    PRUNED\n","43      43  0.625218  ...         1.065653e-01  COMPLETE\n","44      44  0.457911  ...         1.762699e-02    PRUNED\n","45      45  0.365323  ...         5.208456e-04    PRUNED\n","46      46  0.623219  ...         2.238457e-03  COMPLETE\n","47      47  0.376090  ...         3.569614e-10    PRUNED\n","48      48  0.518071  ...         4.971160e-03    PRUNED\n","49      49  0.365323  ...         5.443707e-02    PRUNED\n","\n","[50 rows x 7 columns]"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"ef13tgLLtXxY","colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"status":"ok","timestamp":1640492857395,"user_tz":-60,"elapsed":1372,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"1a140289-cfe7-4dca-fd0e-0becbd5d7cb1"},"source":["fig = optuna.visualization.plot_param_importances(study)\n","fig.show()"],"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"bffb764d-6771-432e-8835-bd8687326fe5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"bffb764d-6771-432e-8835-bd8687326fe5\")) {\n","                    Plotly.newPlot(\n","                        'bffb764d-6771-432e-8835-bd8687326fe5',\n","                        [{\"cliponaxis\": false, \"hovertemplate\": [\"weight_decay (LogUniformDistribution): 0.016709735707040586<extra></extra>\", \"learning_rate (LogUniformDistribution): 0.1963907762715631<extra></extra>\", \"per_device_train_batch_size (CategoricalDistribution): 0.38082866814712907<extra></extra>\", \"num_train_epochs (LogUniformDistribution): 0.4060708198742673<extra></extra>\"], \"marker\": {\"color\": \"rgb(66,146,198)\"}, \"orientation\": \"h\", \"text\": [\"0.016709735707040586\", \"0.1963907762715631\", \"0.38082866814712907\", \"0.4060708198742673\"], \"textposition\": \"outside\", \"texttemplate\": \"%{text:.2f}\", \"type\": \"bar\", \"x\": [0.016709735707040586, 0.1963907762715631, 0.38082866814712907, 0.4060708198742673], \"y\": [\"weight_decay\", \"learning_rate\", \"per_device_train_batch_size\", \"num_train_epochs\"]}],\n","                        {\"showlegend\": false, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Hyperparameter Importances\"}, \"xaxis\": {\"title\": {\"text\": \"Importance for Objective Value\"}}, \"yaxis\": {\"title\": {\"text\": \"Hyperparameter\"}}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('bffb764d-6771-432e-8835-bd8687326fe5');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"D8f8-e0Us-Yd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640492857395,"user_tz":-60,"elapsed":10,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"2e03d303-cf29-4327-abc7-c1e621d6d162"},"source":["best_run"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BestRun(run_id='5', objective=0.6294944760629394, hyperparameters={'learning_rate': 2.6055036956959964e-05, 'num_train_epochs': 3.857300200725326, 'per_device_train_batch_size': 8, 'weight_decay': 0.0008487645275541602})"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"Hr5zSnMFnAfY","colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"status":"ok","timestamp":1640492857836,"user_tz":-60,"elapsed":446,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"80813608-20b5-4b0d-f600-3996bd7b559b"},"source":["optuna.visualization.plot_intermediate_values(study)"],"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"653e9200-78cc-4757-beb1-85ef9e427e3d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"653e9200-78cc-4757-beb1-85ef9e427e3d\")) {\n","                    Plotly.newPlot(\n","                        '653e9200-78cc-4757-beb1-85ef9e427e3d',\n","                        [{\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial0\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4], \"y\": [0.493420327836556, 0.5984427772354994, 0.6080944759000694, 0.6096025186934277, 0.6097196385862345]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial1\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4], \"y\": [0.37587391586382524, 0.47512051914039694, 0.5351351504128634, 0.5409972971295403, 0.5406865472036322]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial2\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.3653230966090851, 0.3653230966090851]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial3\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7], \"y\": [0.375657648716659, 0.4823799397910035, 0.551731189663384, 0.5516012997495705, 0.572272508138735, 0.5710620245161027, 0.5777032304013013, 0.5789527839586361]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial4\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.3703105027563971, 0.3703105027563971]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial5\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3], \"y\": [0.5557766217775074, 0.6320710074307128, 0.628638373704706, 0.6294944760629394]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial6\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.4244924404218289, 0.5550581905756851, 0.559047650168095]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial7\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.45324449703342695, 0.4211449110195494]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial8\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3653230966090851]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial9\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3653230966090851]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial10\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.29794762915782025]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial11\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3653230966090851]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial12\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3653230966090851]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial13\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3], \"y\": [0.5812335530522359, 0.6335941020469975, 0.6301451580831179, 0.6265161243402462]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial14\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.5538467380572644, 0.6168947228713741, 0.6110429589731137]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial15\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3653230966090851]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial16\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.39719274221157147]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial17\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.5397990773968223, 0.5373088382257046]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial18\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3653230966090851]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial19\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3653230966090851]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial20\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3720420233375171]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial21\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.508645381599307, 0.6062920975731073, 0.6106989501954396]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial22\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.5625758156095207, 0.6138250553295381]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial23\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3653230966090851]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial24\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.5229777621448393, 0.619966576902523]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial25\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.5216874168768375, 0.5216874168768375]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial26\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3795261544041585]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial27\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3653230966090851]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial28\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3760900806465247]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial29\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.4646014780957304]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial30\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3653230966090851]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial31\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.5538100195553649, 0.5909698377445777]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial32\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.5604826546003017, 0.6027505526041073]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial33\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.5641194787221775, 0.61058636451899, 0.6148939027665861]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial34\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3923523709889358]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial35\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.4853437094682231]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial36\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.38864291453804245]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial37\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3653230966090851]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial38\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3653230966090851]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial39\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3793928140901826]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial40\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3653230966090851]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial41\", \"type\": \"scatter\", \"x\": [0, 1], \"y\": [0.5393689281750698, 0.5906476699587425]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial42\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.46537966537966535]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial43\", \"type\": \"scatter\", \"x\": [0, 1, 2], \"y\": [0.5608629874908021, 0.626314970444746, 0.6252178845429289]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial44\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.4579108019703818]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial45\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3653230966090851]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial46\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3], \"y\": [0.5507155822768451, 0.621676459333288, 0.6224214769282697, 0.6232188166435766]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial47\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3760900806465247]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial48\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.5180713383838385]}, {\"marker\": {\"maxdisplayed\": 10}, \"mode\": \"lines+markers\", \"name\": \"Trial49\", \"type\": \"scatter\", \"x\": [0], \"y\": [0.3653230966090851]}],\n","                        {\"showlegend\": false, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Intermediate Values Plot\"}, \"xaxis\": {\"title\": {\"text\": \"Step\"}}, \"yaxis\": {\"title\": {\"text\": \"Intermediate Value\"}}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('653e9200-78cc-4757-beb1-85ef9e427e3d');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"uFBmj8ysm9pb","colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"status":"ok","timestamp":1640492858210,"user_tz":-60,"elapsed":379,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"964b7673-c8bd-4007-beac-254558e7ba51"},"source":["optuna.visualization.plot_parallel_coordinate(study)"],"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"eaecd16e-e036-4716-99a3-e84ecb0fddfb\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"eaecd16e-e036-4716-99a3-e84ecb0fddfb\")) {\n","                    Plotly.newPlot(\n","                        'eaecd16e-e036-4716-99a3-e84ecb0fddfb',\n","                        [{\"dimensions\": [{\"label\": \"Objective Value\", \"range\": [0.3653230966090851, 0.6294944760629394], \"values\": [0.6097196385862345, 0.3653230966090851, 0.3703105027563971, 0.6265161243402462, 0.6110429589731137, 0.6106989501954396, 0.6138250553295381, 0.6148939027665861, 0.6252178845429289, 0.6232188166435766, 0.6294944760629394, 0.619966576902523, 0.5406865472036322, 0.5789527839586361]}, {\"label\": \"learning_rate\", \"range\": [-6.0889989056542735, -4.104985560001835], \"ticktext\": [\"8.15e-07\", \"1e-06\", \"1e-05\", \"7.85e-05\"], \"tickvals\": [-6.0889989056542735, -6, -5, -4.104985560001835], \"values\": [-5.349032139272695, -4.104985560001835, -6.0889989056542735, -4.729893900385458, -4.504900488241094, -4.411541754491509, -4.654753718086786, -4.440679326846911, -4.506637685645906, -4.962519042164154, -4.584108306485231, -4.563517860514535, -5.59966239424084, -5.59322464541852]}, {\"label\": \"num_train_epochs\", \"range\": [0.13617439229974737, 0.8795981327380937], \"ticktext\": [\"1.37\", \"7.58\"], \"tickvals\": [0.13617439229974737, 0.8795981327380937], \"values\": [0.6063333281943398, 0.2762845969570218, 0.13617439229974737, 0.5163496674704658, 0.3388987971193577, 0.37346125233000527, 0.2987512945916994, 0.32636588507540143, 0.3638806958808871, 0.48114880722644177, 0.5862834398629297, 0.1707390988698194, 0.6608937999496922, 0.8795981327380937]}, {\"label\": \"per_device_train_...\", \"range\": [0, 3], \"ticktext\": [2, 4, 8, 16], \"tickvals\": [0, 1, 2, 3], \"values\": [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 3, 3]}, {\"label\": \"weight_decay\", \"range\": [-10.587532960251929, -0.35248717814324526], \"ticktext\": [\"2.59e-11\", \"1e-10\", \"1e-09\", \"1e-08\", \"1e-07\", \"1e-06\", \"1e-05\", \"0.0001\", \"0.001\", \"0.01\", \"0.1\", \"0.444\"], \"tickvals\": [-10.587532960251929, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, -0.35248717814324526], \"values\": [-10.587532960251929, -2.5804373878324807, -6.0472754940069215, -0.35248717814324526, -0.49571298526836793, -0.40318951117536506, -0.6833371055206541, -2.799986638245735, -0.972384365812418, -2.6500512468937725, -3.071212779212046, -2.104369836865492, -3.1043411972883646, -2.239893577700153]}], \"labelangle\": 30, \"labelside\": \"bottom\", \"line\": {\"color\": [0.6097196385862345, 0.3653230966090851, 0.3703105027563971, 0.6265161243402462, 0.6110429589731137, 0.6106989501954396, 0.6138250553295381, 0.6148939027665861, 0.6252178845429289, 0.6232188166435766, 0.6294944760629394, 0.619966576902523, 0.5406865472036322, 0.5789527839586361], \"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0.0, \"rgb(247,251,255)\"], [0.125, \"rgb(222,235,247)\"], [0.25, \"rgb(198,219,239)\"], [0.375, \"rgb(158,202,225)\"], [0.5, \"rgb(107,174,214)\"], [0.625, \"rgb(66,146,198)\"], [0.75, \"rgb(33,113,181)\"], [0.875, \"rgb(8,81,156)\"], [1.0, \"rgb(8,48,107)\"]], \"reversescale\": false, \"showscale\": true}, \"type\": \"parcoords\"}],\n","                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Parallel Coordinate Plot\"}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('eaecd16e-e036-4716-99a3-e84ecb0fddfb');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"I4B4vAEnDimm","colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"status":"ok","timestamp":1640492858444,"user_tz":-60,"elapsed":243,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"7d897ee4-7cc0-455b-edca-7af491924ba7"},"source":["optuna.visualization.plot_optimization_history(study)"],"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"9f36e2fe-5def-44b4-bd81-3caf4512c2f2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"9f36e2fe-5def-44b4-bd81-3caf4512c2f2\")) {\n","                    Plotly.newPlot(\n","                        '9f36e2fe-5def-44b4-bd81-3caf4512c2f2',\n","                        [{\"mode\": \"markers\", \"name\": \"Objective Value\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 13, 14, 21, 22, 24, 33, 43, 46], \"y\": [0.6097196385862345, 0.5406865472036322, 0.3653230966090851, 0.5789527839586361, 0.3703105027563971, 0.6294944760629394, 0.6265161243402462, 0.6110429589731137, 0.6106989501954396, 0.6138250553295381, 0.619966576902523, 0.6148939027665861, 0.6252178845429289, 0.6232188166435766]}, {\"name\": \"Best Value\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 13, 14, 21, 22, 24, 33, 43, 46], \"y\": [0.6097196385862345, 0.6097196385862345, 0.6097196385862345, 0.6097196385862345, 0.6097196385862345, 0.6294944760629394, 0.6294944760629394, 0.6294944760629394, 0.6294944760629394, 0.6294944760629394, 0.6294944760629394, 0.6294944760629394, 0.6294944760629394, 0.6294944760629394]}],\n","                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Optimization History Plot\"}, \"xaxis\": {\"title\": {\"text\": \"#Trials\"}}, \"yaxis\": {\"title\": {\"text\": \"Objective Value\"}}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('9f36e2fe-5def-44b4-bd81-3caf4512c2f2');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"_S9z32VznLsH","colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"status":"ok","timestamp":1640492859076,"user_tz":-60,"elapsed":637,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"7e88b09f-95bc-45ae-c58c-de58909ee510"},"source":["optuna.visualization.plot_contour(study)"],"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"2c03c0d3-f408-489e-a02b-258599480a08\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"2c03c0d3-f408-489e-a02b-258599480a08\")) {\n","                    Plotly.newPlot(\n","                        '2c03c0d3-f408-489e-a02b-258599480a08',\n","                        [{\"type\": \"scatter\", \"xaxis\": \"x\", \"yaxis\": \"y\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": true, \"type\": \"contour\", \"x\": [6.483364326628161e-07, 8.147063369349608e-07, 2.513839845748559e-06, 2.551381220577768e-06, 4.476801730909803e-06, 1.0901366929821377e-05, 1.8625421064374152e-05, 2.2143500784731613e-05, 2.6055036956959964e-05, 2.732009086725888e-05, 3.11431339958987e-05, 3.1267957411782596e-05, 3.6251056916748545e-05, 3.8766647519613884e-05, 7.8526174359856e-05, 9.867681136393363e-05], \"xaxis\": \"x5\", \"y\": [1.2560392337639659, 1.3682781516434461, 1.4816277342397546, 1.8892289717413373, 1.9895336756655286, 2.120146567349586, 2.1822213333826617, 2.3114297349756527, 2.3629865645428607, 3.0279507513620847, 3.2835956183603625, 3.857300200725326, 4.039553163999802, 4.580298688440557, 7.578759623990395, 8.255993070366346], \"yaxis\": \"y5\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, 0.3703105027563971, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, 0.619966576902523, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.3653230966090851, null], [null, null, null, null, null, null, null, 0.6138250553295381, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, 0.6148939027665861, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, 0.6110429589731137, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, 0.6252178845429289, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, 0.6106989501954396, null, null], [null, null, null, null, null, 0.6232188166435766, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, 0.6265161243402462, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, 0.6294944760629394, null, null, null, null, null, null, null], [null, null, null, null, 0.6097196385862345, null, null, null, null, null, null, null, null, null, null, null], [null, null, 0.5406865472036322, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, 0.5789527839586361, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [4.476801730909803e-06, 2.513839845748559e-06, 7.8526174359856e-05, 2.551381220577768e-06, 8.147063369349608e-07, 2.6055036956959964e-05, 1.8625421064374152e-05, 3.1267957411782596e-05, 3.8766647519613884e-05, 2.2143500784731613e-05, 2.732009086725888e-05, 3.6251056916748545e-05, 3.11431339958987e-05, 1.0901366929821377e-05], \"xaxis\": \"x5\", \"y\": [4.039553163999802, 4.580298688440557, 1.8892289717413373, 7.578759623990395, 1.3682781516434461, 3.857300200725326, 3.2835956183603625, 2.1822213333826617, 2.3629865645428607, 1.9895336756655286, 1.4816277342397546, 2.120146567349586, 2.3114297349756527, 3.0279507513620847], \"yaxis\": \"y5\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [6.483364326628161e-07, 8.147063369349608e-07, 2.513839845748559e-06, 2.551381220577768e-06, 4.476801730909803e-06, 1.0901366929821377e-05, 1.8625421064374152e-05, 2.2143500784731613e-05, 2.6055036956959964e-05, 2.732009086725888e-05, 3.11431339958987e-05, 3.1267957411782596e-05, 3.6251056916748545e-05, 3.8766647519613884e-05, 7.8526174359856e-05, 9.867681136393363e-05], \"xaxis\": \"x9\", \"y\": [1.2999999999999998, 2, 4, 8, 16, 16.7], \"yaxis\": \"y9\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, 0.3703105027563971, null, null, 0.6097196385862345, null, null, null, null, null, null, null, null, null, 0.3653230966090851, null], [null, null, null, null, null, 0.6232188166435766, 0.6265161243402462, 0.6138250553295381, null, null, 0.6252178845429289, 0.6110429589731137, 0.6148939027665861, 0.6106989501954396, null, null], [null, null, null, null, null, null, null, null, 0.6294944760629394, 0.619966576902523, null, null, null, null, null, null], [null, null, 0.5406865472036322, 0.5789527839586361, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [4.476801730909803e-06, 2.513839845748559e-06, 7.8526174359856e-05, 2.551381220577768e-06, 8.147063369349608e-07, 2.6055036956959964e-05, 1.8625421064374152e-05, 3.1267957411782596e-05, 3.8766647519613884e-05, 2.2143500784731613e-05, 2.732009086725888e-05, 3.6251056916748545e-05, 3.11431339958987e-05, 1.0901366929821377e-05], \"xaxis\": \"x9\", \"y\": [2, 16, 2, 16, 2, 8, 4, 4, 4, 4, 8, 4, 4, 4], \"yaxis\": \"y9\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [6.483364326628161e-07, 8.147063369349608e-07, 2.513839845748559e-06, 2.551381220577768e-06, 4.476801730909803e-06, 1.0901366929821377e-05, 1.8625421064374152e-05, 2.2143500784731613e-05, 2.6055036956959964e-05, 2.732009086725888e-05, 3.11431339958987e-05, 3.1267957411782596e-05, 3.6251056916748545e-05, 3.8766647519613884e-05, 7.8526174359856e-05, 9.867681136393363e-05], \"xaxis\": \"x13\", \"y\": [7.956365959576677e-12, 2.5850386437508592e-11, 8.968596925101844e-07, 0.0007864277011823985, 0.0008487645275541602, 0.0015849419549483902, 0.002238456984312462, 0.002627620322773597, 0.005755809641012473, 0.007863758418677648, 0.10656525639450408, 0.20733035651443532, 0.31936477624863213, 0.3951941333092171, 0.44413277348876284, 1.4429959459604629], \"yaxis\": \"y13\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, 0.6097196385862345, null, null, null, null, null, null, null, null, null, null, null], [null, 0.3703105027563971, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, 0.5406865472036322, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, 0.6294944760629394, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, 0.6148939027665861, null, null, null], [null, null, null, null, null, 0.6232188166435766, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.3653230966090851, null], [null, null, null, 0.5789527839586361, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, 0.619966576902523, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, 0.6252178845429289, null, null, null, null, null], [null, null, null, null, null, null, null, 0.6138250553295381, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, 0.6110429589731137, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, 0.6106989501954396, null, null], [null, null, null, null, null, null, 0.6265161243402462, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [4.476801730909803e-06, 2.513839845748559e-06, 7.8526174359856e-05, 2.551381220577768e-06, 8.147063369349608e-07, 2.6055036956959964e-05, 1.8625421064374152e-05, 3.1267957411782596e-05, 3.8766647519613884e-05, 2.2143500784731613e-05, 2.732009086725888e-05, 3.6251056916748545e-05, 3.11431339958987e-05, 1.0901366929821377e-05], \"xaxis\": \"x13\", \"y\": [2.5850386437508592e-11, 0.0007864277011823985, 0.002627620322773597, 0.005755809641012473, 8.968596925101844e-07, 0.0008487645275541602, 0.44413277348876284, 0.31936477624863213, 0.3951941333092171, 0.20733035651443532, 0.007863758418677648, 0.0015849419549483902, 0.10656525639450408, 0.002238456984312462], \"yaxis\": \"y13\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.2560392337639659, 1.3682781516434461, 1.4816277342397546, 1.8892289717413373, 1.9895336756655286, 2.120146567349586, 2.1822213333826617, 2.3114297349756527, 2.3629865645428607, 3.0279507513620847, 3.2835956183603625, 3.857300200725326, 4.039553163999802, 4.580298688440557, 7.578759623990395, 8.255993070366346], \"xaxis\": \"x2\", \"y\": [6.483364326628161e-07, 8.147063369349608e-07, 2.513839845748559e-06, 2.551381220577768e-06, 4.476801730909803e-06, 1.0901366929821377e-05, 1.8625421064374152e-05, 2.2143500784731613e-05, 2.6055036956959964e-05, 2.732009086725888e-05, 3.11431339958987e-05, 3.1267957411782596e-05, 3.6251056916748545e-05, 3.8766647519613884e-05, 7.8526174359856e-05, 9.867681136393363e-05], \"yaxis\": \"y2\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, 0.3703105027563971, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, 0.5406865472036322, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.5789527839586361, null], [null, null, null, null, null, null, null, null, null, null, null, null, 0.6097196385862345, null, null, null], [null, null, null, null, null, null, null, null, null, 0.6232188166435766, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, 0.6265161243402462, null, null, null, null, null], [null, null, null, null, 0.6138250553295381, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, 0.6294944760629394, null, null, null, null], [null, null, 0.619966576902523, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, 0.6252178845429289, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, 0.6110429589731137, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, 0.6148939027665861, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, 0.6106989501954396, null, null, null, null, null, null, null], [null, null, null, 0.3653230966090851, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [4.039553163999802, 4.580298688440557, 1.8892289717413373, 7.578759623990395, 1.3682781516434461, 3.857300200725326, 3.2835956183603625, 2.1822213333826617, 2.3629865645428607, 1.9895336756655286, 1.4816277342397546, 2.120146567349586, 2.3114297349756527, 3.0279507513620847], \"xaxis\": \"x2\", \"y\": [4.476801730909803e-06, 2.513839845748559e-06, 7.8526174359856e-05, 2.551381220577768e-06, 8.147063369349608e-07, 2.6055036956959964e-05, 1.8625421064374152e-05, 3.1267957411782596e-05, 3.8766647519613884e-05, 2.2143500784731613e-05, 2.732009086725888e-05, 3.6251056916748545e-05, 3.11431339958987e-05, 1.0901366929821377e-05], \"yaxis\": \"y2\"}, {\"type\": \"scatter\", \"xaxis\": \"x6\", \"yaxis\": \"y6\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.2560392337639659, 1.3682781516434461, 1.4816277342397546, 1.8892289717413373, 1.9895336756655286, 2.120146567349586, 2.1822213333826617, 2.3114297349756527, 2.3629865645428607, 3.0279507513620847, 3.2835956183603625, 3.857300200725326, 4.039553163999802, 4.580298688440557, 7.578759623990395, 8.255993070366346], \"xaxis\": \"x10\", \"y\": [1.2999999999999998, 2, 4, 8, 16, 16.7], \"yaxis\": \"y10\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, 0.3703105027563971, null, 0.3653230966090851, null, null, null, null, null, null, null, null, 0.6097196385862345, null, null, null], [null, null, null, null, 0.6138250553295381, 0.6148939027665861, 0.6110429589731137, 0.6252178845429289, 0.6106989501954396, 0.6232188166435766, 0.6265161243402462, null, null, null, null, null], [null, null, 0.619966576902523, null, null, null, null, null, null, null, null, 0.6294944760629394, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, 0.5406865472036322, 0.5789527839586361, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [4.039553163999802, 4.580298688440557, 1.8892289717413373, 7.578759623990395, 1.3682781516434461, 3.857300200725326, 3.2835956183603625, 2.1822213333826617, 2.3629865645428607, 1.9895336756655286, 1.4816277342397546, 2.120146567349586, 2.3114297349756527, 3.0279507513620847], \"xaxis\": \"x10\", \"y\": [2, 16, 2, 16, 2, 8, 4, 4, 4, 4, 8, 4, 4, 4], \"yaxis\": \"y10\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.2560392337639659, 1.3682781516434461, 1.4816277342397546, 1.8892289717413373, 1.9895336756655286, 2.120146567349586, 2.1822213333826617, 2.3114297349756527, 2.3629865645428607, 3.0279507513620847, 3.2835956183603625, 3.857300200725326, 4.039553163999802, 4.580298688440557, 7.578759623990395, 8.255993070366346], \"xaxis\": \"x14\", \"y\": [7.956365959576677e-12, 2.5850386437508592e-11, 8.968596925101844e-07, 0.0007864277011823985, 0.0008487645275541602, 0.0015849419549483902, 0.002238456984312462, 0.002627620322773597, 0.005755809641012473, 0.007863758418677648, 0.10656525639450408, 0.20733035651443532, 0.31936477624863213, 0.3951941333092171, 0.44413277348876284, 1.4429959459604629], \"yaxis\": \"y14\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, 0.6097196385862345, null, null, null], [null, 0.3703105027563971, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, 0.5406865472036322, null, null], [null, null, null, null, null, null, null, null, null, null, null, 0.6294944760629394, null, null, null, null], [null, null, null, null, null, 0.6148939027665861, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, 0.6232188166435766, null, null, null, null, null, null], [null, null, null, 0.3653230966090851, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.5789527839586361, null], [null, null, 0.619966576902523, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, 0.6252178845429289, null, null, null, null, null, null, null, null], [null, null, null, null, 0.6138250553295381, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, 0.6110429589731137, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, 0.6106989501954396, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, 0.6265161243402462, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [4.039553163999802, 4.580298688440557, 1.8892289717413373, 7.578759623990395, 1.3682781516434461, 3.857300200725326, 3.2835956183603625, 2.1822213333826617, 2.3629865645428607, 1.9895336756655286, 1.4816277342397546, 2.120146567349586, 2.3114297349756527, 3.0279507513620847], \"xaxis\": \"x14\", \"y\": [2.5850386437508592e-11, 0.0007864277011823985, 0.002627620322773597, 0.005755809641012473, 8.968596925101844e-07, 0.0008487645275541602, 0.44413277348876284, 0.31936477624863213, 0.3951941333092171, 0.20733035651443532, 0.007863758418677648, 0.0015849419549483902, 0.10656525639450408, 0.002238456984312462], \"yaxis\": \"y14\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.2999999999999998, 2, 4, 8, 16, 16.7], \"xaxis\": \"x3\", \"y\": [6.483364326628161e-07, 8.147063369349608e-07, 2.513839845748559e-06, 2.551381220577768e-06, 4.476801730909803e-06, 1.0901366929821377e-05, 1.8625421064374152e-05, 2.2143500784731613e-05, 2.6055036956959964e-05, 2.732009086725888e-05, 3.11431339958987e-05, 3.1267957411782596e-05, 3.6251056916748545e-05, 3.8766647519613884e-05, 7.8526174359856e-05, 9.867681136393363e-05], \"yaxis\": \"y3\", \"z\": [[null, null, null, null, null, null], [null, 0.3703105027563971, null, null, null, null], [null, null, null, null, 0.5406865472036322, null], [null, null, null, null, 0.5789527839586361, null], [null, 0.6097196385862345, null, null, null, null], [null, null, 0.6232188166435766, null, null, null], [null, null, 0.6265161243402462, null, null, null], [null, null, 0.6138250553295381, null, null, null], [null, null, null, 0.6294944760629394, null, null], [null, null, null, 0.619966576902523, null, null], [null, null, 0.6252178845429289, null, null, null], [null, null, 0.6110429589731137, null, null, null], [null, null, 0.6148939027665861, null, null, null], [null, null, 0.6106989501954396, null, null, null], [null, 0.3653230966090851, null, null, null, null], [null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [2, 16, 2, 16, 2, 8, 4, 4, 4, 4, 8, 4, 4, 4], \"xaxis\": \"x3\", \"y\": [4.476801730909803e-06, 2.513839845748559e-06, 7.8526174359856e-05, 2.551381220577768e-06, 8.147063369349608e-07, 2.6055036956959964e-05, 1.8625421064374152e-05, 3.1267957411782596e-05, 3.8766647519613884e-05, 2.2143500784731613e-05, 2.732009086725888e-05, 3.6251056916748545e-05, 3.11431339958987e-05, 1.0901366929821377e-05], \"yaxis\": \"y3\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.2999999999999998, 2, 4, 8, 16, 16.7], \"xaxis\": \"x7\", \"y\": [1.2560392337639659, 1.3682781516434461, 1.4816277342397546, 1.8892289717413373, 1.9895336756655286, 2.120146567349586, 2.1822213333826617, 2.3114297349756527, 2.3629865645428607, 3.0279507513620847, 3.2835956183603625, 3.857300200725326, 4.039553163999802, 4.580298688440557, 7.578759623990395, 8.255993070366346], \"yaxis\": \"y7\", \"z\": [[null, null, null, null, null, null], [null, 0.3703105027563971, null, null, null, null], [null, null, null, 0.619966576902523, null, null], [null, 0.3653230966090851, null, null, null, null], [null, null, 0.6138250553295381, null, null, null], [null, null, 0.6148939027665861, null, null, null], [null, null, 0.6110429589731137, null, null, null], [null, null, 0.6252178845429289, null, null, null], [null, null, 0.6106989501954396, null, null, null], [null, null, 0.6232188166435766, null, null, null], [null, null, 0.6265161243402462, null, null, null], [null, null, null, 0.6294944760629394, null, null], [null, 0.6097196385862345, null, null, null, null], [null, null, null, null, 0.5406865472036322, null], [null, null, null, null, 0.5789527839586361, null], [null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [2, 16, 2, 16, 2, 8, 4, 4, 4, 4, 8, 4, 4, 4], \"xaxis\": \"x7\", \"y\": [4.039553163999802, 4.580298688440557, 1.8892289717413373, 7.578759623990395, 1.3682781516434461, 3.857300200725326, 3.2835956183603625, 2.1822213333826617, 2.3629865645428607, 1.9895336756655286, 1.4816277342397546, 2.120146567349586, 2.3114297349756527, 3.0279507513620847], \"yaxis\": \"y7\"}, {\"type\": \"scatter\", \"xaxis\": \"x11\", \"yaxis\": \"y11\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [1.2999999999999998, 2, 4, 8, 16, 16.7], \"xaxis\": \"x15\", \"y\": [7.956365959576677e-12, 2.5850386437508592e-11, 8.968596925101844e-07, 0.0007864277011823985, 0.0008487645275541602, 0.0015849419549483902, 0.002238456984312462, 0.002627620322773597, 0.005755809641012473, 0.007863758418677648, 0.10656525639450408, 0.20733035651443532, 0.31936477624863213, 0.3951941333092171, 0.44413277348876284, 1.4429959459604629], \"yaxis\": \"y15\", \"z\": [[null, null, null, null, null, null], [null, 0.6097196385862345, null, null, null, null], [null, 0.3703105027563971, null, null, null, null], [null, null, null, null, 0.5406865472036322, null], [null, null, null, 0.6294944760629394, null, null], [null, null, 0.6148939027665861, null, null, null], [null, null, 0.6232188166435766, null, null, null], [null, 0.3653230966090851, null, null, null, null], [null, null, null, null, 0.5789527839586361, null], [null, null, null, 0.619966576902523, null, null], [null, null, 0.6252178845429289, null, null, null], [null, null, 0.6138250553295381, null, null, null], [null, null, 0.6110429589731137, null, null, null], [null, null, 0.6106989501954396, null, null, null], [null, null, 0.6265161243402462, null, null, null], [null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [2, 16, 2, 16, 2, 8, 4, 4, 4, 4, 8, 4, 4, 4], \"xaxis\": \"x15\", \"y\": [2.5850386437508592e-11, 0.0007864277011823985, 0.002627620322773597, 0.005755809641012473, 8.968596925101844e-07, 0.0008487645275541602, 0.44413277348876284, 0.31936477624863213, 0.3951941333092171, 0.20733035651443532, 0.007863758418677648, 0.0015849419549483902, 0.10656525639450408, 0.002238456984312462], \"yaxis\": \"y15\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [7.956365959576677e-12, 2.5850386437508592e-11, 8.968596925101844e-07, 0.0007864277011823985, 0.0008487645275541602, 0.0015849419549483902, 0.002238456984312462, 0.002627620322773597, 0.005755809641012473, 0.007863758418677648, 0.10656525639450408, 0.20733035651443532, 0.31936477624863213, 0.3951941333092171, 0.44413277348876284, 1.4429959459604629], \"xaxis\": \"x4\", \"y\": [6.483364326628161e-07, 8.147063369349608e-07, 2.513839845748559e-06, 2.551381220577768e-06, 4.476801730909803e-06, 1.0901366929821377e-05, 1.8625421064374152e-05, 2.2143500784731613e-05, 2.6055036956959964e-05, 2.732009086725888e-05, 3.11431339958987e-05, 3.1267957411782596e-05, 3.6251056916748545e-05, 3.8766647519613884e-05, 7.8526174359856e-05, 9.867681136393363e-05], \"yaxis\": \"y4\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, 0.3703105027563971, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, 0.5406865472036322, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, 0.5789527839586361, null, null, null, null, null, null, null], [null, 0.6097196385862345, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, 0.6232188166435766, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.6265161243402462, null], [null, null, null, null, null, null, null, null, null, null, null, 0.6138250553295381, null, null, null, null], [null, null, null, null, 0.6294944760629394, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, 0.619966576902523, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, 0.6252178845429289, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, 0.6110429589731137, null, null, null], [null, null, null, null, null, 0.6148939027665861, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, 0.6106989501954396, null, null], [null, null, null, null, null, null, null, 0.3653230966090851, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [2.5850386437508592e-11, 0.0007864277011823985, 0.002627620322773597, 0.005755809641012473, 8.968596925101844e-07, 0.0008487645275541602, 0.44413277348876284, 0.31936477624863213, 0.3951941333092171, 0.20733035651443532, 0.007863758418677648, 0.0015849419549483902, 0.10656525639450408, 0.002238456984312462], \"xaxis\": \"x4\", \"y\": [4.476801730909803e-06, 2.513839845748559e-06, 7.8526174359856e-05, 2.551381220577768e-06, 8.147063369349608e-07, 2.6055036956959964e-05, 1.8625421064374152e-05, 3.1267957411782596e-05, 3.8766647519613884e-05, 2.2143500784731613e-05, 2.732009086725888e-05, 3.6251056916748545e-05, 3.11431339958987e-05, 1.0901366929821377e-05], \"yaxis\": \"y4\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [7.956365959576677e-12, 2.5850386437508592e-11, 8.968596925101844e-07, 0.0007864277011823985, 0.0008487645275541602, 0.0015849419549483902, 0.002238456984312462, 0.002627620322773597, 0.005755809641012473, 0.007863758418677648, 0.10656525639450408, 0.20733035651443532, 0.31936477624863213, 0.3951941333092171, 0.44413277348876284, 1.4429959459604629], \"xaxis\": \"x8\", \"y\": [1.2560392337639659, 1.3682781516434461, 1.4816277342397546, 1.8892289717413373, 1.9895336756655286, 2.120146567349586, 2.1822213333826617, 2.3114297349756527, 2.3629865645428607, 3.0279507513620847, 3.2835956183603625, 3.857300200725326, 4.039553163999802, 4.580298688440557, 7.578759623990395, 8.255993070366346], \"yaxis\": \"y8\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, 0.3703105027563971, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, 0.619966576902523, null, null, null, null, null, null], [null, null, null, null, null, null, null, 0.3653230966090851, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, 0.6138250553295381, null, null, null, null], [null, null, null, null, null, 0.6148939027665861, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, 0.6110429589731137, null, null, null], [null, null, null, null, null, null, null, null, null, null, 0.6252178845429289, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, 0.6106989501954396, null, null], [null, null, null, null, null, null, 0.6232188166435766, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0.6265161243402462, null], [null, null, null, null, 0.6294944760629394, null, null, null, null, null, null, null, null, null, null, null], [null, 0.6097196385862345, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, 0.5406865472036322, null, null, null, null, null, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, 0.5789527839586361, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [2.5850386437508592e-11, 0.0007864277011823985, 0.002627620322773597, 0.005755809641012473, 8.968596925101844e-07, 0.0008487645275541602, 0.44413277348876284, 0.31936477624863213, 0.3951941333092171, 0.20733035651443532, 0.007863758418677648, 0.0015849419549483902, 0.10656525639450408, 0.002238456984312462], \"xaxis\": \"x8\", \"y\": [4.039553163999802, 4.580298688440557, 1.8892289717413373, 7.578759623990395, 1.3682781516434461, 3.857300200725326, 3.2835956183603625, 2.1822213333826617, 2.3629865645428607, 1.9895336756655286, 1.4816277342397546, 2.120146567349586, 2.3114297349756527, 3.0279507513620847], \"yaxis\": \"y8\"}, {\"colorbar\": {\"title\": {\"text\": \"Objective Value\"}}, \"colorscale\": [[0, \"rgb(5,10,172)\"], [0.35, \"rgb(40,60,190)\"], [0.5, \"rgb(70,100,245)\"], [0.6, \"rgb(90,120,245)\"], [0.7, \"rgb(106,137,247)\"], [1, \"rgb(220,220,220)\"]], \"connectgaps\": true, \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"none\", \"line\": {\"smoothing\": 1.3}, \"reversescale\": false, \"showscale\": false, \"type\": \"contour\", \"x\": [7.956365959576677e-12, 2.5850386437508592e-11, 8.968596925101844e-07, 0.0007864277011823985, 0.0008487645275541602, 0.0015849419549483902, 0.002238456984312462, 0.002627620322773597, 0.005755809641012473, 0.007863758418677648, 0.10656525639450408, 0.20733035651443532, 0.31936477624863213, 0.3951941333092171, 0.44413277348876284, 1.4429959459604629], \"xaxis\": \"x12\", \"y\": [1.2999999999999998, 2, 4, 8, 16, 16.7], \"yaxis\": \"y12\", \"z\": [[null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], [null, 0.6097196385862345, 0.3703105027563971, null, null, null, null, 0.3653230966090851, null, null, null, null, null, null, null, null], [null, null, null, null, null, 0.6148939027665861, 0.6232188166435766, null, null, null, 0.6252178845429289, 0.6138250553295381, 0.6110429589731137, 0.6106989501954396, 0.6265161243402462, null], [null, null, null, null, 0.6294944760629394, null, null, null, null, 0.619966576902523, null, null, null, null, null, null], [null, null, null, 0.5406865472036322, null, null, null, null, 0.5789527839586361, null, null, null, null, null, null, null], [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]]}, {\"marker\": {\"color\": \"black\", \"line\": {\"color\": \"Grey\", \"width\": 0.5}}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [2.5850386437508592e-11, 0.0007864277011823985, 0.002627620322773597, 0.005755809641012473, 8.968596925101844e-07, 0.0008487645275541602, 0.44413277348876284, 0.31936477624863213, 0.3951941333092171, 0.20733035651443532, 0.007863758418677648, 0.0015849419549483902, 0.10656525639450408, 0.002238456984312462], \"xaxis\": \"x12\", \"y\": [2, 16, 2, 16, 2, 8, 4, 4, 4, 4, 8, 4, 4, 4], \"yaxis\": \"y12\"}, {\"type\": \"scatter\", \"xaxis\": \"x16\", \"yaxis\": \"y16\"}],\n","                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Contour Plot\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 0.2125], \"matches\": \"x13\", \"range\": [-6.188199572936895, -4.0057848927192135], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis10\": {\"anchor\": \"y10\", \"domain\": [0.2625, 0.475], \"matches\": \"x14\", \"range\": [0.09900320527783006, 0.9167693197600111], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis11\": {\"anchor\": \"y11\", \"domain\": [0.525, 0.7375], \"matches\": \"x15\", \"range\": [1.2999999999999998, 16.7], \"showticklabels\": false}, \"xaxis12\": {\"anchor\": \"y12\", \"domain\": [0.7875, 1.0], \"matches\": \"x16\", \"range\": [-11.099285249357363, 0.15926511096218884], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis13\": {\"anchor\": \"y13\", \"domain\": [0.0, 0.2125], \"range\": [-6.188199572936895, -4.0057848927192135], \"title\": {\"text\": \"learning_rate\"}, \"type\": \"log\"}, \"xaxis14\": {\"anchor\": \"y14\", \"domain\": [0.2625, 0.475], \"range\": [0.09900320527783006, 0.9167693197600111], \"title\": {\"text\": \"num_train_epochs\"}, \"type\": \"log\"}, \"xaxis15\": {\"anchor\": \"y15\", \"domain\": [0.525, 0.7375], \"range\": [1.2999999999999998, 16.7], \"title\": {\"text\": \"per_device_train_batch_size\"}}, \"xaxis16\": {\"anchor\": \"y16\", \"domain\": [0.7875, 1.0], \"range\": [-11.099285249357363, 0.15926511096218884], \"title\": {\"text\": \"weight_decay\"}, \"type\": \"log\"}, \"xaxis2\": {\"anchor\": \"y2\", \"domain\": [0.2625, 0.475], \"matches\": \"x14\", \"range\": [0.09900320527783006, 0.9167693197600111], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis3\": {\"anchor\": \"y3\", \"domain\": [0.525, 0.7375], \"matches\": \"x15\", \"range\": [1.2999999999999998, 16.7], \"showticklabels\": false}, \"xaxis4\": {\"anchor\": \"y4\", \"domain\": [0.7875, 1.0], \"matches\": \"x16\", \"range\": [-11.099285249357363, 0.15926511096218884], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis5\": {\"anchor\": \"y5\", \"domain\": [0.0, 0.2125], \"matches\": \"x13\", \"range\": [-6.188199572936895, -4.0057848927192135], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis6\": {\"anchor\": \"y6\", \"domain\": [0.2625, 0.475], \"matches\": \"x14\", \"range\": [0.09900320527783006, 0.9167693197600111], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis7\": {\"anchor\": \"y7\", \"domain\": [0.525, 0.7375], \"matches\": \"x15\", \"range\": [1.2999999999999998, 16.7], \"showticklabels\": false}, \"xaxis8\": {\"anchor\": \"y8\", \"domain\": [0.7875, 1.0], \"matches\": \"x16\", \"range\": [-11.099285249357363, 0.15926511096218884], \"showticklabels\": false, \"type\": \"log\"}, \"xaxis9\": {\"anchor\": \"y9\", \"domain\": [0.0, 0.2125], \"matches\": \"x13\", \"range\": [-6.188199572936895, -4.0057848927192135], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.80625, 1.0], \"range\": [-6.188199572936895, -4.0057848927192135], \"title\": {\"text\": \"learning_rate\"}, \"type\": \"log\"}, \"yaxis10\": {\"anchor\": \"x10\", \"domain\": [0.26875, 0.4625], \"matches\": \"y9\", \"range\": [1.2999999999999998, 16.7], \"showticklabels\": false}, \"yaxis11\": {\"anchor\": \"x11\", \"domain\": [0.26875, 0.4625], \"matches\": \"y9\", \"range\": [1.2999999999999998, 16.7], \"showticklabels\": false}, \"yaxis12\": {\"anchor\": \"x12\", \"domain\": [0.26875, 0.4625], \"matches\": \"y9\", \"range\": [1.2999999999999998, 16.7], \"showticklabels\": false}, \"yaxis13\": {\"anchor\": \"x13\", \"domain\": [0.0, 0.19375], \"range\": [-11.099285249357363, 0.15926511096218884], \"title\": {\"text\": \"weight_decay\"}, \"type\": \"log\"}, \"yaxis14\": {\"anchor\": \"x14\", \"domain\": [0.0, 0.19375], \"matches\": \"y13\", \"range\": [-11.099285249357363, 0.15926511096218884], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis15\": {\"anchor\": \"x15\", \"domain\": [0.0, 0.19375], \"matches\": \"y13\", \"range\": [-11.099285249357363, 0.15926511096218884], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis16\": {\"anchor\": \"x16\", \"domain\": [0.0, 0.19375], \"matches\": \"y13\", \"range\": [-11.099285249357363, 0.15926511096218884], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis2\": {\"anchor\": \"x2\", \"domain\": [0.80625, 1.0], \"matches\": \"y\", \"range\": [-6.188199572936895, -4.0057848927192135], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis3\": {\"anchor\": \"x3\", \"domain\": [0.80625, 1.0], \"matches\": \"y\", \"range\": [-6.188199572936895, -4.0057848927192135], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis4\": {\"anchor\": \"x4\", \"domain\": [0.80625, 1.0], \"matches\": \"y\", \"range\": [-6.188199572936895, -4.0057848927192135], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis5\": {\"anchor\": \"x5\", \"domain\": [0.5375, 0.73125], \"range\": [0.09900320527783006, 0.9167693197600111], \"title\": {\"text\": \"num_train_epochs\"}, \"type\": \"log\"}, \"yaxis6\": {\"anchor\": \"x6\", \"domain\": [0.5375, 0.73125], \"matches\": \"y5\", \"range\": [0.09900320527783006, 0.9167693197600111], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis7\": {\"anchor\": \"x7\", \"domain\": [0.5375, 0.73125], \"matches\": \"y5\", \"range\": [0.09900320527783006, 0.9167693197600111], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis8\": {\"anchor\": \"x8\", \"domain\": [0.5375, 0.73125], \"matches\": \"y5\", \"range\": [0.09900320527783006, 0.9167693197600111], \"showticklabels\": false, \"type\": \"log\"}, \"yaxis9\": {\"anchor\": \"x9\", \"domain\": [0.26875, 0.4625], \"range\": [1.2999999999999998, 16.7], \"title\": {\"text\": \"per_device_train_batch_size\"}}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('2c03c0d3-f408-489e-a02b-258599480a08');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"uklDrIbLDvMG","colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"status":"ok","timestamp":1640492859535,"user_tz":-60,"elapsed":466,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"b3314f9d-1bf0-4de9-a08c-32542b3e8a7a"},"source":["optuna.visualization.plot_slice(study)"],"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"ed28274f-91d3-4c30-9f2c-ce8193253f69\" class=\"plotly-graph-div\" style=\"height:525px; width:1200px;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"ed28274f-91d3-4c30-9f2c-ce8193253f69\")) {\n","                    Plotly.newPlot(\n","                        'ed28274f-91d3-4c30-9f2c-ce8193253f69',\n","                        [{\"marker\": {\"color\": [0, 1, 2, 3, 4, 5, 13, 14, 21, 22, 24, 33, 43, 46], \"colorbar\": {\"title\": {\"text\": \"#Trials\"}, \"x\": 1.0, \"xpad\": 40}, \"colorscale\": [[0.0, \"rgb(247,251,255)\"], [0.125, \"rgb(222,235,247)\"], [0.25, \"rgb(198,219,239)\"], [0.375, \"rgb(158,202,225)\"], [0.5, \"rgb(107,174,214)\"], [0.625, \"rgb(66,146,198)\"], [0.75, \"rgb(33,113,181)\"], [0.875, \"rgb(8,81,156)\"], [1.0, \"rgb(8,48,107)\"]], \"line\": {\"color\": \"Grey\", \"width\": 0.5}, \"showscale\": true}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [4.476801730909803e-06, 2.513839845748559e-06, 7.8526174359856e-05, 2.551381220577768e-06, 8.147063369349608e-07, 2.6055036956959964e-05, 1.8625421064374152e-05, 3.1267957411782596e-05, 3.8766647519613884e-05, 2.2143500784731613e-05, 2.732009086725888e-05, 3.6251056916748545e-05, 3.11431339958987e-05, 1.0901366929821377e-05], \"xaxis\": \"x\", \"y\": [0.6097196385862345, 0.5406865472036322, 0.3653230966090851, 0.5789527839586361, 0.3703105027563971, 0.6294944760629394, 0.6265161243402462, 0.6110429589731137, 0.6106989501954396, 0.6138250553295381, 0.619966576902523, 0.6148939027665861, 0.6252178845429289, 0.6232188166435766], \"yaxis\": \"y\"}, {\"marker\": {\"color\": [0, 1, 2, 3, 4, 5, 13, 14, 21, 22, 24, 33, 43, 46], \"colorbar\": {\"title\": {\"text\": \"#Trials\"}, \"x\": 1.0, \"xpad\": 40}, \"colorscale\": [[0.0, \"rgb(247,251,255)\"], [0.125, \"rgb(222,235,247)\"], [0.25, \"rgb(198,219,239)\"], [0.375, \"rgb(158,202,225)\"], [0.5, \"rgb(107,174,214)\"], [0.625, \"rgb(66,146,198)\"], [0.75, \"rgb(33,113,181)\"], [0.875, \"rgb(8,81,156)\"], [1.0, \"rgb(8,48,107)\"]], \"line\": {\"color\": \"Grey\", \"width\": 0.5}, \"showscale\": false}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [4.039553163999802, 4.580298688440557, 1.8892289717413373, 7.578759623990395, 1.3682781516434461, 3.857300200725326, 3.2835956183603625, 2.1822213333826617, 2.3629865645428607, 1.9895336756655286, 1.4816277342397546, 2.120146567349586, 2.3114297349756527, 3.0279507513620847], \"xaxis\": \"x2\", \"y\": [0.6097196385862345, 0.5406865472036322, 0.3653230966090851, 0.5789527839586361, 0.3703105027563971, 0.6294944760629394, 0.6265161243402462, 0.6110429589731137, 0.6106989501954396, 0.6138250553295381, 0.619966576902523, 0.6148939027665861, 0.6252178845429289, 0.6232188166435766], \"yaxis\": \"y2\"}, {\"marker\": {\"color\": [0, 1, 2, 3, 4, 5, 13, 14, 21, 22, 24, 33, 43, 46], \"colorbar\": {\"title\": {\"text\": \"#Trials\"}, \"x\": 1.0, \"xpad\": 40}, \"colorscale\": [[0.0, \"rgb(247,251,255)\"], [0.125, \"rgb(222,235,247)\"], [0.25, \"rgb(198,219,239)\"], [0.375, \"rgb(158,202,225)\"], [0.5, \"rgb(107,174,214)\"], [0.625, \"rgb(66,146,198)\"], [0.75, \"rgb(33,113,181)\"], [0.875, \"rgb(8,81,156)\"], [1.0, \"rgb(8,48,107)\"]], \"line\": {\"color\": \"Grey\", \"width\": 0.5}, \"showscale\": false}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [2, 16, 2, 16, 2, 8, 4, 4, 4, 4, 8, 4, 4, 4], \"xaxis\": \"x3\", \"y\": [0.6097196385862345, 0.5406865472036322, 0.3653230966090851, 0.5789527839586361, 0.3703105027563971, 0.6294944760629394, 0.6265161243402462, 0.6110429589731137, 0.6106989501954396, 0.6138250553295381, 0.619966576902523, 0.6148939027665861, 0.6252178845429289, 0.6232188166435766], \"yaxis\": \"y3\"}, {\"marker\": {\"color\": [0, 1, 2, 3, 4, 5, 13, 14, 21, 22, 24, 33, 43, 46], \"colorbar\": {\"title\": {\"text\": \"#Trials\"}, \"x\": 1.0, \"xpad\": 40}, \"colorscale\": [[0.0, \"rgb(247,251,255)\"], [0.125, \"rgb(222,235,247)\"], [0.25, \"rgb(198,219,239)\"], [0.375, \"rgb(158,202,225)\"], [0.5, \"rgb(107,174,214)\"], [0.625, \"rgb(66,146,198)\"], [0.75, \"rgb(33,113,181)\"], [0.875, \"rgb(8,81,156)\"], [1.0, \"rgb(8,48,107)\"]], \"line\": {\"color\": \"Grey\", \"width\": 0.5}, \"showscale\": false}, \"mode\": \"markers\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [2.5850386437508592e-11, 0.0007864277011823985, 0.002627620322773597, 0.005755809641012473, 8.968596925101844e-07, 0.0008487645275541602, 0.44413277348876284, 0.31936477624863213, 0.3951941333092171, 0.20733035651443532, 0.007863758418677648, 0.0015849419549483902, 0.10656525639450408, 0.002238456984312462], \"xaxis\": \"x4\", \"y\": [0.6097196385862345, 0.5406865472036322, 0.3653230966090851, 0.5789527839586361, 0.3703105027563971, 0.6294944760629394, 0.6265161243402462, 0.6110429589731137, 0.6106989501954396, 0.6138250553295381, 0.619966576902523, 0.6148939027665861, 0.6252178845429289, 0.6232188166435766], \"yaxis\": \"y4\"}],\n","                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Slice Plot\"}, \"width\": 1200, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 0.2125], \"title\": {\"text\": \"learning_rate\"}, \"type\": \"log\"}, \"xaxis2\": {\"anchor\": \"y2\", \"domain\": [0.2625, 0.475], \"title\": {\"text\": \"num_train_epochs\"}, \"type\": \"log\"}, \"xaxis3\": {\"anchor\": \"y3\", \"domain\": [0.525, 0.7375], \"title\": {\"text\": \"per_device_train_batch_size\"}}, \"xaxis4\": {\"anchor\": \"y4\", \"domain\": [0.7875, 1.0], \"title\": {\"text\": \"weight_decay\"}, \"type\": \"log\"}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Objective Value\"}}, \"yaxis2\": {\"anchor\": \"x2\", \"domain\": [0.0, 1.0], \"matches\": \"y\", \"showticklabels\": false}, \"yaxis3\": {\"anchor\": \"x3\", \"domain\": [0.0, 1.0], \"matches\": \"y\", \"showticklabels\": false}, \"yaxis4\": {\"anchor\": \"x4\", \"domain\": [0.0, 1.0], \"matches\": \"y\", \"showticklabels\": false}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('ed28274f-91d3-4c30-9f2c-ce8193253f69');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"PnOWxc8TD52A","colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"status":"ok","timestamp":1640492859536,"user_tz":-60,"elapsed":14,"user":{"displayName":"d fernau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtOrt2LZcdWykNSlBAN7tMtM_7UVRFOVHOQH3v=s64","userId":"14123066965164812428"}},"outputId":"d11e895a-e6d3-4d4c-c4e9-c70c23eff2ed"},"source":["optuna.visualization.plot_edf(study)"],"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"0036cd84-82be-4281-904b-ff60dace0039\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"0036cd84-82be-4281-904b-ff60dace0039\")) {\n","                    Plotly.newPlot(\n","                        '0036cd84-82be-4281-904b-ff60dace0039',\n","                        [{\"mode\": \"lines\", \"name\": \"SIMCSE_BERT_myPers_CON\", \"type\": \"scatter\", \"x\": [0.3653230966090851, 0.3679914943813462, 0.3706598921536074, 0.37332828992586853, 0.3759966876981297, 0.37866508547039085, 0.381333483242652, 0.38400188101491317, 0.3866702787871743, 0.38933867655943544, 0.3920070743316966, 0.39467547210395776, 0.39734386987621895, 0.4000122676484801, 0.4026806654207412, 0.4053490631930024, 0.40801746096526353, 0.4106858587375247, 0.41335425650978586, 0.416022654282047, 0.4186910520543082, 0.4213594498265693, 0.4240278475988305, 0.42669624537109163, 0.42936464314335276, 0.43203304091561395, 0.4347014386878751, 0.4373698364601363, 0.4400382342323974, 0.44270663200465854, 0.44537502977691973, 0.44804342754918086, 0.45071182532144205, 0.4533802230937032, 0.4560486208659643, 0.4587170186382255, 0.46138541641048664, 0.4640538141827478, 0.46672221195500896, 0.4693906097272701, 0.4720590074995312, 0.4747274052717924, 0.4773958030440536, 0.48006420081631473, 0.48273259858857587, 0.485400996360837, 0.4880693941330982, 0.4907377919053594, 0.4934061896776205, 0.49607458744988164, 0.4987429852221428, 0.501411382994404, 0.5040797807666652, 0.5067481785389263, 0.5094165763111874, 0.5120849740834486, 0.5147533718557098, 0.5174217696279709, 0.5200901674002321, 0.5227585651724932, 0.5254269629447543, 0.5280953607170156, 0.5307637584892766, 0.5334321562615378, 0.536100554033799, 0.5387689518060601, 0.5414373495783213, 0.5441057473505824, 0.5467741451228436, 0.5494425428951047, 0.5521109406673659, 0.554779338439627, 0.5574477362118881, 0.5601161339841494, 0.5627845317564105, 0.5654529295286717, 0.5681213273009328, 0.5707897250731939, 0.5734581228454552, 0.5761265206177163, 0.5787949183899774, 0.5814633161622386, 0.5841317139344997, 0.5868001117067609, 0.5894685094790221, 0.5921369072512832, 0.5948053050235443, 0.5974737027958055, 0.6001421005680667, 0.6028104983403278, 0.605478896112589, 0.6081472938848501, 0.6108156916571112, 0.6134840894293725, 0.6161524872016336, 0.6188208849738948, 0.621489282746156, 0.624157680518417, 0.6268260782906783, 0.6294944760629394], \"y\": [0.07142857142857142, 0.07142857142857142, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.21428571428571427, 0.21428571428571427, 0.21428571428571427, 0.21428571428571427, 0.21428571428571427, 0.21428571428571427, 0.21428571428571427, 0.21428571428571427, 0.21428571428571427, 0.21428571428571427, 0.21428571428571427, 0.21428571428571427, 0.21428571428571427, 0.21428571428571427, 0.21428571428571427, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.42857142857142855, 0.5, 0.6428571428571429, 0.6428571428571429, 0.7142857142857143, 0.7857142857142857, 0.9285714285714286, 1.0]}],\n","                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Empirical Distribution Function Plot\"}, \"xaxis\": {\"title\": {\"text\": \"Objective Value\"}}, \"yaxis\": {\"range\": [0, 1], \"title\": {\"text\": \"Cumulative Probability\"}}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('0036cd84-82be-4281-904b-ff60dace0039');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{}}]}]}